{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d532991c-1075-4380-9029-12971e07d339",
      "metadata": {
        "id": "d532991c-1075-4380-9029-12971e07d339",
        "tags": []
      },
      "source": [
        "# Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b06c6fa-0960-45b3-803b-91bdb0a5e1c3",
      "metadata": {
        "id": "3b06c6fa-0960-45b3-803b-91bdb0a5e1c3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b0d6ea-b522-464b-acea-3d6a2edd9f76",
      "metadata": {
        "id": "a7b0d6ea-b522-464b-acea-3d6a2edd9f76"
      },
      "outputs": [],
      "source": [
        "pip uninstall tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1299e097-4416-4870-bc54-0f0f17d6ab8a",
      "metadata": {
        "id": "1299e097-4416-4870-bc54-0f0f17d6ab8a"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow==2.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c298c4-bff0-4249-baf1-3a004ce23a20",
      "metadata": {
        "id": "c4c298c4-bff0-4249-baf1-3a004ce23a20"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b574fc6-1a0d-4bab-aed3-102b9df892cf",
      "metadata": {
        "scrolled": true,
        "id": "2b574fc6-1a0d-4bab-aed3-102b9df892cf"
      },
      "outputs": [],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af0b9334-fbc9-4dd9-9350-841dc7c9f8f3",
      "metadata": {
        "id": "af0b9334-fbc9-4dd9-9350-841dc7c9f8f3"
      },
      "outputs": [],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5HGqtiyPq2ag",
      "metadata": {
        "id": "5HGqtiyPq2ag"
      },
      "outputs": [],
      "source": [
        "##from google.colab import drive\n",
        "##drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768c2e0a-a95f-4a73-8512-11d02bf10a32",
      "metadata": {
        "id": "768c2e0a-a95f-4a73-8512-11d02bf10a32"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589yAds8rjkS",
      "metadata": {
        "id": "589yAds8rjkS"
      },
      "outputs": [],
      "source": [
        "file_path = 'train_sentiment.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e7d32b-c6cc-4616-89d9-25d579c2bf6a",
      "metadata": {
        "id": "73e7d32b-c6cc-4616-89d9-25d579c2bf6a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "DATASET_COLUMNS=['labels','id','Date','Flag','User','Text']\n",
        "DATASET_ENCODING = \"utf-8\"\n",
        "df = pd.read_csv(file_path, encoding=DATASET_ENCODING, names=DATASET_COLUMNS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e47af7a-fb32-4391-8b8d-837e0c0a00e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0e47af7a-fb32-4391-8b8d-837e0c0a00e0",
        "outputId": "2885668e-b04e-4078-999e-8ec4ca7a36ed",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>Date</th>\n",
              "      <th>Flag</th>\n",
              "      <th>User</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>2000548391</td>\n",
              "      <td>Mon Jun 01 22:22:01 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ticia42</td>\n",
              "      <td>@Z12 She can't open the door by herself, so I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2191932827</td>\n",
              "      <td>Tue Jun 16 06:13:30 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>lhotfoot</td>\n",
              "      <td>@inournuclearage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>1754199174</td>\n",
              "      <td>Sun May 10 05:23:02 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>hockeycrew</td>\n",
              "      <td>@jesthebes At least your lawn hasn't been take...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1994056674</td>\n",
              "      <td>Mon Jun 01 11:20:41 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>GeoBlack_Cat</td>\n",
              "      <td>umm.. like, hello? where's the child support p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1980150068</td>\n",
              "      <td>Sun May 31 05:51:31 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>rawrcelne</td>\n",
              "      <td>Joined twitter</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   labels          id                          Date      Flag          User  \\\n",
              "0       4  2000548391  Mon Jun 01 22:22:01 PDT 2009  NO_QUERY       ticia42   \n",
              "1       0  2191932827  Tue Jun 16 06:13:30 PDT 2009  NO_QUERY      lhotfoot   \n",
              "2       4  1754199174  Sun May 10 05:23:02 PDT 2009  NO_QUERY    hockeycrew   \n",
              "3       0  1994056674  Mon Jun 01 11:20:41 PDT 2009  NO_QUERY  GeoBlack_Cat   \n",
              "4       4  1980150068  Sun May 31 05:51:31 PDT 2009  NO_QUERY     rawrcelne   \n",
              "\n",
              "                                                Text  \n",
              "0  @Z12 She can't open the door by herself, so I ...  \n",
              "1                                  @inournuclearage   \n",
              "2  @jesthebes At least your lawn hasn't been take...  \n",
              "3  umm.. like, hello? where's the child support p...  \n",
              "4                                    Joined twitter   "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983a8cec-ea87-45f8-b8aa-22d5904e5dd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "983a8cec-ea87-45f8-b8aa-22d5904e5dd0",
        "outputId": "52597a51-4deb-49cd-dbda-34a0f887bebd",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000000, 6)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de259550-06d3-44a8-a12e-c172f3920eaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de259550-06d3-44a8-a12e-c172f3920eaf",
        "outputId": "22ca68c9-fffb-4e6f-95dc-2da07c088c9a",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "labels    0\n",
              "id        0\n",
              "Date      0\n",
              "Flag      0\n",
              "User      0\n",
              "Text      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78a0b32-810a-4aee-a0db-05e78835bd60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a78a0b32-810a-4aee-a0db-05e78835bd60",
        "outputId": "5efbb9c9-8ccc-4db8-9c3c-242d09354c09",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['labels', 'id', 'Date', 'Flag', 'User', 'Text'], dtype='object')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b41733a3-2819-445b-9d1b-dc2261063dde",
      "metadata": {
        "id": "b41733a3-2819-445b-9d1b-dc2261063dde",
        "tags": []
      },
      "source": [
        "## Duplications detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6c75da-eda4-4462-b04d-1d278155f06d",
      "metadata": {
        "id": "fe6c75da-eda4-4462-b04d-1d278155f06d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "duplicates = df[df.drop(columns=['labels']).duplicated()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QkhO2oTILV1n",
      "metadata": {
        "id": "QkhO2oTILV1n"
      },
      "source": [
        "Get all rows which are duplications while ignoring labels. Only labels differ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d925858-5cf9-4ded-a356-a1ab9373e86a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3d925858-5cf9-4ded-a356-a1ab9373e86a",
        "outputId": "f464c80c-3bbe-4ca5-b84a-9c7d9dd74bc5",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>Date</th>\n",
              "      <th>Flag</th>\n",
              "      <th>User</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17116</th>\n",
              "      <td>0</td>\n",
              "      <td>1989814053</td>\n",
              "      <td>Mon Jun 01 02:54:18 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AlleX91</td>\n",
              "      <td>Uugh  ...  school time ...again  Raining outsi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17244</th>\n",
              "      <td>4</td>\n",
              "      <td>1677642549</td>\n",
              "      <td>Sat May 02 03:11:33 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>torilovesbradie</td>\n",
              "      <td>@charlii1 awwww they are lovely  i wish i had ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43683</th>\n",
              "      <td>0</td>\n",
              "      <td>1978117620</td>\n",
              "      <td>Sat May 30 22:29:18 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Kutski</td>\n",
              "      <td>Singapore was OFF THE HOOK last night  forced ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47638</th>\n",
              "      <td>0</td>\n",
              "      <td>1793140987</td>\n",
              "      <td>Thu May 14 01:31:50 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>StampfliTurci</td>\n",
              "      <td>@billingtonart I felt the same way as the weat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56575</th>\n",
              "      <td>4</td>\n",
              "      <td>2175919778</td>\n",
              "      <td>Mon Jun 15 02:22:59 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>TheNewBradie</td>\n",
              "      <td>@NessaSlashRice hiii  it was ummm lonely SO MA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996001</th>\n",
              "      <td>4</td>\n",
              "      <td>1979562113</td>\n",
              "      <td>Sun May 31 03:34:54 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>hayleytrotter</td>\n",
              "      <td>Ahhh i think CIbulkova is about to beat Szavay...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997561</th>\n",
              "      <td>4</td>\n",
              "      <td>1974742920</td>\n",
              "      <td>Sat May 30 13:45:20 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>KristineDulay</td>\n",
              "      <td>@TWITTAH_G I know  everytime I see commercials...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998144</th>\n",
              "      <td>0</td>\n",
              "      <td>1978027912</td>\n",
              "      <td>Sat May 30 22:15:44 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>lesleeyvonne</td>\n",
              "      <td>Yeah sarah!! At my bouse!  lol.  I misss Fresno.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999333</th>\n",
              "      <td>4</td>\n",
              "      <td>1687630113</td>\n",
              "      <td>Sun May 03 09:11:42 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>raeraeverret</td>\n",
              "      <td>Shreveport this week for 311, NOLA next week f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999715</th>\n",
              "      <td>4</td>\n",
              "      <td>2014733587</td>\n",
              "      <td>Wed Jun 03 02:30:31 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>QueenBxoxo</td>\n",
              "      <td>@xbllygbsnii no, to her sister! yes to deryck!...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>660 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        labels          id                          Date      Flag  \\\n",
              "17116        0  1989814053  Mon Jun 01 02:54:18 PDT 2009  NO_QUERY   \n",
              "17244        4  1677642549  Sat May 02 03:11:33 PDT 2009  NO_QUERY   \n",
              "43683        0  1978117620  Sat May 30 22:29:18 PDT 2009  NO_QUERY   \n",
              "47638        0  1793140987  Thu May 14 01:31:50 PDT 2009  NO_QUERY   \n",
              "56575        4  2175919778  Mon Jun 15 02:22:59 PDT 2009  NO_QUERY   \n",
              "...        ...         ...                           ...       ...   \n",
              "996001       4  1979562113  Sun May 31 03:34:54 PDT 2009  NO_QUERY   \n",
              "997561       4  1974742920  Sat May 30 13:45:20 PDT 2009  NO_QUERY   \n",
              "998144       0  1978027912  Sat May 30 22:15:44 PDT 2009  NO_QUERY   \n",
              "999333       4  1687630113  Sun May 03 09:11:42 PDT 2009  NO_QUERY   \n",
              "999715       4  2014733587  Wed Jun 03 02:30:31 PDT 2009  NO_QUERY   \n",
              "\n",
              "                   User                                               Text  \n",
              "17116           AlleX91  Uugh  ...  school time ...again  Raining outsi...  \n",
              "17244   torilovesbradie  @charlii1 awwww they are lovely  i wish i had ...  \n",
              "43683            Kutski  Singapore was OFF THE HOOK last night  forced ...  \n",
              "47638     StampfliTurci  @billingtonart I felt the same way as the weat...  \n",
              "56575      TheNewBradie  @NessaSlashRice hiii  it was ummm lonely SO MA...  \n",
              "...                 ...                                                ...  \n",
              "996001    hayleytrotter  Ahhh i think CIbulkova is about to beat Szavay...  \n",
              "997561    KristineDulay  @TWITTAH_G I know  everytime I see commercials...  \n",
              "998144     lesleeyvonne   Yeah sarah!! At my bouse!  lol.  I misss Fresno.  \n",
              "999333     raeraeverret  Shreveport this week for 311, NOLA next week f...  \n",
              "999715       QueenBxoxo  @xbllygbsnii no, to her sister! yes to deryck!...  \n",
              "\n",
              "[660 rows x 6 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deeb0f40-b36d-4664-8b67-320ee7095f8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deeb0f40-b36d-4664-8b67-320ee7095f8e",
        "outputId": "e726ab75-ddac-4f93-ad6e-d969bebfb60e",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "labels\n",
              "4    348\n",
              "0    312\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duplicates.labels.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f0b532-b84e-4992-8a01-f533b5df0d7d",
      "metadata": {
        "id": "57f0b532-b84e-4992-8a01-f533b5df0d7d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df = df.drop(duplicates.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4c516a-4e8c-4b42-8ccf-7aa3b6cf74b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c4c516a-4e8c-4b42-8ccf-7aa3b6cf74b1",
        "outputId": "64ee16b0-ae81-4e00-db2c-b9bbeca1720d",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(999340, 6)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f6f734-e1ff-4570-b457-cd7892df7964",
      "metadata": {
        "id": "d0f6f734-e1ff-4570-b457-cd7892df7964",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Group by users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b229a743-a694-42a8-a7db-6da4ec5499ad",
      "metadata": {
        "id": "b229a743-a694-42a8-a7db-6da4ec5499ad",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_filtered = df.groupby('User').filter(lambda x: len(x) > 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3209c0-45ae-409b-b9b7-2844c25820f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa3209c0-45ae-409b-b9b7-2844c25820f9",
        "outputId": "d2e9e993-398c-450d-e5d2-36e23cbce697",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "User\n",
              "lost_dog         340\n",
              "webwoke          210\n",
              "tweetpet         183\n",
              "VioletsCRUK      176\n",
              "mcraddictal      166\n",
              "                ... \n",
              "alannahapple       2\n",
              "Chittaranjan       2\n",
              "MaliStack          2\n",
              "DStringzzZ         2\n",
              "BFlautista624      2\n",
              "Name: count, Length: 165371, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "usernames = df_filtered['User'].value_counts()\n",
        "usernames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1105c7c3-1395-4428-ab4b-ea7157be854d",
      "metadata": {
        "id": "1105c7c3-1395-4428-ab4b-ea7157be854d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_filtered = df.groupby('User').filter(lambda x: len(x) == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd4e19a-ee41-4d9a-9375-3f9d51680316",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dd4e19a-ee41-4d9a-9375-3f9d51680316",
        "outputId": "801e15be-ea68-4c68-cdce-f50e9090a65e",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "labels     int64\n",
              "id         int64\n",
              "Date      object\n",
              "Flag      object\n",
              "User      object\n",
              "Text      object\n",
              "dtype: object"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8f95138-2f49-4325-ae8e-01b733b2b10e",
      "metadata": {
        "id": "c8f95138-2f49-4325-ae8e-01b733b2b10e",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Drop redundant Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a958fe40-ae25-4e10-942a-8d057cbaba41",
      "metadata": {
        "id": "a958fe40-ae25-4e10-942a-8d057cbaba41",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['Flag'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9b5613-22ef-4033-80a4-b779f228f7f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de9b5613-22ef-4033-80a4-b779f228f7f6",
        "outputId": "ab73d5ad-4029-431b-9926-b964b980dd08",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "labels     int64\n",
              "id         int64\n",
              "Date      object\n",
              "User      object\n",
              "Text      object\n",
              "dtype: object"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61dec169-cb0a-4e57-92d9-b9d4581374d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61dec169-cb0a-4e57-92d9-b9d4581374d9",
        "outputId": "27daa023-9796-4ed3-934f-8af2f52bbbdf",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4, 0], dtype=int64)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.labels.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53dde17b-a508-487b-82a0-c66b360ea0f5",
      "metadata": {
        "id": "53dde17b-a508-487b-82a0-c66b360ea0f5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#df['labels'] = df['labels'].replace(4,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35aaa33e-f9cd-4b13-8b62-758cc6a4b455",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "35aaa33e-f9cd-4b13-8b62-758cc6a4b455",
        "outputId": "a264ee01-9535-445a-83df-33f4afcbf72e",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>Date</th>\n",
              "      <th>User</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>2000548391</td>\n",
              "      <td>Mon Jun 01 22:22:01 PDT 2009</td>\n",
              "      <td>ticia42</td>\n",
              "      <td>@Z12 She can't open the door by herself, so I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2191932827</td>\n",
              "      <td>Tue Jun 16 06:13:30 PDT 2009</td>\n",
              "      <td>lhotfoot</td>\n",
              "      <td>@inournuclearage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>1754199174</td>\n",
              "      <td>Sun May 10 05:23:02 PDT 2009</td>\n",
              "      <td>hockeycrew</td>\n",
              "      <td>@jesthebes At least your lawn hasn't been take...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1994056674</td>\n",
              "      <td>Mon Jun 01 11:20:41 PDT 2009</td>\n",
              "      <td>GeoBlack_Cat</td>\n",
              "      <td>umm.. like, hello? where's the child support p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1980150068</td>\n",
              "      <td>Sun May 31 05:51:31 PDT 2009</td>\n",
              "      <td>rawrcelne</td>\n",
              "      <td>Joined twitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999995</th>\n",
              "      <td>0</td>\n",
              "      <td>1985361990</td>\n",
              "      <td>Sun May 31 16:57:39 PDT 2009</td>\n",
              "      <td>lutheasalom</td>\n",
              "      <td>this song's middle change just doesn't want to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999996</th>\n",
              "      <td>4</td>\n",
              "      <td>2057029784</td>\n",
              "      <td>Sat Jun 06 12:14:24 PDT 2009</td>\n",
              "      <td>beeluz</td>\n",
              "      <td>@officialnjonas Good luck with that</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999997</th>\n",
              "      <td>0</td>\n",
              "      <td>1835639354</td>\n",
              "      <td>Mon May 18 06:26:21 PDT 2009</td>\n",
              "      <td>lordmuttley</td>\n",
              "      <td>@ProudGamerTweet I rather average 32370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999998</th>\n",
              "      <td>0</td>\n",
              "      <td>2246780174</td>\n",
              "      <td>Fri Jun 19 18:06:46 PDT 2009</td>\n",
              "      <td>MizSadittyFancy</td>\n",
              "      <td>Pickin up @misstinayao waitin on @sadittysash ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999999</th>\n",
              "      <td>0</td>\n",
              "      <td>1833617173</td>\n",
              "      <td>Sun May 17 23:52:31 PDT 2009</td>\n",
              "      <td>dindahh</td>\n",
              "      <td>@ home studying for maths wooot ! im so going ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>999340 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        labels          id                          Date             User  \\\n",
              "0            4  2000548391  Mon Jun 01 22:22:01 PDT 2009          ticia42   \n",
              "1            0  2191932827  Tue Jun 16 06:13:30 PDT 2009         lhotfoot   \n",
              "2            4  1754199174  Sun May 10 05:23:02 PDT 2009       hockeycrew   \n",
              "3            0  1994056674  Mon Jun 01 11:20:41 PDT 2009     GeoBlack_Cat   \n",
              "4            4  1980150068  Sun May 31 05:51:31 PDT 2009        rawrcelne   \n",
              "...        ...         ...                           ...              ...   \n",
              "999995       0  1985361990  Sun May 31 16:57:39 PDT 2009      lutheasalom   \n",
              "999996       4  2057029784  Sat Jun 06 12:14:24 PDT 2009           beeluz   \n",
              "999997       0  1835639354  Mon May 18 06:26:21 PDT 2009      lordmuttley   \n",
              "999998       0  2246780174  Fri Jun 19 18:06:46 PDT 2009  MizSadittyFancy   \n",
              "999999       0  1833617173  Sun May 17 23:52:31 PDT 2009          dindahh   \n",
              "\n",
              "                                                     Text  \n",
              "0       @Z12 She can't open the door by herself, so I ...  \n",
              "1                                       @inournuclearage   \n",
              "2       @jesthebes At least your lawn hasn't been take...  \n",
              "3       umm.. like, hello? where's the child support p...  \n",
              "4                                         Joined twitter   \n",
              "...                                                   ...  \n",
              "999995  this song's middle change just doesn't want to...  \n",
              "999996               @officialnjonas Good luck with that   \n",
              "999997           @ProudGamerTweet I rather average 32370   \n",
              "999998  Pickin up @misstinayao waitin on @sadittysash ...  \n",
              "999999  @ home studying for maths wooot ! im so going ...  \n",
              "\n",
              "[999340 rows x 5 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a852aa2d-c25c-440e-8163-91bde70267ec",
      "metadata": {
        "id": "a852aa2d-c25c-440e-8163-91bde70267ec",
        "tags": []
      },
      "source": [
        " ## Text cultivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39005a16-b214-44c3-be10-6cd6d25ab1ab",
      "metadata": {
        "id": "39005a16-b214-44c3-be10-6cd6d25ab1ab",
        "tags": []
      },
      "outputs": [],
      "source": [
        "standard_stopwords = [\n",
        "    'a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at',\n",
        "    'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'by', 'can', 'd', 'did',\n",
        "    'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has',\n",
        "    'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if',\n",
        "    'in', 'into', 'is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'more', 'most', 'my',\n",
        "    'myself', 'now', 'o', 'of', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out',\n",
        "    'own', 're', 's', 'same', 'she', \"shes\", 'should', \"shouldve\", 'so', 'some', 'such', 't', 'than',\n",
        "    'that', \"thatll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they',\n",
        "    'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'we', 'were',\n",
        "    'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', 'y', 'you',\n",
        "    \"youd\", \"youll\", \"youre\", \"youve\", 'your', 'yours', 'yourself', 'yourselves'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e640c61-3725-4148-965d-cf8e0f28bd42",
      "metadata": {
        "id": "0e640c61-3725-4148-965d-cf8e0f28bd42",
        "tags": []
      },
      "outputs": [],
      "source": [
        "twitter_stopwords = [\n",
        "    'rt', 'via', 'http', 'https', 'www', 'u', 'us', 'im', 'dont', 'ive', 'youre', 'amp',\n",
        "    '@', '#', '&'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b17e3dc-5430-4d9b-abce-d09e46e7bfdc",
      "metadata": {
        "id": "2b17e3dc-5430-4d9b-abce-d09e46e7bfdc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "custom_stopwords = set(standard_stopwords + twitter_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bcdda41-cff9-4d00-81f1-668d689b84bb",
      "metadata": {
        "id": "3bcdda41-cff9-4d00-81f1-668d689b84bb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "important_words = {'not', 'no', 'very', 'xoxo', 'lol', 'omg', 'thx', 'haha'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1caea2b1-18fa-495d-a1e9-41843309c061",
      "metadata": {
        "id": "1caea2b1-18fa-495d-a1e9-41843309c061",
        "tags": []
      },
      "outputs": [],
      "source": [
        "custom_stopwords = custom_stopwords - important_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "106732d1-a834-4759-9249-3e222353f75c",
      "metadata": {
        "id": "106732d1-a834-4759-9249-3e222353f75c"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d7893b-a042-49c8-962f-6a23464ede61",
      "metadata": {
        "id": "b5d7893b-a042-49c8-962f-6a23464ede61",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def preprocess_tweet(tweet, stopwords):\n",
        "    # Remove URLs\n",
        "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
        "    # Remove user @ references\n",
        "    tweet = re.sub(r'\\@\\w+', '', tweet)\n",
        "    # Tokenize tweet\n",
        "    words = tweet.split()\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word.lower() not in stopwords]\n",
        "    return ' '.join(words) if words else \"EMPTY_TEXT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce3b8a5-80c4-417e-9f9b-d289c83e007d",
      "metadata": {
        "id": "1ce3b8a5-80c4-417e-9f9b-d289c83e007d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df['Cleaned_Text'] = df['Text'].apply(lambda x: preprocess_tweet(x, custom_stopwords))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DnfG9ZyPMM94",
      "metadata": {
        "id": "DnfG9ZyPMM94"
      },
      "source": [
        "## Fix double encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a044789c-d1b3-463b-909a-3b7d28f1a9fd",
      "metadata": {
        "id": "a044789c-d1b3-463b-909a-3b7d28f1a9fd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def fix_double_encoding(text):\n",
        "    try:\n",
        "        # Encode the incorrectly decoded string back to bytes using 'latin1'\n",
        "        byte_text = text.encode('latin1')\n",
        "        # Decode it correctly using 'utf-8' with error handling\n",
        "        return byte_text.decode('utf-8', errors='replace')\n",
        "    except UnicodeEncodeError:\n",
        "        return text if text else 'EMPTY_TEXT'\n",
        "\n",
        "# Apply the function to fix encoding\n",
        "df['Fixed_Text'] = df['Cleaned_Text'].apply(fix_double_encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023353a4-4d0b-4d39-91ad-243652219bf6",
      "metadata": {
        "id": "023353a4-4d0b-4d39-91ad-243652219bf6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_cleaned = df[['labels','id','Date','User', 'Fixed_Text']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b67f5ea-7523-4863-8cfc-6741445ab896",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1b67f5ea-7523-4863-8cfc-6741445ab896",
        "outputId": "88844645-90ed-48ec-d652-86ac8acc2d92",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>Date</th>\n",
              "      <th>User</th>\n",
              "      <th>Fixed_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>2000548391</td>\n",
              "      <td>Mon Jun 01 22:22:01 PDT 2009</td>\n",
              "      <td>ticia42</td>\n",
              "      <td>can't open door herself, think feels pain would.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2191932827</td>\n",
              "      <td>Tue Jun 16 06:13:30 PDT 2009</td>\n",
              "      <td>lhotfoot</td>\n",
              "      <td>EMPTY_TEXT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>1754199174</td>\n",
              "      <td>Sun May 10 05:23:02 PDT 2009</td>\n",
              "      <td>hockeycrew</td>\n",
              "      <td>least lawn hasn't taken over field weeds!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1994056674</td>\n",
              "      <td>Mon Jun 01 11:20:41 PDT 2009</td>\n",
              "      <td>GeoBlack_Cat</td>\n",
              "      <td>umm.. like, hello? where's child support payme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1980150068</td>\n",
              "      <td>Sun May 31 05:51:31 PDT 2009</td>\n",
              "      <td>rawrcelne</td>\n",
              "      <td>Joined twitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999995</th>\n",
              "      <td>0</td>\n",
              "      <td>1985361990</td>\n",
              "      <td>Sun May 31 16:57:39 PDT 2009</td>\n",
              "      <td>lutheasalom</td>\n",
              "      <td>song's middle change doesn't want born..... ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999996</th>\n",
              "      <td>4</td>\n",
              "      <td>2057029784</td>\n",
              "      <td>Sat Jun 06 12:14:24 PDT 2009</td>\n",
              "      <td>beeluz</td>\n",
              "      <td>Good luck</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999997</th>\n",
              "      <td>0</td>\n",
              "      <td>1835639354</td>\n",
              "      <td>Mon May 18 06:26:21 PDT 2009</td>\n",
              "      <td>lordmuttley</td>\n",
              "      <td>rather average 32370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999998</th>\n",
              "      <td>0</td>\n",
              "      <td>2246780174</td>\n",
              "      <td>Fri Jun 19 18:06:46 PDT 2009</td>\n",
              "      <td>MizSadittyFancy</td>\n",
              "      <td>Pickin waitin 2 hurry up...I odeeee missed dem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999999</th>\n",
              "      <td>0</td>\n",
              "      <td>1833617173</td>\n",
              "      <td>Sun May 17 23:52:31 PDT 2009</td>\n",
              "      <td>dindahh</td>\n",
              "      <td>home studying maths wooot ! going fail shit</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>999340 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        labels          id                          Date             User  \\\n",
              "0            4  2000548391  Mon Jun 01 22:22:01 PDT 2009          ticia42   \n",
              "1            0  2191932827  Tue Jun 16 06:13:30 PDT 2009         lhotfoot   \n",
              "2            4  1754199174  Sun May 10 05:23:02 PDT 2009       hockeycrew   \n",
              "3            0  1994056674  Mon Jun 01 11:20:41 PDT 2009     GeoBlack_Cat   \n",
              "4            4  1980150068  Sun May 31 05:51:31 PDT 2009        rawrcelne   \n",
              "...        ...         ...                           ...              ...   \n",
              "999995       0  1985361990  Sun May 31 16:57:39 PDT 2009      lutheasalom   \n",
              "999996       4  2057029784  Sat Jun 06 12:14:24 PDT 2009           beeluz   \n",
              "999997       0  1835639354  Mon May 18 06:26:21 PDT 2009      lordmuttley   \n",
              "999998       0  2246780174  Fri Jun 19 18:06:46 PDT 2009  MizSadittyFancy   \n",
              "999999       0  1833617173  Sun May 17 23:52:31 PDT 2009          dindahh   \n",
              "\n",
              "                                               Fixed_Text  \n",
              "0        can't open door herself, think feels pain would.  \n",
              "1                                              EMPTY_TEXT  \n",
              "2               least lawn hasn't taken over field weeds!  \n",
              "3       umm.. like, hello? where's child support payme...  \n",
              "4                                          Joined twitter  \n",
              "...                                                   ...  \n",
              "999995  song's middle change doesn't want born..... ar...  \n",
              "999996                                          Good luck  \n",
              "999997                               rather average 32370  \n",
              "999998  Pickin waitin 2 hurry up...I odeeee missed dem...  \n",
              "999999        home studying maths wooot ! going fail shit  \n",
              "\n",
              "[999340 rows x 5 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab6db5a-8ff8-4eee-953b-1279f514f15e",
      "metadata": {
        "id": "3ab6db5a-8ff8-4eee-953b-1279f514f15e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "empty_tweet = df_cleaned[df_cleaned[\"Fixed_Text\"] == \"EMPTY_TEXT\"]\n",
        "df_cleaned.drop(empty_tweet.index,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a37ecda-9e55-404f-8f6a-75e11c0cd59d",
      "metadata": {
        "id": "6a37ecda-9e55-404f-8f6a-75e11c0cd59d",
        "tags": []
      },
      "source": [
        "## Emoji extraction Note: This part was not used for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d0a21a4-ae79-425b-bff7-bff0bd62fe49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d0a21a4-ae79-425b-bff7-bff0bd62fe49",
        "outputId": "a3d6bb28-1548-4e79-e21c-d1b2b479b537",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(996715, 5)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8f2837-7ec6-4d5f-8637-55444dcff390",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c8f2837-7ec6-4d5f-8637-55444dcff390",
        "outputId": "487bf465-9ca2-422d-931a-b9b388d7babd",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (2.12.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from emoji) (4.11.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b0e5dc-9d2e-4f68-8c53-e341644f3f41",
      "metadata": {
        "id": "30b0e5dc-9d2e-4f68-8c53-e341644f3f41",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "\n",
        "def extract_emojis(text):\n",
        "    return ' '.join(c for c in text if emoji.is_emoji(c))\n",
        "\n",
        "# Apply the function to extract emojis\n",
        "df_cleaned['Emojis'] = df_cleaned['Fixed_Text'].apply(extract_emojis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb222cd-9a7d-42e6-8b6a-ae45076bf52f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ceb222cd-9a7d-42e6-8b6a-ae45076bf52f",
        "outputId": "1fd12de6-ca48-42d3-b96b-c04c6ba2f7ac",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>Date</th>\n",
              "      <th>User</th>\n",
              "      <th>Fixed_Text</th>\n",
              "      <th>Emojis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>2000548391</td>\n",
              "      <td>Mon Jun 01 22:22:01 PDT 2009</td>\n",
              "      <td>ticia42</td>\n",
              "      <td>can't open door herself, think feels pain would.</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>1754199174</td>\n",
              "      <td>Sun May 10 05:23:02 PDT 2009</td>\n",
              "      <td>hockeycrew</td>\n",
              "      <td>least lawn hasn't taken over field weeds!</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1994056674</td>\n",
              "      <td>Mon Jun 01 11:20:41 PDT 2009</td>\n",
              "      <td>GeoBlack_Cat</td>\n",
              "      <td>umm.. like, hello? where's child support payme...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1980150068</td>\n",
              "      <td>Sun May 31 05:51:31 PDT 2009</td>\n",
              "      <td>rawrcelne</td>\n",
              "      <td>Joined twitter</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>2242922534</td>\n",
              "      <td>Fri Jun 19 12:48:06 PDT 2009</td>\n",
              "      <td>Whacky</td>\n",
              "      <td>Gayle wrong guy wrong team much like Brian Lar...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999995</th>\n",
              "      <td>0</td>\n",
              "      <td>1985361990</td>\n",
              "      <td>Sun May 31 16:57:39 PDT 2009</td>\n",
              "      <td>lutheasalom</td>\n",
              "      <td>song's middle change doesn't want born..... ar...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999996</th>\n",
              "      <td>4</td>\n",
              "      <td>2057029784</td>\n",
              "      <td>Sat Jun 06 12:14:24 PDT 2009</td>\n",
              "      <td>beeluz</td>\n",
              "      <td>Good luck</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999997</th>\n",
              "      <td>0</td>\n",
              "      <td>1835639354</td>\n",
              "      <td>Mon May 18 06:26:21 PDT 2009</td>\n",
              "      <td>lordmuttley</td>\n",
              "      <td>rather average 32370</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999998</th>\n",
              "      <td>0</td>\n",
              "      <td>2246780174</td>\n",
              "      <td>Fri Jun 19 18:06:46 PDT 2009</td>\n",
              "      <td>MizSadittyFancy</td>\n",
              "      <td>Pickin waitin 2 hurry up...I odeeee missed dem...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999999</th>\n",
              "      <td>0</td>\n",
              "      <td>1833617173</td>\n",
              "      <td>Sun May 17 23:52:31 PDT 2009</td>\n",
              "      <td>dindahh</td>\n",
              "      <td>home studying maths wooot ! going fail shit</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>996715 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        labels          id                          Date             User  \\\n",
              "0            4  2000548391  Mon Jun 01 22:22:01 PDT 2009          ticia42   \n",
              "2            4  1754199174  Sun May 10 05:23:02 PDT 2009       hockeycrew   \n",
              "3            0  1994056674  Mon Jun 01 11:20:41 PDT 2009     GeoBlack_Cat   \n",
              "4            4  1980150068  Sun May 31 05:51:31 PDT 2009        rawrcelne   \n",
              "5            0  2242922534  Fri Jun 19 12:48:06 PDT 2009           Whacky   \n",
              "...        ...         ...                           ...              ...   \n",
              "999995       0  1985361990  Sun May 31 16:57:39 PDT 2009      lutheasalom   \n",
              "999996       4  2057029784  Sat Jun 06 12:14:24 PDT 2009           beeluz   \n",
              "999997       0  1835639354  Mon May 18 06:26:21 PDT 2009      lordmuttley   \n",
              "999998       0  2246780174  Fri Jun 19 18:06:46 PDT 2009  MizSadittyFancy   \n",
              "999999       0  1833617173  Sun May 17 23:52:31 PDT 2009          dindahh   \n",
              "\n",
              "                                               Fixed_Text Emojis  \n",
              "0        can't open door herself, think feels pain would.         \n",
              "2               least lawn hasn't taken over field weeds!         \n",
              "3       umm.. like, hello? where's child support payme...         \n",
              "4                                          Joined twitter         \n",
              "5       Gayle wrong guy wrong team much like Brian Lar...         \n",
              "...                                                   ...    ...  \n",
              "999995  song's middle change doesn't want born..... ar...         \n",
              "999996                                          Good luck         \n",
              "999997                               rather average 32370         \n",
              "999998  Pickin waitin 2 hurry up...I odeeee missed dem...         \n",
              "999999        home studying maths wooot ! going fail shit         \n",
              "\n",
              "[996715 rows x 6 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aecd007-ba12-4d14-b267-c3e215b8fe73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4aecd007-ba12-4d14-b267-c3e215b8fe73",
        "outputId": "7c6400b8-7380-4dd4-8e2b-31039c29a099",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>Date</th>\n",
              "      <th>User</th>\n",
              "      <th>Fixed_Text</th>\n",
              "      <th>Emojis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10813</th>\n",
              "      <td>0</td>\n",
              "      <td>2325953939</td>\n",
              "      <td>Thu Jun 25 06:19:48 PDT 2009</td>\n",
              "      <td>AlmaVienna</td>\n",
              "      <td>ps. wrote message myspace ;) But didnt wrote y...</td>\n",
              "      <td>♥</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16879</th>\n",
              "      <td>0</td>\n",
              "      <td>2196976038</td>\n",
              "      <td>Tue Jun 16 13:10:38 PDT 2009</td>\n",
              "      <td>NileyLovers</td>\n",
              "      <td>hey back not back.. maybe internets off.. miss...</td>\n",
              "      <td>♥</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30765</th>\n",
              "      <td>0</td>\n",
              "      <td>2256398984</td>\n",
              "      <td>Sat Jun 20 12:42:54 PDT 2009</td>\n",
              "      <td>LittleFloWer17</td>\n",
              "      <td>(: ♥ ♥ ...love not bad but it's little difficu...</td>\n",
              "      <td>♥ ♥ ♥ ♥</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34239</th>\n",
              "      <td>0</td>\n",
              "      <td>2321444599</td>\n",
              "      <td>Wed Jun 24 21:03:55 PDT 2009</td>\n",
              "      <td>jesslee1331</td>\n",
              "      <td>good luck tonight =D♥ wish competitions coz wo...</td>\n",
              "      <td>♥</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39762</th>\n",
              "      <td>0</td>\n",
              "      <td>2071449813</td>\n",
              "      <td>Sun Jun 07 19:24:59 PDT 2009</td>\n",
              "      <td>trashii</td>\n",
              "      <td>Wish weekend didn't end quickly lovelovelove♥ ...</td>\n",
              "      <td>♥</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>981578</th>\n",
              "      <td>0</td>\n",
              "      <td>2068113481</td>\n",
              "      <td>Sun Jun 07 13:31:17 PDT 2009</td>\n",
              "      <td>krissydietz</td>\n",
              "      <td>go off, honey enough ! see tomorrow ! love ♥ t...</td>\n",
              "      <td>♥</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982071</th>\n",
              "      <td>0</td>\n",
              "      <td>1997803602</td>\n",
              "      <td>Mon Jun 01 17:29:09 PDT 2009</td>\n",
              "      <td>rebeurka34</td>\n",
              "      <td>wish didn't say goodbye 2 months wcs ♥</td>\n",
              "      <td>♥</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983550</th>\n",
              "      <td>0</td>\n",
              "      <td>2252191918</td>\n",
              "      <td>Sat Jun 20 05:41:26 PDT 2009</td>\n",
              "      <td>x_chiquita_x</td>\n",
              "      <td>going back vienna now... can't wait see roomma...</td>\n",
              "      <td>♥</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994612</th>\n",
              "      <td>0</td>\n",
              "      <td>2322471006</td>\n",
              "      <td>Wed Jun 24 22:40:34 PDT 2009</td>\n",
              "      <td>sierrabardot</td>\n",
              "      <td>saved life. taught life lessons. yet i've stil...</td>\n",
              "      <td>☮ ♥</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996642</th>\n",
              "      <td>0</td>\n",
              "      <td>2262215253</td>\n",
              "      <td>Sat Jun 20 22:05:31 PDT 2009</td>\n",
              "      <td>anicole2009</td>\n",
              "      <td>I'm Finally 18 Babes!!!! ♥ ;) Drinks Got Stoma...</td>\n",
              "      <td>♥</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>315 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        labels          id                          Date            User  \\\n",
              "10813        0  2325953939  Thu Jun 25 06:19:48 PDT 2009      AlmaVienna   \n",
              "16879        0  2196976038  Tue Jun 16 13:10:38 PDT 2009     NileyLovers   \n",
              "30765        0  2256398984  Sat Jun 20 12:42:54 PDT 2009  LittleFloWer17   \n",
              "34239        0  2321444599  Wed Jun 24 21:03:55 PDT 2009     jesslee1331   \n",
              "39762        0  2071449813  Sun Jun 07 19:24:59 PDT 2009         trashii   \n",
              "...        ...         ...                           ...             ...   \n",
              "981578       0  2068113481  Sun Jun 07 13:31:17 PDT 2009     krissydietz   \n",
              "982071       0  1997803602  Mon Jun 01 17:29:09 PDT 2009      rebeurka34   \n",
              "983550       0  2252191918  Sat Jun 20 05:41:26 PDT 2009    x_chiquita_x   \n",
              "994612       0  2322471006  Wed Jun 24 22:40:34 PDT 2009    sierrabardot   \n",
              "996642       0  2262215253  Sat Jun 20 22:05:31 PDT 2009     anicole2009   \n",
              "\n",
              "                                               Fixed_Text   Emojis  \n",
              "10813   ps. wrote message myspace ;) But didnt wrote y...        ♥  \n",
              "16879   hey back not back.. maybe internets off.. miss...        ♥  \n",
              "30765   (: ♥ ♥ ...love not bad but it's little difficu...  ♥ ♥ ♥ ♥  \n",
              "34239   good luck tonight =D♥ wish competitions coz wo...        ♥  \n",
              "39762   Wish weekend didn't end quickly lovelovelove♥ ...        ♥  \n",
              "...                                                   ...      ...  \n",
              "981578  go off, honey enough ! see tomorrow ! love ♥ t...        ♥  \n",
              "982071             wish didn't say goodbye 2 months wcs ♥        ♥  \n",
              "983550  going back vienna now... can't wait see roomma...        ♥  \n",
              "994612  saved life. taught life lessons. yet i've stil...      ☮ ♥  \n",
              "996642  I'm Finally 18 Babes!!!! ♥ ;) Drinks Got Stoma...        ♥  \n",
              "\n",
              "[315 rows x 6 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned[(df_cleaned['Emojis'] != \"\") & (df_cleaned['labels']==0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZTJwemNpQElp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTJwemNpQElp",
        "outputId": "118fe343-781d-4881-a3eb-863d6ac57923"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10813    ps. wrote message myspace ;) But didnt wrote yet Much Love,xoxo♥Alma\n",
              "Name: Fixed_Text, dtype: object"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "df_cleaned['Fixed_Text'][ df_cleaned['id'] == 2325953939]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QuomyyvXMulA",
      "metadata": {
        "id": "QuomyyvXMulA"
      },
      "source": [
        "## Remove tweets with garbled text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee90d6e-b307-48f3-bfb6-0e87bef87c87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "eee90d6e-b307-48f3-bfb6-0e87bef87c87",
        "outputId": "9ea031be-4ad5-44a5-8f9a-107f4adb6d70",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>Date</th>\n",
              "      <th>User</th>\n",
              "      <th>Fixed_Text</th>\n",
              "      <th>Emojis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10157</th>\n",
              "      <td>4</td>\n",
              "      <td>2001157680</td>\n",
              "      <td>Tue Jun 02 00:00:45 PDT 2009</td>\n",
              "      <td>d1g_cartoon</td>\n",
              "      <td>الضرورات تبيح ال� حظورات: الضرورات تبيح ال� حظورات ...Author: أ� ير الا� ة Added: الثلاثاء, 02 يونيو, 2009 08..</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       labels          id                          Date         User  \\\n",
              "10157       4  2001157680  Tue Jun 02 00:00:45 PDT 2009  d1g_cartoon   \n",
              "\n",
              "                                                                                                            Fixed_Text  \\\n",
              "10157  الضرورات تبيح ال� حظورات: الضرورات تبيح ال� حظورات ...Author: أ� ير الا� ة Added: الثلاثاء, 02 يونيو, 2009 08..   \n",
              "\n",
              "      Emojis  \n",
              "10157         "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned[df_cleaned['id'] == 2001157680]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cP73JD2iMzux",
      "metadata": {
        "id": "cP73JD2iMzux"
      },
      "outputs": [],
      "source": [
        "## Check garbled text in DF\n",
        "\n",
        "def contains_non_ascii(text):\n",
        "    try:\n",
        "        text.encode('ascii')\n",
        "    except UnicodeEncodeError as e:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "df_cleaned['ContainsGarbledText'] = df_cleaned['Fixed_Text'].apply(contains_non_ascii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ruVs39tWM6Xr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "ruVs39tWM6Xr",
        "outputId": "3b9cdd09-faee-402a-81b6-c70c08184584"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>Date</th>\n",
              "      <th>User</th>\n",
              "      <th>Fixed_Text</th>\n",
              "      <th>Emojis</th>\n",
              "      <th>ContainsGarbledText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>1990245213</td>\n",
              "      <td>Mon Jun 01 04:19:15 PDT 2009</td>\n",
              "      <td>jedbackhouse</td>\n",
              "      <td>today's agenda... - ♦ food town ♦ 1.15pm concert Uni ♦ Greenhead park w/mates &amp;amp; football ♦ drinks in...</td>\n",
              "      <td>♦ ♦ ♦ ♦</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>4</td>\n",
              "      <td>2176385388</td>\n",
              "      <td>Mon Jun 15 03:43:49 PDT 2009</td>\n",
              "      <td>Omertoso</td>\n",
              "      <td>#musicmonday Time - John Cena ♫ ♪ ♫ Rafaga ♪ ♫ ♪</td>\n",
              "      <td></td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>4</td>\n",
              "      <td>1694108452</td>\n",
              "      <td>Mon May 04 01:07:19 PDT 2009</td>\n",
              "      <td>bubbameadows</td>\n",
              "      <td>Video: Today�s video blog/vlog�thing.</td>\n",
              "      <td></td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>4</td>\n",
              "      <td>1997694855</td>\n",
              "      <td>Mon Jun 01 17:17:50 PDT 2009</td>\n",
              "      <td>lelialinden</td>\n",
              "      <td>japanese restaraunt, drink saquê, order HAHA</td>\n",
              "      <td></td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>0</td>\n",
              "      <td>1966179531</td>\n",
              "      <td>Fri May 29 17:16:29 PDT 2009</td>\n",
              "      <td>FindingDani</td>\n",
              "      <td>Hey boys, wanna see teeth? (damn it, twitter don�t wanna change picture)</td>\n",
              "      <td></td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999624</th>\n",
              "      <td>4</td>\n",
              "      <td>2175611191</td>\n",
              "      <td>Mon Jun 15 01:28:56 PDT 2009</td>\n",
              "      <td>jemappellekim</td>\n",
              "      <td>Best cover version ever ♫</td>\n",
              "      <td></td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999736</th>\n",
              "      <td>0</td>\n",
              "      <td>2204878136</td>\n",
              "      <td>Wed Jun 17 03:22:19 PDT 2009</td>\n",
              "      <td>LLinae</td>\n",
              "      <td>ended even started. - wont long. 30th June, 13 days. I’m scared be...</td>\n",
              "      <td></td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999822</th>\n",
              "      <td>4</td>\n",
              "      <td>1989354486</td>\n",
              "      <td>Mon Jun 01 01:14:01 PDT 2009</td>\n",
              "      <td>jangles</td>\n",
              "      <td>song help ♫</td>\n",
              "      <td></td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999856</th>\n",
              "      <td>4</td>\n",
              "      <td>2014715682</td>\n",
              "      <td>Wed Jun 03 02:26:53 PDT 2009</td>\n",
              "      <td>lenseffect</td>\n",
              "      <td>а по радиото какви �?а, аууу. ..</td>\n",
              "      <td></td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999866</th>\n",
              "      <td>0</td>\n",
              "      <td>2231413583</td>\n",
              "      <td>Thu Jun 18 18:30:45 PDT 2009</td>\n",
              "      <td>jackalstudio</td>\n",
              "      <td>ok tethering doesnt work iphone Thanx Vodafone è_é</td>\n",
              "      <td></td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9086 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        labels          id                          Date           User  \\\n",
              "7            4  1990245213  Mon Jun 01 04:19:15 PDT 2009   jedbackhouse   \n",
              "64           4  2176385388  Mon Jun 15 03:43:49 PDT 2009       Omertoso   \n",
              "136          4  1694108452  Mon May 04 01:07:19 PDT 2009   bubbameadows   \n",
              "312          4  1997694855  Mon Jun 01 17:17:50 PDT 2009    lelialinden   \n",
              "359          0  1966179531  Fri May 29 17:16:29 PDT 2009    FindingDani   \n",
              "...        ...         ...                           ...            ...   \n",
              "999624       4  2175611191  Mon Jun 15 01:28:56 PDT 2009  jemappellekim   \n",
              "999736       0  2204878136  Wed Jun 17 03:22:19 PDT 2009         LLinae   \n",
              "999822       4  1989354486  Mon Jun 01 01:14:01 PDT 2009        jangles   \n",
              "999856       4  2014715682  Wed Jun 03 02:26:53 PDT 2009     lenseffect   \n",
              "999866       0  2231413583  Thu Jun 18 18:30:45 PDT 2009   jackalstudio   \n",
              "\n",
              "                                                                                                         Fixed_Text  \\\n",
              "7       today's agenda... - ♦ food town ♦ 1.15pm concert Uni ♦ Greenhead park w/mates &amp; football ♦ drinks in...   \n",
              "64                                                                 #musicmonday Time - John Cena ♫ ♪ ♫ Rafaga ♪ ♫ ♪   \n",
              "136                                                                           Video: Today�s video blog/vlog�thing.   \n",
              "312                                                                    japanese restaraunt, drink saquê, order HAHA   \n",
              "359                                        Hey boys, wanna see teeth? (damn it, twitter don�t wanna change picture)   \n",
              "...                                                                                                             ...   \n",
              "999624                                                                                    Best cover version ever ♫   \n",
              "999736                                        ended even started. - wont long. 30th June, 13 days. I’m scared be...   \n",
              "999822                                                                                                  song help ♫   \n",
              "999856                                                                             а по радиото какви �?а, аууу. ..   \n",
              "999866                                                           ok tethering doesnt work iphone Thanx Vodafone è_é   \n",
              "\n",
              "         Emojis  ContainsGarbledText  \n",
              "7       ♦ ♦ ♦ ♦                 True  \n",
              "64                              True  \n",
              "136                             True  \n",
              "312                             True  \n",
              "359                             True  \n",
              "...         ...                  ...  \n",
              "999624                          True  \n",
              "999736                          True  \n",
              "999822                          True  \n",
              "999856                          True  \n",
              "999866                          True  \n",
              "\n",
              "[9086 rows x 7 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned[df_cleaned['ContainsGarbledText']== True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "avqEPZn9NFoz",
      "metadata": {
        "id": "avqEPZn9NFoz"
      },
      "outputs": [],
      "source": [
        "df_cleaned.drop(df_cleaned[df_cleaned['ContainsGarbledText'] == True].index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R_JdMihcNIeS",
      "metadata": {
        "id": "R_JdMihcNIeS"
      },
      "outputs": [],
      "source": [
        "df_cleaned.drop(columns=['ContainsGarbledText'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OAuo9Wg3NM-i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "OAuo9Wg3NM-i",
        "outputId": "0d7fcd6f-4837-47aa-a4b2-606980c300b9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>Date</th>\n",
              "      <th>User</th>\n",
              "      <th>Fixed_Text</th>\n",
              "      <th>Emojis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>2000548391</td>\n",
              "      <td>Mon Jun 01 22:22:01 PDT 2009</td>\n",
              "      <td>ticia42</td>\n",
              "      <td>can't open door herself, think feels pain would.</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>1754199174</td>\n",
              "      <td>Sun May 10 05:23:02 PDT 2009</td>\n",
              "      <td>hockeycrew</td>\n",
              "      <td>least lawn hasn't taken over field weeds!</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1994056674</td>\n",
              "      <td>Mon Jun 01 11:20:41 PDT 2009</td>\n",
              "      <td>GeoBlack_Cat</td>\n",
              "      <td>umm.. like, hello? where's child support payment? Hope ex still gainfully employed - many people know currently not</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1980150068</td>\n",
              "      <td>Sun May 31 05:51:31 PDT 2009</td>\n",
              "      <td>rawrcelne</td>\n",
              "      <td>Joined twitter</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>2242922534</td>\n",
              "      <td>Fri Jun 19 12:48:06 PDT 2009</td>\n",
              "      <td>Whacky</td>\n",
              "      <td>Gayle wrong guy wrong team much like Brian Lara #t20wc</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999995</th>\n",
              "      <td>0</td>\n",
              "      <td>1985361990</td>\n",
              "      <td>Sun May 31 16:57:39 PDT 2009</td>\n",
              "      <td>lutheasalom</td>\n",
              "      <td>song's middle change doesn't want born..... arghhhh!!</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999996</th>\n",
              "      <td>4</td>\n",
              "      <td>2057029784</td>\n",
              "      <td>Sat Jun 06 12:14:24 PDT 2009</td>\n",
              "      <td>beeluz</td>\n",
              "      <td>Good luck</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999997</th>\n",
              "      <td>0</td>\n",
              "      <td>1835639354</td>\n",
              "      <td>Mon May 18 06:26:21 PDT 2009</td>\n",
              "      <td>lordmuttley</td>\n",
              "      <td>rather average 32370</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999998</th>\n",
              "      <td>0</td>\n",
              "      <td>2246780174</td>\n",
              "      <td>Fri Jun 19 18:06:46 PDT 2009</td>\n",
              "      <td>MizSadittyFancy</td>\n",
              "      <td>Pickin waitin 2 hurry up...I odeeee missed dem Table talk 2nite...LOL bout fat...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999999</th>\n",
              "      <td>0</td>\n",
              "      <td>1833617173</td>\n",
              "      <td>Sun May 17 23:52:31 PDT 2009</td>\n",
              "      <td>dindahh</td>\n",
              "      <td>home studying maths wooot ! going fail shit</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>987629 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        labels          id                          Date             User  \\\n",
              "0            4  2000548391  Mon Jun 01 22:22:01 PDT 2009          ticia42   \n",
              "2            4  1754199174  Sun May 10 05:23:02 PDT 2009       hockeycrew   \n",
              "3            0  1994056674  Mon Jun 01 11:20:41 PDT 2009     GeoBlack_Cat   \n",
              "4            4  1980150068  Sun May 31 05:51:31 PDT 2009        rawrcelne   \n",
              "5            0  2242922534  Fri Jun 19 12:48:06 PDT 2009           Whacky   \n",
              "...        ...         ...                           ...              ...   \n",
              "999995       0  1985361990  Sun May 31 16:57:39 PDT 2009      lutheasalom   \n",
              "999996       4  2057029784  Sat Jun 06 12:14:24 PDT 2009           beeluz   \n",
              "999997       0  1835639354  Mon May 18 06:26:21 PDT 2009      lordmuttley   \n",
              "999998       0  2246780174  Fri Jun 19 18:06:46 PDT 2009  MizSadittyFancy   \n",
              "999999       0  1833617173  Sun May 17 23:52:31 PDT 2009          dindahh   \n",
              "\n",
              "                                                                                                                 Fixed_Text  \\\n",
              "0                                                                          can't open door herself, think feels pain would.   \n",
              "2                                                                                 least lawn hasn't taken over field weeds!   \n",
              "3       umm.. like, hello? where's child support payment? Hope ex still gainfully employed - many people know currently not   \n",
              "4                                                                                                            Joined twitter   \n",
              "5                                                                    Gayle wrong guy wrong team much like Brian Lara #t20wc   \n",
              "...                                                                                                                     ...   \n",
              "999995                                                                song's middle change doesn't want born..... arghhhh!!   \n",
              "999996                                                                                                            Good luck   \n",
              "999997                                                                                                 rather average 32370   \n",
              "999998                                    Pickin waitin 2 hurry up...I odeeee missed dem Table talk 2nite...LOL bout fat...   \n",
              "999999                                                                          home studying maths wooot ! going fail shit   \n",
              "\n",
              "       Emojis  \n",
              "0              \n",
              "2              \n",
              "3              \n",
              "4              \n",
              "5              \n",
              "...       ...  \n",
              "999995         \n",
              "999996         \n",
              "999997         \n",
              "999998         \n",
              "999999         \n",
              "\n",
              "[987629 rows x 6 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Au_ZY46gNQPC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au_ZY46gNQPC",
        "outputId": "0e541e86-0958-49b4-9929-605c4b8304bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(987629, 6)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf86db9a-906d-4a46-9b39-f9e092640d6b",
      "metadata": {
        "id": "cf86db9a-906d-4a46-9b39-f9e092640d6b"
      },
      "source": [
        "## Extract necessary information from Date: This part was not used for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7516ca-0bff-4fbd-9363-742aa712d8cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d7516ca-0bff-4fbd-9363-742aa712d8cd",
        "outputId": "7c091627-2075-42f0-96dc-7f3c6b7dccb2",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         Mon Jun 01 22:22:01 PDT 2009\n",
              "1         Tue Jun 16 06:13:30 PDT 2009\n",
              "2         Sun May 10 05:23:02 PDT 2009\n",
              "3         Mon Jun 01 11:20:41 PDT 2009\n",
              "4         Sun May 31 05:51:31 PDT 2009\n",
              "                      ...             \n",
              "999995    Sun May 31 16:57:39 PDT 2009\n",
              "999996    Sat Jun 06 12:14:24 PDT 2009\n",
              "999997    Mon May 18 06:26:21 PDT 2009\n",
              "999998    Fri Jun 19 18:06:46 PDT 2009\n",
              "999999    Sun May 17 23:52:31 PDT 2009\n",
              "Name: Date, Length: 999340, dtype: object"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee225fc9-1042-46e1-8d30-a5f821200697",
      "metadata": {
        "id": "ee225fc9-1042-46e1-8d30-a5f821200697",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from dateutil import parser\n",
        "from dateutil.tz import gettz\n",
        "\n",
        "# Define the timezone mapping\n",
        "tzinfos = {\n",
        "    'PDT': gettz('America/Los_Angeles')\n",
        "}\n",
        "\n",
        "# Convert the 'Datetime' column to datetime with timezone awareness\n",
        "df_cleaned['Date'] = df_cleaned['Date'].apply(lambda x: parser.parse(x, tzinfos=tzinfos))\n",
        "\n",
        "\n",
        "\n",
        "#### Error will be fixed after kernel restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8135c641-74b4-4ce5-807f-304bfcf1fd04",
      "metadata": {
        "id": "8135c641-74b4-4ce5-807f-304bfcf1fd04",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Extract features\n",
        "df_cleaned['Year'] = df_cleaned['Date'].dt.year\n",
        "df_cleaned['Month'] = df_cleaned['Date'].dt.month\n",
        "df_cleaned['Day'] = df_cleaned['Date'].dt.day\n",
        "df_cleaned['Hour'] = df_cleaned['Date'].dt.hour\n",
        "df_cleaned['Minute'] = df_cleaned['Date'].dt.minute\n",
        "df_cleaned['Second'] = df_cleaned['Date'].dt.second\n",
        "df_cleaned['DayOfWeek'] = df_cleaned['Date'].dt.dayofweek\n",
        "df_cleaned['IsWeekend'] = df_cleaned['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)  # Weekend if Saturday(5) or Sunday(6)\n",
        "df_cleaned['TimeOfDay'] = df_cleaned['Hour'].apply(lambda x: 'Night' if 0 <= x < 6 else 'Morning' if 6 <= x < 12 else 'Afternoon' if 12 <= x < 18 else 'Evening')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcb1a1ab-651a-4d2c-a5f3-f8a78513747c",
      "metadata": {
        "id": "fcb1a1ab-651a-4d2c-a5f3-f8a78513747c"
      },
      "source": [
        "DayOfWeek 0 -> Monday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6772f67-73f6-40b9-b511-e31815421b8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "c6772f67-73f6-40b9-b511-e31815421b8b",
        "outputId": "70efea58-cbf0-4ae7-b7f2-42410d02a87a",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>Date</th>\n",
              "      <th>User</th>\n",
              "      <th>Fixed_Text</th>\n",
              "      <th>Emojis</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Day</th>\n",
              "      <th>Hour</th>\n",
              "      <th>Minute</th>\n",
              "      <th>Second</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>IsWeekend</th>\n",
              "      <th>TimeOfDay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>2000548391</td>\n",
              "      <td>2009-06-01 22:22:01-07:00</td>\n",
              "      <td>ticia42</td>\n",
              "      <td>can't open door herself, think feels pain would.</td>\n",
              "      <td></td>\n",
              "      <td>2009</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Evening</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>1754199174</td>\n",
              "      <td>2009-05-10 05:23:02-07:00</td>\n",
              "      <td>hockeycrew</td>\n",
              "      <td>least lawn hasn't taken over field weeds!</td>\n",
              "      <td></td>\n",
              "      <td>2009</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>Night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1994056674</td>\n",
              "      <td>2009-06-01 11:20:41-07:00</td>\n",
              "      <td>GeoBlack_Cat</td>\n",
              "      <td>umm.. like, hello? where's child support payment? Hope ex still gainfully employed - many people know currently not</td>\n",
              "      <td></td>\n",
              "      <td>2009</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Morning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1980150068</td>\n",
              "      <td>2009-05-31 05:51:31-07:00</td>\n",
              "      <td>rawrcelne</td>\n",
              "      <td>Joined twitter</td>\n",
              "      <td></td>\n",
              "      <td>2009</td>\n",
              "      <td>5</td>\n",
              "      <td>31</td>\n",
              "      <td>5</td>\n",
              "      <td>51</td>\n",
              "      <td>31</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>Night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>2242922534</td>\n",
              "      <td>2009-06-19 12:48:06-07:00</td>\n",
              "      <td>Whacky</td>\n",
              "      <td>Gayle wrong guy wrong team much like Brian Lara #t20wc</td>\n",
              "      <td></td>\n",
              "      <td>2009</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Afternoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999995</th>\n",
              "      <td>0</td>\n",
              "      <td>1985361990</td>\n",
              "      <td>2009-05-31 16:57:39-07:00</td>\n",
              "      <td>lutheasalom</td>\n",
              "      <td>song's middle change doesn't want born..... arghhhh!!</td>\n",
              "      <td></td>\n",
              "      <td>2009</td>\n",
              "      <td>5</td>\n",
              "      <td>31</td>\n",
              "      <td>16</td>\n",
              "      <td>57</td>\n",
              "      <td>39</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>Afternoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999996</th>\n",
              "      <td>4</td>\n",
              "      <td>2057029784</td>\n",
              "      <td>2009-06-06 12:14:24-07:00</td>\n",
              "      <td>beeluz</td>\n",
              "      <td>Good luck</td>\n",
              "      <td></td>\n",
              "      <td>2009</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>24</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>Afternoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999997</th>\n",
              "      <td>0</td>\n",
              "      <td>1835639354</td>\n",
              "      <td>2009-05-18 06:26:21-07:00</td>\n",
              "      <td>lordmuttley</td>\n",
              "      <td>rather average 32370</td>\n",
              "      <td></td>\n",
              "      <td>2009</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>26</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Morning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999998</th>\n",
              "      <td>0</td>\n",
              "      <td>2246780174</td>\n",
              "      <td>2009-06-19 18:06:46-07:00</td>\n",
              "      <td>MizSadittyFancy</td>\n",
              "      <td>Pickin waitin 2 hurry up...I odeeee missed dem Table talk 2nite...LOL bout fat...</td>\n",
              "      <td></td>\n",
              "      <td>2009</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>46</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Evening</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999999</th>\n",
              "      <td>0</td>\n",
              "      <td>1833617173</td>\n",
              "      <td>2009-05-17 23:52:31-07:00</td>\n",
              "      <td>dindahh</td>\n",
              "      <td>home studying maths wooot ! going fail shit</td>\n",
              "      <td></td>\n",
              "      <td>2009</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>23</td>\n",
              "      <td>52</td>\n",
              "      <td>31</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>Evening</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>987629 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        labels          id                      Date             User  \\\n",
              "0            4  2000548391 2009-06-01 22:22:01-07:00          ticia42   \n",
              "2            4  1754199174 2009-05-10 05:23:02-07:00       hockeycrew   \n",
              "3            0  1994056674 2009-06-01 11:20:41-07:00     GeoBlack_Cat   \n",
              "4            4  1980150068 2009-05-31 05:51:31-07:00        rawrcelne   \n",
              "5            0  2242922534 2009-06-19 12:48:06-07:00           Whacky   \n",
              "...        ...         ...                       ...              ...   \n",
              "999995       0  1985361990 2009-05-31 16:57:39-07:00      lutheasalom   \n",
              "999996       4  2057029784 2009-06-06 12:14:24-07:00           beeluz   \n",
              "999997       0  1835639354 2009-05-18 06:26:21-07:00      lordmuttley   \n",
              "999998       0  2246780174 2009-06-19 18:06:46-07:00  MizSadittyFancy   \n",
              "999999       0  1833617173 2009-05-17 23:52:31-07:00          dindahh   \n",
              "\n",
              "                                                                                                                 Fixed_Text  \\\n",
              "0                                                                          can't open door herself, think feels pain would.   \n",
              "2                                                                                 least lawn hasn't taken over field weeds!   \n",
              "3       umm.. like, hello? where's child support payment? Hope ex still gainfully employed - many people know currently not   \n",
              "4                                                                                                            Joined twitter   \n",
              "5                                                                    Gayle wrong guy wrong team much like Brian Lara #t20wc   \n",
              "...                                                                                                                     ...   \n",
              "999995                                                                song's middle change doesn't want born..... arghhhh!!   \n",
              "999996                                                                                                            Good luck   \n",
              "999997                                                                                                 rather average 32370   \n",
              "999998                                    Pickin waitin 2 hurry up...I odeeee missed dem Table talk 2nite...LOL bout fat...   \n",
              "999999                                                                          home studying maths wooot ! going fail shit   \n",
              "\n",
              "       Emojis  Year  Month  Day  Hour  Minute  Second  DayOfWeek  IsWeekend  \\\n",
              "0              2009      6    1    22      22       1          0          0   \n",
              "2              2009      5   10     5      23       2          6          1   \n",
              "3              2009      6    1    11      20      41          0          0   \n",
              "4              2009      5   31     5      51      31          6          1   \n",
              "5              2009      6   19    12      48       6          4          0   \n",
              "...       ...   ...    ...  ...   ...     ...     ...        ...        ...   \n",
              "999995         2009      5   31    16      57      39          6          1   \n",
              "999996         2009      6    6    12      14      24          5          1   \n",
              "999997         2009      5   18     6      26      21          0          0   \n",
              "999998         2009      6   19    18       6      46          4          0   \n",
              "999999         2009      5   17    23      52      31          6          1   \n",
              "\n",
              "        TimeOfDay  \n",
              "0         Evening  \n",
              "2           Night  \n",
              "3         Morning  \n",
              "4           Night  \n",
              "5       Afternoon  \n",
              "...           ...  \n",
              "999995  Afternoon  \n",
              "999996  Afternoon  \n",
              "999997    Morning  \n",
              "999998    Evening  \n",
              "999999    Evening  \n",
              "\n",
              "[987629 rows x 15 columns]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575bf6b6-cb25-497c-acde-d5407ceae900",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "575bf6b6-cb25-497c-acde-d5407ceae900",
        "outputId": "9ac287fc-4bea-4e3c-c1ad-3b284d2b4bce",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2009])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned.Year.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460ca207-a4e8-49ec-9ed1-ea064286a274",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "460ca207-a4e8-49ec-9ed1-ea064286a274",
        "outputId": "39713415-b881-4b8f-841c-b7323b53e25f",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([6, 5, 4])"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned.Month.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ffef3ad-3ad1-4ec8-b728-6b4f08364bf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ffef3ad-3ad1-4ec8-b728-6b4f08364bf7",
        "outputId": "ef7799f3-2982-4a92-b61c-db0fe746a793",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1, 10, 31, 19,  6,  3, 30, 16,  5, 20, 14, 22, 29, 15, 18, 23, 28,\n",
              "        2, 17,  7,  4, 21,  9, 11, 24, 13, 25, 26, 27])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned.Day.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9798a0e-e966-4cd8-9da2-e25d2626af29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9798a0e-e966-4cd8-9da2-e25d2626af29",
        "outputId": "eb0e6f6f-3ed5-49f1-904c-56f433c1c8a7",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 6, 4, 2, 5, 1, 3])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned.DayOfWeek.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab51a908-4efc-4587-9add-421af7b8397f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab51a908-4efc-4587-9add-421af7b8397f",
        "outputId": "80dc3639-88e2-476d-bdc3-67b521d12fe5",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1], dtype=int64)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned.IsWeekend.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-5zCpsNhOHC2",
      "metadata": {
        "id": "-5zCpsNhOHC2"
      },
      "outputs": [],
      "source": [
        "df_cleaned.drop(columns=['Date', 'Year', 'Month', 'Day', 'DayOfWeek'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aZA4V9pIOZ9b",
      "metadata": {
        "id": "aZA4V9pIOZ9b"
      },
      "outputs": [],
      "source": [
        "df_cleaned.drop(columns=['Hour', 'Minute', 'Second'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NIKChoWDOXWj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "NIKChoWDOXWj",
        "outputId": "a7497921-ff9e-408d-ea94-199615765264"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>User</th>\n",
              "      <th>Fixed_Text</th>\n",
              "      <th>Emojis</th>\n",
              "      <th>IsWeekend</th>\n",
              "      <th>TimeOfDay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>2000548391</td>\n",
              "      <td>ticia42</td>\n",
              "      <td>can't open door herself, think feels pain would.</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>Evening</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>1754199174</td>\n",
              "      <td>hockeycrew</td>\n",
              "      <td>least lawn hasn't taken over field weeds!</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>Night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1994056674</td>\n",
              "      <td>GeoBlack_Cat</td>\n",
              "      <td>umm.. like, hello? where's child support payment? Hope ex still gainfully employed - many people know currently not</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>Morning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1980150068</td>\n",
              "      <td>rawrcelne</td>\n",
              "      <td>Joined twitter</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>Night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>2242922534</td>\n",
              "      <td>Whacky</td>\n",
              "      <td>Gayle wrong guy wrong team much like Brian Lara #t20wc</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>Afternoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999995</th>\n",
              "      <td>0</td>\n",
              "      <td>1985361990</td>\n",
              "      <td>lutheasalom</td>\n",
              "      <td>song's middle change doesn't want born..... arghhhh!!</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>Afternoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999996</th>\n",
              "      <td>4</td>\n",
              "      <td>2057029784</td>\n",
              "      <td>beeluz</td>\n",
              "      <td>Good luck</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>Afternoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999997</th>\n",
              "      <td>0</td>\n",
              "      <td>1835639354</td>\n",
              "      <td>lordmuttley</td>\n",
              "      <td>rather average 32370</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>Morning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999998</th>\n",
              "      <td>0</td>\n",
              "      <td>2246780174</td>\n",
              "      <td>MizSadittyFancy</td>\n",
              "      <td>Pickin waitin 2 hurry up...I odeeee missed dem Table talk 2nite...LOL bout fat...</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>Evening</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999999</th>\n",
              "      <td>0</td>\n",
              "      <td>1833617173</td>\n",
              "      <td>dindahh</td>\n",
              "      <td>home studying maths wooot ! going fail shit</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>Evening</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>987629 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        labels          id             User  \\\n",
              "0            4  2000548391          ticia42   \n",
              "2            4  1754199174       hockeycrew   \n",
              "3            0  1994056674     GeoBlack_Cat   \n",
              "4            4  1980150068        rawrcelne   \n",
              "5            0  2242922534           Whacky   \n",
              "...        ...         ...              ...   \n",
              "999995       0  1985361990      lutheasalom   \n",
              "999996       4  2057029784           beeluz   \n",
              "999997       0  1835639354      lordmuttley   \n",
              "999998       0  2246780174  MizSadittyFancy   \n",
              "999999       0  1833617173          dindahh   \n",
              "\n",
              "                                                                                                                 Fixed_Text  \\\n",
              "0                                                                          can't open door herself, think feels pain would.   \n",
              "2                                                                                 least lawn hasn't taken over field weeds!   \n",
              "3       umm.. like, hello? where's child support payment? Hope ex still gainfully employed - many people know currently not   \n",
              "4                                                                                                            Joined twitter   \n",
              "5                                                                    Gayle wrong guy wrong team much like Brian Lara #t20wc   \n",
              "...                                                                                                                     ...   \n",
              "999995                                                                song's middle change doesn't want born..... arghhhh!!   \n",
              "999996                                                                                                            Good luck   \n",
              "999997                                                                                                 rather average 32370   \n",
              "999998                                    Pickin waitin 2 hurry up...I odeeee missed dem Table talk 2nite...LOL bout fat...   \n",
              "999999                                                                          home studying maths wooot ! going fail shit   \n",
              "\n",
              "       Emojis  IsWeekend  TimeOfDay  \n",
              "0                      0    Evening  \n",
              "2                      1      Night  \n",
              "3                      0    Morning  \n",
              "4                      1      Night  \n",
              "5                      0  Afternoon  \n",
              "...       ...        ...        ...  \n",
              "999995                 1  Afternoon  \n",
              "999996                 1  Afternoon  \n",
              "999997                 0    Morning  \n",
              "999998                 0    Evening  \n",
              "999999                 1    Evening  \n",
              "\n",
              "[987629 rows x 7 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2095be2f-0215-436a-956a-3e40bfb9b5e6",
      "metadata": {
        "id": "2095be2f-0215-436a-956a-3e40bfb9b5e6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset = df_cleaned.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c645765b-a259-4763-af1c-f7a7f130d5a1",
      "metadata": {
        "id": "c645765b-a259-4763-af1c-f7a7f130d5a1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Check garbled text in DF\n",
        "\n",
        "def contains_non_ascii(text):\n",
        "    try:\n",
        "        text.encode('ascii')\n",
        "    except UnicodeEncodeError as e:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "dataset['ContainsGarbledText'] = dataset['Fixed_Text'].apply(contains_non_ascii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d01a7f3-ae35-4a93-aa66-97424f916893",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5d01a7f3-ae35-4a93-aa66-97424f916893",
        "outputId": "d9dad9f1-b171-4096-f507-8eadbe27e0d9",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>id</th>\n",
              "      <th>User</th>\n",
              "      <th>Fixed_Text</th>\n",
              "      <th>Emojis</th>\n",
              "      <th>IsWeekend</th>\n",
              "      <th>TimeOfDay</th>\n",
              "      <th>ContainsGarbledText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [labels, id, User, Fixed_Text, Emojis, IsWeekend, TimeOfDay, ContainsGarbledText]\n",
              "Index: []"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[dataset['ContainsGarbledText']== True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zUh7ceyaOrDL",
      "metadata": {
        "id": "zUh7ceyaOrDL"
      },
      "outputs": [],
      "source": [
        "dataset.drop(columns=['ContainsGarbledText'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5952f085-f8b0-43d8-8926-814ecde184c2",
      "metadata": {
        "id": "5952f085-f8b0-43d8-8926-814ecde184c2"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VR6IcT8xPWRG",
      "metadata": {
        "id": "VR6IcT8xPWRG",
        "outputId": "de3afeb6-61f1-41b3-a588-85f3c5f5c783"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
            "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ed6cb9-7a93-4202-9e5e-ced886d5f6a0",
      "metadata": {
        "id": "a5ed6cb9-7a93-4202-9e5e-ced886d5f6a0"
      },
      "outputs": [],
      "source": [
        "data = dataset.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pl4N7g2oh1mU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl4N7g2oh1mU",
        "outputId": "4cb8d7d8-7459-42d0-eaab-b7199df30476"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['labels', 'id', 'User', 'Fixed_Text', 'Emojis', 'IsWeekend',\n",
              "       'TimeOfDay'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18acf0b6-ebaa-43ba-b256-345282dac091",
      "metadata": {
        "id": "18acf0b6-ebaa-43ba-b256-345282dac091"
      },
      "outputs": [],
      "source": [
        "data = data[['Fixed_Text', 'labels']]\n",
        "data = data.rename(columns={'Fixed_Text': 'text', 'labels': 'label'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d6ce99-8892-4bf4-99b7-48ed5c808b8a",
      "metadata": {
        "id": "14d6ce99-8892-4bf4-99b7-48ed5c808b8a"
      },
      "outputs": [],
      "source": [
        "data['label'] = data['label'].apply(lambda x: 0 if x == 0 else 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries import"
      ],
      "metadata": {
        "id": "CptxTGqgyFEM"
      },
      "id": "CptxTGqgyFEM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84337003-1997-4967-b158-a18c637a98e6",
      "metadata": {
        "id": "84337003-1997-4967-b158-a18c637a98e6",
        "outputId": "c651e574-b225-4d56-e94d-ce392737fc61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (4.42.4)\n",
            "Requirement already satisfied: datasets in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (2.20.0)\n",
            "Requirement already satisfied: torch in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (2.3.1+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "189e9c17-628c-4785-becc-b8acf4431e87",
      "metadata": {
        "id": "189e9c17-628c-4785-becc-b8acf4431e87",
        "outputId": "15c71d2a-a02e-4f7d-d2f6-76e888eb1e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (4.42.4)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: torch in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (2.3.1+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf0d760-bad9-474b-bb3c-87e4fe3220b9",
      "metadata": {
        "id": "baf0d760-bad9-474b-bb3c-87e4fe3220b9",
        "outputId": "fb1448b8-2fab-4ab5-8419-72eacea084ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (2.3.1+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (0.18.1+cu118)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (2.3.1+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b72818ea-4bbd-4a0e-8bf2-e14380d1c10d",
      "metadata": {
        "scrolled": true,
        "id": "b72818ea-4bbd-4a0e-8bf2-e14380d1c10d",
        "outputId": "9d90c945-8e1e-42ca-80bd-871430e34b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (4.42.4)\n",
            "Requirement already satisfied: accelerate in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (0.32.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: psutil in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from accelerate) (2.3.1+cu118)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.13.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "425b6f83-4a1b-4505-831c-a1c04ba45fdb",
      "metadata": {
        "id": "425b6f83-4a1b-4505-831c-a1c04ba45fdb"
      },
      "outputs": [],
      "source": [
        "pip install transformers==4.18.0 accelerate==0.7.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c1c1dce-3c38-4661-9125-2dcaaeb943d0",
      "metadata": {
        "scrolled": true,
        "id": "5c1c1dce-3c38-4661-9125-2dcaaeb943d0"
      },
      "outputs": [],
      "source": [
        "pip show transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ffacbe-7f2d-437b-a348-a9161e3b8cf0",
      "metadata": {
        "scrolled": true,
        "id": "f5ffacbe-7f2d-437b-a348-a9161e3b8cf0"
      },
      "outputs": [],
      "source": [
        "pip install transformers==4.18.0 accelerate==0.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe87dc64-0523-4ac7-b9f0-ce2bf2377bd1",
      "metadata": {
        "scrolled": true,
        "id": "fe87dc64-0523-4ac7-b9f0-ce2bf2377bd1"
      },
      "outputs": [],
      "source": [
        "pip install transformers[torch] accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a9f5eca-4b96-4720-8994-0634b52c679c",
      "metadata": {
        "id": "6a9f5eca-4b96-4720-8994-0634b52c679c"
      },
      "outputs": [],
      "source": [
        "pip install accelerate>=0.21.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a768cdb-91d5-42e6-9c71-83256a3cf2aa",
      "metadata": {
        "id": "4a768cdb-91d5-42e6-9c71-83256a3cf2aa",
        "outputId": "cc46d373-02b1-48f3-95ba-c15f792a939d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
            "Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
            "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
            "   - -------------------------------------- 30.7/991.5 kB 1.4 MB/s eta 0:00:01\n",
            "   - -------------------------------------- 30.7/991.5 kB 1.4 MB/s eta 0:00:01\n",
            "   -- ------------------------------------ 61.4/991.5 kB 409.6 kB/s eta 0:00:03\n",
            "   --- ----------------------------------- 92.2/991.5 kB 525.1 kB/s eta 0:00:02\n",
            "   ---- --------------------------------- 112.6/991.5 kB 547.6 kB/s eta 0:00:02\n",
            "   ----- -------------------------------- 143.4/991.5 kB 568.9 kB/s eta 0:00:02\n",
            "   ----- -------------------------------- 153.6/991.5 kB 573.4 kB/s eta 0:00:02\n",
            "   ------- ------------------------------ 194.6/991.5 kB 622.7 kB/s eta 0:00:02\n",
            "   -------- ----------------------------- 225.3/991.5 kB 655.6 kB/s eta 0:00:02\n",
            "   --------- ---------------------------- 256.0/991.5 kB 655.4 kB/s eta 0:00:02\n",
            "   ----------- -------------------------- 307.2/991.5 kB 705.4 kB/s eta 0:00:01\n",
            "   ------------ ------------------------- 337.9/991.5 kB 723.4 kB/s eta 0:00:01\n",
            "   ------------- ------------------------ 358.4/991.5 kB 743.9 kB/s eta 0:00:01\n",
            "   --------------- ---------------------- 399.4/991.5 kB 732.8 kB/s eta 0:00:01\n",
            "   ----------------- -------------------- 450.6/991.5 kB 761.8 kB/s eta 0:00:01\n",
            "   ----------------- -------------------- 450.6/991.5 kB 761.8 kB/s eta 0:00:01\n",
            "   ------------------ ------------------- 481.3/991.5 kB 736.0 kB/s eta 0:00:01\n",
            "   -------------------- ----------------- 532.5/991.5 kB 760.2 kB/s eta 0:00:01\n",
            "   -------------------- ----------------- 532.5/991.5 kB 760.2 kB/s eta 0:00:01\n",
            "   ---------------------- --------------- 583.7/991.5 kB 781.3 kB/s eta 0:00:01\n",
            "   ----------------------- -------------- 614.4/991.5 kB 773.6 kB/s eta 0:00:01\n",
            "   ------------------------ ------------- 645.1/991.5 kB 739.1 kB/s eta 0:00:01\n",
            "   ------------------------- ------------ 665.6/991.5 kB 749.0 kB/s eta 0:00:01\n",
            "   -------------------------- ----------- 696.3/991.5 kB 757.3 kB/s eta 0:00:01\n",
            "   ----------------------------- -------- 768.0/991.5 kB 782.8 kB/s eta 0:00:01\n",
            "   ------------------------------ ------- 798.7/991.5 kB 776.8 kB/s eta 0:00:01\n",
            "   ------------------------------ ------- 798.7/991.5 kB 776.8 kB/s eta 0:00:01\n",
            "   -------------------------------- ----- 860.2/991.5 kB 788.4 kB/s eta 0:00:01\n",
            "   ---------------------------------- --- 890.9/991.5 kB 794.1 kB/s eta 0:00:01\n",
            "   ------------------------------------ - 962.6/991.5 kB 813.3 kB/s eta 0:00:01\n",
            "   ------------------------------------ - 962.6/991.5 kB 813.3 kB/s eta 0:00:01\n",
            "   -------------------------------------- 991.5/991.5 kB 785.0 kB/s eta 0:00:00\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaned data"
      ],
      "metadata": {
        "id": "GN2T_p_oyMOQ"
      },
      "id": "GN2T_p_oyMOQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distilbert"
      ],
      "metadata": {
        "id": "7JR5yortyYrn"
      },
      "id": "7JR5yortyYrn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f917dda4-c7c0-4023-92f1-81660d1d2d4e",
      "metadata": {
        "id": "f917dda4-c7c0-4023-92f1-81660d1d2d4e"
      },
      "outputs": [],
      "source": [
        "data.to_csv('saved_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4501da77-84b9-4c26-a60f-b23b395dc18a",
      "metadata": {
        "id": "4501da77-84b9-4c26-a60f-b23b395dc18a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe7058a-4c7d-4469-8524-91635f3a41ef",
      "metadata": {
        "id": "7fe7058a-4c7d-4469-8524-91635f3a41ef"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('saved_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0112b3-d37e-4ca7-9337-eda91365c36c",
      "metadata": {
        "id": "3f0112b3-d37e-4ca7-9337-eda91365c36c",
        "outputId": "06d81394-3c4f-4ad6-c979-405c266e6315"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>can't open door herself, think feels pain would.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>least lawn hasn't taken over field weeds!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>umm.. like, hello? where's child support payme...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Joined twitter</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Gayle wrong guy wrong team much like Brian Lar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987624</th>\n",
              "      <td>999995</td>\n",
              "      <td>song's middle change doesn't want born..... ar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987625</th>\n",
              "      <td>999996</td>\n",
              "      <td>Good luck</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987626</th>\n",
              "      <td>999997</td>\n",
              "      <td>rather average 32370</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987627</th>\n",
              "      <td>999998</td>\n",
              "      <td>Pickin waitin 2 hurry up...I odeeee missed dem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987628</th>\n",
              "      <td>999999</td>\n",
              "      <td>home studying maths wooot ! going fail shit</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>987629 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0                                               text  label\n",
              "0                0   can't open door herself, think feels pain would.      1\n",
              "1                2          least lawn hasn't taken over field weeds!      1\n",
              "2                3  umm.. like, hello? where's child support payme...      0\n",
              "3                4                                     Joined twitter      1\n",
              "4                5  Gayle wrong guy wrong team much like Brian Lar...      0\n",
              "...            ...                                                ...    ...\n",
              "987624      999995  song's middle change doesn't want born..... ar...      0\n",
              "987625      999996                                          Good luck      1\n",
              "987626      999997                               rather average 32370      0\n",
              "987627      999998  Pickin waitin 2 hurry up...I odeeee missed dem...      0\n",
              "987628      999999        home studying maths wooot ! going fail shit      0\n",
              "\n",
              "[987629 rows x 3 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e35e73c4-cb6b-4162-8bb1-35506cf9723b",
      "metadata": {
        "id": "e35e73c4-cb6b-4162-8bb1-35506cf9723b"
      },
      "outputs": [],
      "source": [
        "data['text'] = data['text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9fedf8-2725-4a65-9747-388273dbf7b0",
      "metadata": {
        "id": "4a9fedf8-2725-4a65-9747-388273dbf7b0",
        "outputId": "3ed52814-82b3-440a-8828-a88a90a4e05a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>can't open door herself, think feels pain would.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>least lawn hasn't taken over field weeds!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>umm.. like, hello? where's child support payme...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Joined twitter</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Gayle wrong guy wrong team much like Brian Lar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987624</th>\n",
              "      <td>999995</td>\n",
              "      <td>song's middle change doesn't want born..... ar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987625</th>\n",
              "      <td>999996</td>\n",
              "      <td>Good luck</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987626</th>\n",
              "      <td>999997</td>\n",
              "      <td>rather average 32370</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987627</th>\n",
              "      <td>999998</td>\n",
              "      <td>Pickin waitin 2 hurry up...I odeeee missed dem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987628</th>\n",
              "      <td>999999</td>\n",
              "      <td>home studying maths wooot ! going fail shit</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>987629 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0                                               text  label\n",
              "0                0   can't open door herself, think feels pain would.      1\n",
              "1                2          least lawn hasn't taken over field weeds!      1\n",
              "2                3  umm.. like, hello? where's child support payme...      0\n",
              "3                4                                     Joined twitter      1\n",
              "4                5  Gayle wrong guy wrong team much like Brian Lar...      0\n",
              "...            ...                                                ...    ...\n",
              "987624      999995  song's middle change doesn't want born..... ar...      0\n",
              "987625      999996                                          Good luck      1\n",
              "987626      999997                               rather average 32370      0\n",
              "987627      999998  Pickin waitin 2 hurry up...I odeeee missed dem...      0\n",
              "987628      999999        home studying maths wooot ! going fail shit      0\n",
              "\n",
              "[987629 rows x 3 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a483657d-5247-4a97-836e-db2b0eb0ac4f",
      "metadata": {
        "id": "a483657d-5247-4a97-836e-db2b0eb0ac4f"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6634bb6c-d45f-4792-b2f3-b0cab680b8c3",
      "metadata": {
        "id": "6634bb6c-d45f-4792-b2f3-b0cab680b8c3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(data['text'].tolist(), data['label'].tolist(), test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fbf2dd2-bad3-447a-91a2-c097e96ca5e0",
      "metadata": {
        "id": "4fbf2dd2-bad3-447a-91a2-c097e96ca5e0"
      },
      "outputs": [],
      "source": [
        "data_sampled = data.sample(frac=1, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data_sampled['text'].tolist(),\n",
        "    data_sampled['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd25025-5201-449e-9ba8-44f38cf9f208",
      "metadata": {
        "id": "bcd25025-5201-449e-9ba8-44f38cf9f208"
      },
      "outputs": [],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenize(train_texts)\n",
        "test_encodings = tokenize(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de43f986-0438-47f5-82e2-f5d10ebae875",
      "metadata": {
        "id": "de43f986-0438-47f5-82e2-f5d10ebae875",
        "outputId": "b4e4aa5d-56ef-445e-9e36-a72bf0f888b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2183, 27090,  ...,     0,     0,     0],\n",
              "        [  101,  2279,  2048,  ...,     0,     0,     0],\n",
              "        [  101,  2183,  3509,  ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101,  2134,  2102,  ...,     0,     0,     0],\n",
              "        [  101, 24026,  1012,  ...,     0,     0,     0],\n",
              "        [  101,  4654,  1005,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]])}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be4e1be-c455-4837-a9ca-e500ff29f01d",
      "metadata": {
        "id": "3be4e1be-c455-4837-a9ca-e500ff29f01d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx]).clone().detach()\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b730d22-3f43-4f8d-a60a-c4ad78d89fae",
      "metadata": {
        "id": "3b730d22-3f43-4f8d-a60a-c4ad78d89fae"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextDataset(train_encodings, train_labels)\n",
        "test_dataset = TextDataset(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c6f98e-50a7-4e82-be07-37315794356a",
      "metadata": {
        "id": "e3c6f98e-50a7-4e82-be07-37315794356a"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adb66eaa-99b6-446a-b92f-c9a76c09add7",
      "metadata": {
        "id": "adb66eaa-99b6-446a-b92f-c9a76c09add7"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1afc787c-d1ed-474e-8e30-7df0d42ed64b",
      "metadata": {
        "id": "1afc787c-d1ed-474e-8e30-7df0d42ed64b",
        "outputId": "e80f84da-0a82-4a11-8496-86f3fcf8a268"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb80683c-bd00-43b5-9c9f-d2070fc32f98",
      "metadata": {
        "id": "bb80683c-bd00-43b5-9c9f-d2070fc32f98"
      },
      "outputs": [],
      "source": [
        "# Set up the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training function with batch-wise validation\n",
        "def train(epoch, model, train_loader, test_loader, optimizer):\n",
        "    model.train()\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy, val_loss = evaluate(model, test_loader)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_idx}, Training Loss: {loss.item()}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "    return best_model_state, best_val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e61a51bf-57c1-408d-9745-208b0be8a2c5",
      "metadata": {
        "id": "e61a51bf-57c1-408d-9745-208b0be8a2c5"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            logits = outputs.logits\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            label_ids = labels.cpu().numpy()\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(label_ids)\n",
        "\n",
        "    average_loss = total_loss / len(test_loader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy, average_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47bd45c-fb84-4d42-83a6-040660c31d96",
      "metadata": {
        "id": "d47bd45c-fb84-4d42-83a6-040660c31d96",
        "outputId": "f7e3ece1-0069-4c98-8e78-ffa9c4069086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 0, Training Loss: 0.6901992559432983, Validation Loss: 0.6962, Accuracy: 0.4991\n",
            "Epoch 0, Batch 1, Training Loss: 0.7084370851516724, Validation Loss: 0.6881, Accuracy: 0.5527\n",
            "Epoch 0, Batch 2, Training Loss: 0.6797231435775757, Validation Loss: 0.6850, Accuracy: 0.5683\n",
            "Epoch 0, Batch 3, Training Loss: 0.6836034059524536, Validation Loss: 0.6834, Accuracy: 0.5787\n",
            "Epoch 0, Batch 4, Training Loss: 0.6806811690330505, Validation Loss: 0.6807, Accuracy: 0.5826\n",
            "Epoch 0, Batch 5, Training Loss: 0.6665593981742859, Validation Loss: 0.6787, Accuracy: 0.5906\n",
            "Epoch 0, Batch 6, Training Loss: 0.6942813992500305, Validation Loss: 0.6847, Accuracy: 0.5312\n",
            "Epoch 0, Batch 7, Training Loss: 0.6686683297157288, Validation Loss: 0.6914, Accuracy: 0.5032\n",
            "Epoch 0, Batch 8, Training Loss: 0.6629774570465088, Validation Loss: 0.6939, Accuracy: 0.5013\n",
            "Epoch 0, Batch 9, Training Loss: 0.7244338393211365, Validation Loss: 0.6886, Accuracy: 0.5112\n",
            "Epoch 0, Batch 10, Training Loss: 0.714262843132019, Validation Loss: 0.6770, Accuracy: 0.5595\n",
            "Epoch 0, Batch 11, Training Loss: 0.7026423811912537, Validation Loss: 0.6664, Accuracy: 0.6092\n",
            "Epoch 0, Batch 12, Training Loss: 0.6637403964996338, Validation Loss: 0.6595, Accuracy: 0.6213\n",
            "Epoch 0, Batch 13, Training Loss: 0.6646303534507751, Validation Loss: 0.6543, Accuracy: 0.6230\n",
            "Epoch 0, Batch 14, Training Loss: 0.62534499168396, Validation Loss: 0.6494, Accuracy: 0.6312\n",
            "Epoch 0, Batch 15, Training Loss: 0.6235894560813904, Validation Loss: 0.6422, Accuracy: 0.6398\n",
            "Epoch 0, Batch 16, Training Loss: 0.6817532181739807, Validation Loss: 0.6351, Accuracy: 0.6502\n",
            "Epoch 0, Batch 17, Training Loss: 0.6735484600067139, Validation Loss: 0.6288, Accuracy: 0.6566\n",
            "Epoch 0, Batch 18, Training Loss: 0.6955422163009644, Validation Loss: 0.6283, Accuracy: 0.6572\n",
            "Epoch 0, Batch 19, Training Loss: 0.58408123254776, Validation Loss: 0.6204, Accuracy: 0.6671\n",
            "Epoch 0, Batch 20, Training Loss: 0.6839277148246765, Validation Loss: 0.6037, Accuracy: 0.6918\n",
            "Epoch 0, Batch 21, Training Loss: 0.65315181016922, Validation Loss: 0.5907, Accuracy: 0.7169\n",
            "Epoch 0, Batch 22, Training Loss: 0.5308690667152405, Validation Loss: 0.5841, Accuracy: 0.7235\n",
            "Epoch 0, Batch 23, Training Loss: 0.5538413524627686, Validation Loss: 0.5747, Accuracy: 0.7258\n",
            "Epoch 0, Batch 24, Training Loss: 0.5559212565422058, Validation Loss: 0.5613, Accuracy: 0.7325\n",
            "Epoch 0, Batch 25, Training Loss: 0.5702211260795593, Validation Loss: 0.5497, Accuracy: 0.7371\n",
            "Epoch 0, Batch 26, Training Loss: 0.5385410785675049, Validation Loss: 0.5400, Accuracy: 0.7418\n",
            "Epoch 0, Batch 27, Training Loss: 0.5997585654258728, Validation Loss: 0.5578, Accuracy: 0.7188\n",
            "Epoch 0, Batch 28, Training Loss: 0.5749174356460571, Validation Loss: 0.5941, Accuracy: 0.6918\n",
            "Epoch 0, Batch 29, Training Loss: 0.5786493420600891, Validation Loss: 0.6114, Accuracy: 0.6813\n",
            "Epoch 0, Batch 30, Training Loss: 0.5873757600784302, Validation Loss: 0.6131, Accuracy: 0.6813\n",
            "Epoch 0, Batch 31, Training Loss: 0.6649470925331116, Validation Loss: 0.5484, Accuracy: 0.7291\n",
            "Epoch 0, Batch 32, Training Loss: 0.6618167161941528, Validation Loss: 0.5266, Accuracy: 0.7474\n",
            "Epoch 0, Batch 33, Training Loss: 0.6824008226394653, Validation Loss: 0.5205, Accuracy: 0.7569\n",
            "Epoch 0, Batch 34, Training Loss: 0.4961460530757904, Validation Loss: 0.5210, Accuracy: 0.7562\n",
            "Epoch 0, Batch 35, Training Loss: 0.43961411714553833, Validation Loss: 0.5260, Accuracy: 0.7554\n",
            "Epoch 0, Batch 36, Training Loss: 0.4435983896255493, Validation Loss: 0.5223, Accuracy: 0.7570\n",
            "Epoch 0, Batch 37, Training Loss: 0.5104811191558838, Validation Loss: 0.5240, Accuracy: 0.7570\n",
            "Epoch 0, Batch 38, Training Loss: 0.48468685150146484, Validation Loss: 0.5213, Accuracy: 0.7596\n",
            "Epoch 0, Batch 39, Training Loss: 0.6333221793174744, Validation Loss: 0.5131, Accuracy: 0.7613\n",
            "Epoch 0, Batch 40, Training Loss: 0.6387577056884766, Validation Loss: 0.5105, Accuracy: 0.7625\n",
            "Epoch 0, Batch 41, Training Loss: 0.4787014424800873, Validation Loss: 0.5082, Accuracy: 0.7625\n",
            "Epoch 0, Batch 42, Training Loss: 0.4442219138145447, Validation Loss: 0.5048, Accuracy: 0.7640\n",
            "Epoch 0, Batch 43, Training Loss: 0.46573156118392944, Validation Loss: 0.5054, Accuracy: 0.7586\n",
            "Epoch 0, Batch 44, Training Loss: 0.4339083433151245, Validation Loss: 0.5038, Accuracy: 0.7590\n",
            "Epoch 0, Batch 45, Training Loss: 0.43281790614128113, Validation Loss: 0.4959, Accuracy: 0.7689\n",
            "Epoch 0, Batch 46, Training Loss: 0.36997878551483154, Validation Loss: 0.4970, Accuracy: 0.7703\n",
            "Epoch 0, Batch 47, Training Loss: 0.5314603447914124, Validation Loss: 0.5232, Accuracy: 0.7611\n",
            "Epoch 0, Batch 48, Training Loss: 0.6007118821144104, Validation Loss: 0.5426, Accuracy: 0.7539\n",
            "Epoch 0, Batch 49, Training Loss: 0.5033121109008789, Validation Loss: 0.5423, Accuracy: 0.7551\n",
            "Epoch 0, Batch 50, Training Loss: 0.47446727752685547, Validation Loss: 0.5181, Accuracy: 0.7641\n",
            "Epoch 0, Batch 51, Training Loss: 0.6631574034690857, Validation Loss: 0.4909, Accuracy: 0.7737\n",
            "Epoch 0, Batch 52, Training Loss: 0.5841461420059204, Validation Loss: 0.4973, Accuracy: 0.7693\n",
            "Epoch 0, Batch 53, Training Loss: 0.4156044125556946, Validation Loss: 0.5050, Accuracy: 0.7646\n",
            "Epoch 0, Batch 54, Training Loss: 0.564260721206665, Validation Loss: 0.5031, Accuracy: 0.7659\n",
            "Epoch 0, Batch 55, Training Loss: 0.48635295033454895, Validation Loss: 0.4913, Accuracy: 0.7722\n",
            "Epoch 0, Batch 56, Training Loss: 0.500978410243988, Validation Loss: 0.4840, Accuracy: 0.7739\n",
            "Epoch 0, Batch 57, Training Loss: 0.3677669167518616, Validation Loss: 0.4872, Accuracy: 0.7723\n",
            "Epoch 0, Batch 58, Training Loss: 0.5858815908432007, Validation Loss: 0.4980, Accuracy: 0.7661\n",
            "Epoch 0, Batch 59, Training Loss: 0.49243366718292236, Validation Loss: 0.5123, Accuracy: 0.7561\n",
            "Epoch 0, Batch 60, Training Loss: 0.5015699863433838, Validation Loss: 0.5105, Accuracy: 0.7552\n",
            "Epoch 0, Batch 61, Training Loss: 0.39733725786209106, Validation Loss: 0.5012, Accuracy: 0.7605\n",
            "Epoch 0, Batch 62, Training Loss: 0.49930891394615173, Validation Loss: 0.4925, Accuracy: 0.7662\n",
            "Epoch 0, Batch 63, Training Loss: 0.6885359287261963, Validation Loss: 0.4880, Accuracy: 0.7726\n",
            "Epoch 0, Batch 64, Training Loss: 0.5318111777305603, Validation Loss: 0.4921, Accuracy: 0.7740\n",
            "Epoch 0, Batch 65, Training Loss: 0.5098718404769897, Validation Loss: 0.5005, Accuracy: 0.7737\n",
            "Epoch 0, Batch 66, Training Loss: 0.4261069595813751, Validation Loss: 0.5056, Accuracy: 0.7721\n",
            "Epoch 0, Batch 67, Training Loss: 0.627561092376709, Validation Loss: 0.5038, Accuracy: 0.7739\n",
            "Epoch 0, Batch 68, Training Loss: 0.5079671144485474, Validation Loss: 0.4990, Accuracy: 0.7766\n",
            "Epoch 0, Batch 69, Training Loss: 0.5591751337051392, Validation Loss: 0.4938, Accuracy: 0.7798\n",
            "Epoch 0, Batch 70, Training Loss: 0.47385549545288086, Validation Loss: 0.4883, Accuracy: 0.7815\n",
            "Epoch 0, Batch 71, Training Loss: 0.5096864104270935, Validation Loss: 0.4853, Accuracy: 0.7802\n",
            "Epoch 0, Batch 72, Training Loss: 0.5114526748657227, Validation Loss: 0.4839, Accuracy: 0.7784\n",
            "Epoch 0, Batch 73, Training Loss: 0.5290485620498657, Validation Loss: 0.4839, Accuracy: 0.7769\n",
            "Epoch 0, Batch 74, Training Loss: 0.43564459681510925, Validation Loss: 0.4863, Accuracy: 0.7691\n",
            "Epoch 0, Batch 75, Training Loss: 0.5877862572669983, Validation Loss: 0.4864, Accuracy: 0.7678\n",
            "Epoch 0, Batch 76, Training Loss: 0.4911557137966156, Validation Loss: 0.4837, Accuracy: 0.7711\n",
            "Epoch 0, Batch 77, Training Loss: 0.4915566146373749, Validation Loss: 0.4797, Accuracy: 0.7753\n",
            "Epoch 0, Batch 78, Training Loss: 0.5673010945320129, Validation Loss: 0.4783, Accuracy: 0.7770\n",
            "Epoch 0, Batch 79, Training Loss: 0.534680962562561, Validation Loss: 0.4766, Accuracy: 0.7788\n",
            "Epoch 0, Batch 80, Training Loss: 0.4905249774456024, Validation Loss: 0.4755, Accuracy: 0.7810\n",
            "Epoch 0, Batch 81, Training Loss: 0.3801339864730835, Validation Loss: 0.4749, Accuracy: 0.7823\n",
            "Epoch 0, Batch 82, Training Loss: 0.4015902280807495, Validation Loss: 0.4755, Accuracy: 0.7829\n",
            "Epoch 0, Batch 83, Training Loss: 0.5316047668457031, Validation Loss: 0.4785, Accuracy: 0.7825\n",
            "Epoch 0, Batch 84, Training Loss: 0.3972390294075012, Validation Loss: 0.4802, Accuracy: 0.7811\n",
            "Epoch 0, Batch 85, Training Loss: 0.4640006721019745, Validation Loss: 0.4748, Accuracy: 0.7821\n",
            "Epoch 0, Batch 86, Training Loss: 0.5355464220046997, Validation Loss: 0.4728, Accuracy: 0.7811\n",
            "Epoch 0, Batch 87, Training Loss: 0.4360411465167999, Validation Loss: 0.4755, Accuracy: 0.7783\n",
            "Epoch 0, Batch 88, Training Loss: 0.3792716860771179, Validation Loss: 0.4792, Accuracy: 0.7763\n",
            "Epoch 0, Batch 89, Training Loss: 0.5485407114028931, Validation Loss: 0.4894, Accuracy: 0.7700\n",
            "Epoch 0, Batch 90, Training Loss: 0.4207567870616913, Validation Loss: 0.5059, Accuracy: 0.7610\n",
            "Epoch 0, Batch 91, Training Loss: 0.5007296800613403, Validation Loss: 0.5116, Accuracy: 0.7565\n",
            "Epoch 0, Batch 92, Training Loss: 0.4457665681838989, Validation Loss: 0.5114, Accuracy: 0.7561\n",
            "Epoch 0, Batch 93, Training Loss: 0.3778628408908844, Validation Loss: 0.5081, Accuracy: 0.7568\n",
            "Epoch 0, Batch 94, Training Loss: 0.6808436512947083, Validation Loss: 0.4914, Accuracy: 0.7642\n",
            "Epoch 0, Batch 95, Training Loss: 0.5732985138893127, Validation Loss: 0.4771, Accuracy: 0.7720\n",
            "Epoch 0, Batch 96, Training Loss: 0.32723474502563477, Validation Loss: 0.4716, Accuracy: 0.7782\n",
            "Epoch 0, Batch 97, Training Loss: 0.694847583770752, Validation Loss: 0.4730, Accuracy: 0.7805\n",
            "Epoch 0, Batch 98, Training Loss: 0.5356714725494385, Validation Loss: 0.4790, Accuracy: 0.7814\n",
            "Epoch 0, Batch 99, Training Loss: 0.39889785647392273, Validation Loss: 0.4818, Accuracy: 0.7819\n",
            "Epoch 0, Batch 100, Training Loss: 0.4386887550354004, Validation Loss: 0.4851, Accuracy: 0.7803\n",
            "Epoch 0, Batch 101, Training Loss: 0.3715297281742096, Validation Loss: 0.4842, Accuracy: 0.7808\n",
            "Epoch 0, Batch 102, Training Loss: 0.43577516078948975, Validation Loss: 0.4807, Accuracy: 0.7808\n",
            "Epoch 0, Batch 103, Training Loss: 0.4778924286365509, Validation Loss: 0.4760, Accuracy: 0.7822\n",
            "Epoch 0, Batch 104, Training Loss: 0.41296836733818054, Validation Loss: 0.4709, Accuracy: 0.7844\n",
            "Epoch 0, Batch 105, Training Loss: 0.647087037563324, Validation Loss: 0.4695, Accuracy: 0.7808\n",
            "Epoch 0, Batch 106, Training Loss: 0.5598354339599609, Validation Loss: 0.4715, Accuracy: 0.7765\n",
            "Epoch 0, Batch 107, Training Loss: 0.4205474555492401, Validation Loss: 0.4810, Accuracy: 0.7648\n",
            "Epoch 0, Batch 108, Training Loss: 0.3994635343551636, Validation Loss: 0.4916, Accuracy: 0.7566\n",
            "Epoch 0, Batch 109, Training Loss: 0.39759889245033264, Validation Loss: 0.4956, Accuracy: 0.7554\n",
            "Epoch 0, Batch 110, Training Loss: 0.5878050327301025, Validation Loss: 0.4892, Accuracy: 0.7609\n",
            "Epoch 0, Batch 111, Training Loss: 0.39390432834625244, Validation Loss: 0.4797, Accuracy: 0.7688\n",
            "Epoch 0, Batch 112, Training Loss: 0.4733443856239319, Validation Loss: 0.4687, Accuracy: 0.7787\n",
            "Epoch 0, Batch 113, Training Loss: 0.5336874127388, Validation Loss: 0.4595, Accuracy: 0.7860\n",
            "Epoch 0, Batch 114, Training Loss: 0.4015059173107147, Validation Loss: 0.4621, Accuracy: 0.7847\n",
            "Epoch 0, Batch 115, Training Loss: 0.45749178528785706, Validation Loss: 0.4727, Accuracy: 0.7797\n",
            "Epoch 0, Batch 116, Training Loss: 0.39456093311309814, Validation Loss: 0.4734, Accuracy: 0.7796\n",
            "Epoch 0, Batch 117, Training Loss: 0.3094078600406647, Validation Loss: 0.4694, Accuracy: 0.7816\n",
            "Epoch 0, Batch 118, Training Loss: 0.30203887820243835, Validation Loss: 0.4680, Accuracy: 0.7856\n",
            "Epoch 0, Batch 119, Training Loss: 0.34668147563934326, Validation Loss: 0.4704, Accuracy: 0.7859\n",
            "Epoch 0, Batch 120, Training Loss: 0.7172833681106567, Validation Loss: 0.4697, Accuracy: 0.7873\n",
            "Epoch 0, Batch 121, Training Loss: 0.5422423481941223, Validation Loss: 0.4729, Accuracy: 0.7858\n",
            "Epoch 0, Batch 122, Training Loss: 0.45637863874435425, Validation Loss: 0.4813, Accuracy: 0.7827\n",
            "Epoch 0, Batch 123, Training Loss: 0.29729941487312317, Validation Loss: 0.4949, Accuracy: 0.7770\n",
            "Epoch 0, Batch 124, Training Loss: 0.3710317611694336, Validation Loss: 0.5069, Accuracy: 0.7726\n",
            "Epoch 0, Batch 125, Training Loss: 0.3992950916290283, Validation Loss: 0.4988, Accuracy: 0.7759\n",
            "Epoch 0, Batch 126, Training Loss: 0.4626343250274658, Validation Loss: 0.4779, Accuracy: 0.7833\n",
            "Epoch 0, Batch 127, Training Loss: 0.6867213249206543, Validation Loss: 0.4644, Accuracy: 0.7848\n",
            "Epoch 0, Batch 128, Training Loss: 0.3515319228172302, Validation Loss: 0.4654, Accuracy: 0.7810\n",
            "Epoch 0, Batch 129, Training Loss: 0.5689222812652588, Validation Loss: 0.4653, Accuracy: 0.7810\n",
            "Epoch 0, Batch 130, Training Loss: 0.3983454704284668, Validation Loss: 0.4632, Accuracy: 0.7826\n",
            "Epoch 0, Batch 131, Training Loss: 0.44352880120277405, Validation Loss: 0.4604, Accuracy: 0.7847\n",
            "Epoch 0, Batch 132, Training Loss: 0.4045514464378357, Validation Loss: 0.4602, Accuracy: 0.7875\n",
            "Epoch 0, Batch 133, Training Loss: 0.43998974561691284, Validation Loss: 0.4644, Accuracy: 0.7882\n",
            "Epoch 0, Batch 134, Training Loss: 0.45702463388442993, Validation Loss: 0.4737, Accuracy: 0.7807\n",
            "Epoch 0, Batch 135, Training Loss: 0.4309869408607483, Validation Loss: 0.4839, Accuracy: 0.7756\n",
            "Epoch 0, Batch 136, Training Loss: 0.5109032392501831, Validation Loss: 0.4959, Accuracy: 0.7688\n",
            "Epoch 0, Batch 137, Training Loss: 0.3908555209636688, Validation Loss: 0.5019, Accuracy: 0.7667\n",
            "Epoch 0, Batch 138, Training Loss: 0.5143738389015198, Validation Loss: 0.4870, Accuracy: 0.7746\n",
            "Epoch 0, Batch 139, Training Loss: 0.26755544543266296, Validation Loss: 0.4779, Accuracy: 0.7815\n",
            "Epoch 0, Batch 140, Training Loss: 0.5362586975097656, Validation Loss: 0.4717, Accuracy: 0.7846\n",
            "Epoch 0, Batch 141, Training Loss: 0.5759680867195129, Validation Loss: 0.4640, Accuracy: 0.7880\n",
            "Epoch 0, Batch 142, Training Loss: 0.32692134380340576, Validation Loss: 0.4600, Accuracy: 0.7896\n",
            "Epoch 0, Batch 143, Training Loss: 0.38466447591781616, Validation Loss: 0.4603, Accuracy: 0.7896\n",
            "Epoch 0, Batch 144, Training Loss: 0.438835471868515, Validation Loss: 0.4618, Accuracy: 0.7895\n",
            "Epoch 0, Batch 145, Training Loss: 0.6293764114379883, Validation Loss: 0.4620, Accuracy: 0.7887\n",
            "Epoch 0, Batch 146, Training Loss: 0.6347566843032837, Validation Loss: 0.4606, Accuracy: 0.7889\n",
            "Epoch 0, Batch 147, Training Loss: 0.4982527792453766, Validation Loss: 0.4573, Accuracy: 0.7896\n",
            "Epoch 0, Batch 148, Training Loss: 0.4781188368797302, Validation Loss: 0.4541, Accuracy: 0.7909\n",
            "Epoch 0, Batch 149, Training Loss: 0.5340241193771362, Validation Loss: 0.4537, Accuracy: 0.7911\n",
            "Epoch 0, Batch 150, Training Loss: 0.4629640281200409, Validation Loss: 0.4541, Accuracy: 0.7907\n",
            "Epoch 0, Batch 151, Training Loss: 0.4869430363178253, Validation Loss: 0.4547, Accuracy: 0.7906\n",
            "Epoch 0, Batch 152, Training Loss: 0.4374951124191284, Validation Loss: 0.4557, Accuracy: 0.7906\n",
            "Epoch 0, Batch 153, Training Loss: 0.5298227071762085, Validation Loss: 0.4560, Accuracy: 0.7916\n",
            "Epoch 0, Batch 154, Training Loss: 0.5553035140037537, Validation Loss: 0.4577, Accuracy: 0.7913\n",
            "Epoch 0, Batch 155, Training Loss: 0.5749623775482178, Validation Loss: 0.4619, Accuracy: 0.7902\n",
            "Epoch 0, Batch 156, Training Loss: 0.5619502663612366, Validation Loss: 0.4668, Accuracy: 0.7889\n",
            "Epoch 0, Batch 157, Training Loss: 0.552085816860199, Validation Loss: 0.4686, Accuracy: 0.7896\n",
            "Epoch 0, Batch 158, Training Loss: 0.3982633352279663, Validation Loss: 0.4688, Accuracy: 0.7902\n",
            "Epoch 0, Batch 159, Training Loss: 0.39911729097366333, Validation Loss: 0.4675, Accuracy: 0.7902\n",
            "Epoch 0, Batch 160, Training Loss: 0.4788944125175476, Validation Loss: 0.4648, Accuracy: 0.7913\n",
            "Epoch 0, Batch 161, Training Loss: 0.4104917049407959, Validation Loss: 0.4632, Accuracy: 0.7917\n",
            "Epoch 0, Batch 162, Training Loss: 0.4199157655239105, Validation Loss: 0.4606, Accuracy: 0.7917\n",
            "Epoch 0, Batch 163, Training Loss: 0.48881638050079346, Validation Loss: 0.4575, Accuracy: 0.7922\n",
            "Epoch 0, Batch 164, Training Loss: 0.42961716651916504, Validation Loss: 0.4559, Accuracy: 0.7900\n",
            "Epoch 0, Batch 165, Training Loss: 0.4277881383895874, Validation Loss: 0.4580, Accuracy: 0.7870\n",
            "Epoch 0, Batch 166, Training Loss: 0.6764489412307739, Validation Loss: 0.4587, Accuracy: 0.7858\n",
            "Epoch 0, Batch 167, Training Loss: 0.4035382866859436, Validation Loss: 0.4572, Accuracy: 0.7864\n",
            "Epoch 0, Batch 168, Training Loss: 0.4041709005832672, Validation Loss: 0.4542, Accuracy: 0.7882\n",
            "Epoch 0, Batch 169, Training Loss: 0.47922244668006897, Validation Loss: 0.4546, Accuracy: 0.7882\n",
            "Epoch 0, Batch 170, Training Loss: 0.6256586909294128, Validation Loss: 0.4530, Accuracy: 0.7896\n",
            "Epoch 0, Batch 171, Training Loss: 0.411334365606308, Validation Loss: 0.4517, Accuracy: 0.7898\n",
            "Epoch 0, Batch 172, Training Loss: 0.42384248971939087, Validation Loss: 0.4503, Accuracy: 0.7914\n",
            "Epoch 0, Batch 173, Training Loss: 0.6850342750549316, Validation Loss: 0.4493, Accuracy: 0.7911\n",
            "Epoch 0, Batch 174, Training Loss: 0.5671694874763489, Validation Loss: 0.4494, Accuracy: 0.7913\n",
            "Epoch 0, Batch 175, Training Loss: 0.400023490190506, Validation Loss: 0.4520, Accuracy: 0.7891\n",
            "Epoch 0, Batch 176, Training Loss: 0.4469916820526123, Validation Loss: 0.4561, Accuracy: 0.7872\n",
            "Epoch 0, Batch 177, Training Loss: 0.4152548015117645, Validation Loss: 0.4607, Accuracy: 0.7847\n",
            "Epoch 0, Batch 178, Training Loss: 0.6071027517318726, Validation Loss: 0.4649, Accuracy: 0.7821\n",
            "Epoch 0, Batch 179, Training Loss: 0.4551207423210144, Validation Loss: 0.4720, Accuracy: 0.7771\n",
            "Epoch 0, Batch 180, Training Loss: 0.5515664219856262, Validation Loss: 0.4752, Accuracy: 0.7749\n",
            "Epoch 0, Batch 181, Training Loss: 0.44770383834838867, Validation Loss: 0.4736, Accuracy: 0.7756\n",
            "Epoch 0, Batch 182, Training Loss: 0.6570641994476318, Validation Loss: 0.4712, Accuracy: 0.7790\n",
            "Epoch 0, Batch 183, Training Loss: 0.48053935170173645, Validation Loss: 0.4699, Accuracy: 0.7815\n",
            "Epoch 0, Batch 184, Training Loss: 0.6475518941879272, Validation Loss: 0.4709, Accuracy: 0.7835\n",
            "Epoch 0, Batch 185, Training Loss: 0.3621741533279419, Validation Loss: 0.4705, Accuracy: 0.7851\n",
            "Epoch 0, Batch 186, Training Loss: 0.417880117893219, Validation Loss: 0.4692, Accuracy: 0.7862\n",
            "Epoch 0, Batch 187, Training Loss: 0.5452566146850586, Validation Loss: 0.4680, Accuracy: 0.7870\n",
            "Epoch 0, Batch 188, Training Loss: 0.4092579483985901, Validation Loss: 0.4651, Accuracy: 0.7875\n",
            "Epoch 0, Batch 189, Training Loss: 0.3783310651779175, Validation Loss: 0.4605, Accuracy: 0.7877\n",
            "Epoch 0, Batch 190, Training Loss: 0.5531766414642334, Validation Loss: 0.4570, Accuracy: 0.7894\n",
            "Epoch 0, Batch 191, Training Loss: 0.6045520305633545, Validation Loss: 0.4548, Accuracy: 0.7905\n",
            "Epoch 0, Batch 192, Training Loss: 0.5095592141151428, Validation Loss: 0.4528, Accuracy: 0.7917\n",
            "Epoch 0, Batch 193, Training Loss: 0.4586344361305237, Validation Loss: 0.4513, Accuracy: 0.7916\n",
            "Epoch 0, Batch 194, Training Loss: 0.44772326946258545, Validation Loss: 0.4535, Accuracy: 0.7890\n",
            "Epoch 0, Batch 195, Training Loss: 0.4319148361682892, Validation Loss: 0.4570, Accuracy: 0.7853\n",
            "Epoch 0, Batch 196, Training Loss: 0.5327401757240295, Validation Loss: 0.4571, Accuracy: 0.7849\n",
            "Epoch 0, Batch 197, Training Loss: 0.43738776445388794, Validation Loss: 0.4520, Accuracy: 0.7885\n",
            "Epoch 0, Batch 198, Training Loss: 0.4899362623691559, Validation Loss: 0.4457, Accuracy: 0.7924\n",
            "Epoch 0, Batch 199, Training Loss: 0.46446847915649414, Validation Loss: 0.4437, Accuracy: 0.7945\n",
            "Epoch 0, Batch 200, Training Loss: 0.49786412715911865, Validation Loss: 0.4469, Accuracy: 0.7918\n",
            "Epoch 0, Batch 201, Training Loss: 0.3071425259113312, Validation Loss: 0.4515, Accuracy: 0.7888\n",
            "Epoch 0, Batch 202, Training Loss: 0.44394451379776, Validation Loss: 0.4569, Accuracy: 0.7850\n",
            "Epoch 0, Batch 203, Training Loss: 0.49788278341293335, Validation Loss: 0.4553, Accuracy: 0.7867\n",
            "Epoch 0, Batch 204, Training Loss: 0.4800882637500763, Validation Loss: 0.4489, Accuracy: 0.7920\n",
            "Epoch 0, Batch 205, Training Loss: 0.41293784976005554, Validation Loss: 0.4445, Accuracy: 0.7948\n",
            "Epoch 0, Batch 206, Training Loss: 0.5019311308860779, Validation Loss: 0.4439, Accuracy: 0.7949\n",
            "Epoch 0, Batch 207, Training Loss: 0.5313133597373962, Validation Loss: 0.4494, Accuracy: 0.7916\n",
            "Epoch 0, Batch 208, Training Loss: 0.5029210448265076, Validation Loss: 0.4560, Accuracy: 0.7866\n",
            "Epoch 0, Batch 209, Training Loss: 0.4345795810222626, Validation Loss: 0.4634, Accuracy: 0.7829\n",
            "Epoch 0, Batch 210, Training Loss: 0.5374258160591125, Validation Loss: 0.4601, Accuracy: 0.7848\n",
            "Epoch 0, Batch 211, Training Loss: 0.22618235647678375, Validation Loss: 0.4554, Accuracy: 0.7886\n",
            "Epoch 0, Batch 212, Training Loss: 0.5408318042755127, Validation Loss: 0.4480, Accuracy: 0.7931\n",
            "Epoch 0, Batch 213, Training Loss: 0.4454428553581238, Validation Loss: 0.4465, Accuracy: 0.7933\n",
            "Epoch 0, Batch 214, Training Loss: 0.516668975353241, Validation Loss: 0.4443, Accuracy: 0.7947\n",
            "Epoch 0, Batch 215, Training Loss: 0.34949228167533875, Validation Loss: 0.4437, Accuracy: 0.7939\n",
            "Epoch 0, Batch 216, Training Loss: 0.4421809911727905, Validation Loss: 0.4446, Accuracy: 0.7925\n",
            "Epoch 0, Batch 217, Training Loss: 0.5382089614868164, Validation Loss: 0.4459, Accuracy: 0.7906\n",
            "Epoch 0, Batch 218, Training Loss: 0.35585400462150574, Validation Loss: 0.4451, Accuracy: 0.7914\n",
            "Epoch 0, Batch 219, Training Loss: 0.5188359022140503, Validation Loss: 0.4444, Accuracy: 0.7916\n",
            "Epoch 0, Batch 220, Training Loss: 0.5542073249816895, Validation Loss: 0.4458, Accuracy: 0.7903\n",
            "Epoch 0, Batch 221, Training Loss: 0.5361238718032837, Validation Loss: 0.4476, Accuracy: 0.7896\n",
            "Epoch 0, Batch 222, Training Loss: 0.4119502604007721, Validation Loss: 0.4512, Accuracy: 0.7870\n",
            "Epoch 0, Batch 223, Training Loss: 0.34018686413764954, Validation Loss: 0.4551, Accuracy: 0.7830\n",
            "Epoch 0, Batch 224, Training Loss: 0.46683448553085327, Validation Loss: 0.4559, Accuracy: 0.7824\n",
            "Epoch 0, Batch 225, Training Loss: 0.33706527948379517, Validation Loss: 0.4542, Accuracy: 0.7837\n",
            "Epoch 0, Batch 226, Training Loss: 0.46990761160850525, Validation Loss: 0.4512, Accuracy: 0.7867\n",
            "Epoch 0, Batch 227, Training Loss: 0.40496471524238586, Validation Loss: 0.4480, Accuracy: 0.7901\n",
            "Epoch 0, Batch 228, Training Loss: 0.5782811045646667, Validation Loss: 0.4458, Accuracy: 0.7922\n",
            "Epoch 0, Batch 229, Training Loss: 0.36312633752822876, Validation Loss: 0.4457, Accuracy: 0.7924\n",
            "Epoch 0, Batch 230, Training Loss: 0.4926234185695648, Validation Loss: 0.4506, Accuracy: 0.7907\n",
            "Epoch 0, Batch 231, Training Loss: 0.28802257776260376, Validation Loss: 0.4608, Accuracy: 0.7864\n",
            "Epoch 0, Batch 232, Training Loss: 0.5329576730728149, Validation Loss: 0.4781, Accuracy: 0.7766\n",
            "Epoch 0, Batch 233, Training Loss: 0.40404340624809265, Validation Loss: 0.4938, Accuracy: 0.7682\n",
            "Epoch 0, Batch 234, Training Loss: 0.5325015783309937, Validation Loss: 0.4938, Accuracy: 0.7680\n",
            "Epoch 0, Batch 235, Training Loss: 0.5075881481170654, Validation Loss: 0.4835, Accuracy: 0.7729\n",
            "Epoch 0, Batch 236, Training Loss: 0.6137821674346924, Validation Loss: 0.4646, Accuracy: 0.7837\n",
            "Epoch 0, Batch 237, Training Loss: 0.6017045378684998, Validation Loss: 0.4503, Accuracy: 0.7897\n",
            "Epoch 0, Batch 238, Training Loss: 0.4302973747253418, Validation Loss: 0.4487, Accuracy: 0.7886\n",
            "Epoch 0, Batch 239, Training Loss: 0.31359219551086426, Validation Loss: 0.4560, Accuracy: 0.7849\n",
            "Epoch 0, Batch 240, Training Loss: 0.33270350098609924, Validation Loss: 0.4677, Accuracy: 0.7763\n",
            "Epoch 0, Batch 241, Training Loss: 0.5195460915565491, Validation Loss: 0.4769, Accuracy: 0.7685\n",
            "Epoch 0, Batch 242, Training Loss: 0.4087255597114563, Validation Loss: 0.4747, Accuracy: 0.7714\n",
            "Epoch 0, Batch 243, Training Loss: 0.4135275185108185, Validation Loss: 0.4687, Accuracy: 0.7763\n",
            "Epoch 0, Batch 244, Training Loss: 0.3506568670272827, Validation Loss: 0.4599, Accuracy: 0.7831\n",
            "Epoch 0, Batch 245, Training Loss: 0.5664375424385071, Validation Loss: 0.4541, Accuracy: 0.7859\n",
            "Epoch 0, Batch 246, Training Loss: 0.4130047857761383, Validation Loss: 0.4498, Accuracy: 0.7893\n",
            "Epoch 0, Batch 247, Training Loss: 0.4590313136577606, Validation Loss: 0.4513, Accuracy: 0.7883\n",
            "Epoch 0, Batch 248, Training Loss: 0.3833598494529724, Validation Loss: 0.4559, Accuracy: 0.7874\n",
            "Epoch 0, Batch 249, Training Loss: 0.3723483681678772, Validation Loss: 0.4644, Accuracy: 0.7817\n",
            "Epoch 0, Batch 250, Training Loss: 0.3856663405895233, Validation Loss: 0.4710, Accuracy: 0.7777\n",
            "Epoch 0, Batch 251, Training Loss: 0.599175751209259, Validation Loss: 0.4774, Accuracy: 0.7752\n",
            "Epoch 0, Batch 252, Training Loss: 0.6272484064102173, Validation Loss: 0.4823, Accuracy: 0.7713\n",
            "Epoch 0, Batch 253, Training Loss: 0.41578787565231323, Validation Loss: 0.4893, Accuracy: 0.7678\n",
            "Epoch 0, Batch 254, Training Loss: 0.3027873635292053, Validation Loss: 0.4971, Accuracy: 0.7646\n",
            "Epoch 0, Batch 255, Training Loss: 0.3865536153316498, Validation Loss: 0.5000, Accuracy: 0.7639\n",
            "Epoch 0, Batch 256, Training Loss: 0.55861496925354, Validation Loss: 0.4895, Accuracy: 0.7688\n",
            "Epoch 0, Batch 257, Training Loss: 0.5219358801841736, Validation Loss: 0.4793, Accuracy: 0.7740\n",
            "Epoch 0, Batch 258, Training Loss: 0.5393537282943726, Validation Loss: 0.4643, Accuracy: 0.7818\n",
            "Epoch 0, Batch 259, Training Loss: 0.43329906463623047, Validation Loss: 0.4537, Accuracy: 0.7871\n",
            "Epoch 0, Batch 260, Training Loss: 0.5644837617874146, Validation Loss: 0.4476, Accuracy: 0.7914\n",
            "Epoch 0, Batch 261, Training Loss: 0.5195103287696838, Validation Loss: 0.4456, Accuracy: 0.7920\n",
            "Epoch 0, Batch 262, Training Loss: 0.3004184365272522, Validation Loss: 0.4500, Accuracy: 0.7920\n",
            "Epoch 0, Batch 263, Training Loss: 0.511260986328125, Validation Loss: 0.4584, Accuracy: 0.7878\n",
            "Epoch 0, Batch 264, Training Loss: 0.492290198802948, Validation Loss: 0.4587, Accuracy: 0.7878\n",
            "Epoch 0, Batch 265, Training Loss: 0.46006184816360474, Validation Loss: 0.4549, Accuracy: 0.7892\n",
            "Epoch 0, Batch 266, Training Loss: 0.6500544548034668, Validation Loss: 0.4515, Accuracy: 0.7922\n",
            "Epoch 0, Batch 267, Training Loss: 0.33076515793800354, Validation Loss: 0.4503, Accuracy: 0.7934\n",
            "Epoch 0, Batch 268, Training Loss: 0.3907535672187805, Validation Loss: 0.4498, Accuracy: 0.7946\n",
            "Epoch 0, Batch 269, Training Loss: 0.37459564208984375, Validation Loss: 0.4491, Accuracy: 0.7944\n",
            "Epoch 0, Batch 270, Training Loss: 0.5635982155799866, Validation Loss: 0.4491, Accuracy: 0.7950\n",
            "Epoch 0, Batch 271, Training Loss: 0.4663761556148529, Validation Loss: 0.4488, Accuracy: 0.7961\n",
            "Epoch 0, Batch 272, Training Loss: 0.4713398814201355, Validation Loss: 0.4500, Accuracy: 0.7953\n",
            "Epoch 0, Batch 273, Training Loss: 0.4630212187767029, Validation Loss: 0.4518, Accuracy: 0.7931\n",
            "Epoch 0, Batch 274, Training Loss: 0.46516743302345276, Validation Loss: 0.4549, Accuracy: 0.7903\n",
            "Epoch 0, Batch 275, Training Loss: 0.4422197639942169, Validation Loss: 0.4609, Accuracy: 0.7856\n",
            "Epoch 0, Batch 276, Training Loss: 0.5675055980682373, Validation Loss: 0.4688, Accuracy: 0.7781\n",
            "Epoch 0, Batch 277, Training Loss: 0.4814065396785736, Validation Loss: 0.4756, Accuracy: 0.7713\n",
            "Epoch 0, Batch 278, Training Loss: 0.40641894936561584, Validation Loss: 0.4776, Accuracy: 0.7688\n",
            "Epoch 0, Batch 279, Training Loss: 0.376762717962265, Validation Loss: 0.4760, Accuracy: 0.7701\n",
            "Epoch 0, Batch 280, Training Loss: 0.38798773288726807, Validation Loss: 0.4699, Accuracy: 0.7759\n",
            "Epoch 0, Batch 281, Training Loss: 0.4074130654335022, Validation Loss: 0.4635, Accuracy: 0.7798\n",
            "Epoch 0, Batch 282, Training Loss: 0.698955774307251, Validation Loss: 0.4526, Accuracy: 0.7866\n",
            "Epoch 0, Batch 283, Training Loss: 0.3713501989841461, Validation Loss: 0.4466, Accuracy: 0.7908\n",
            "Epoch 0, Batch 284, Training Loss: 0.43456053733825684, Validation Loss: 0.4409, Accuracy: 0.7961\n",
            "Epoch 0, Batch 285, Training Loss: 0.5573816895484924, Validation Loss: 0.4391, Accuracy: 0.7986\n",
            "Epoch 0, Batch 286, Training Loss: 0.6273925304412842, Validation Loss: 0.4388, Accuracy: 0.7989\n",
            "Epoch 0, Batch 287, Training Loss: 0.5091561675071716, Validation Loss: 0.4385, Accuracy: 0.7989\n",
            "Epoch 0, Batch 288, Training Loss: 0.4987916350364685, Validation Loss: 0.4381, Accuracy: 0.7989\n",
            "Epoch 0, Batch 289, Training Loss: 0.49372440576553345, Validation Loss: 0.4375, Accuracy: 0.7984\n",
            "Epoch 0, Batch 290, Training Loss: 0.3089243173599243, Validation Loss: 0.4376, Accuracy: 0.7985\n",
            "Epoch 0, Batch 291, Training Loss: 0.32228726148605347, Validation Loss: 0.4382, Accuracy: 0.7975\n",
            "Epoch 0, Batch 292, Training Loss: 0.383078932762146, Validation Loss: 0.4385, Accuracy: 0.7973\n",
            "Epoch 0, Batch 293, Training Loss: 0.45894932746887207, Validation Loss: 0.4395, Accuracy: 0.7971\n",
            "Epoch 0, Batch 294, Training Loss: 0.4924771189689636, Validation Loss: 0.4399, Accuracy: 0.7965\n",
            "Epoch 0, Batch 295, Training Loss: 0.5610572695732117, Validation Loss: 0.4415, Accuracy: 0.7956\n",
            "Epoch 0, Batch 296, Training Loss: 0.35539931058883667, Validation Loss: 0.4418, Accuracy: 0.7957\n",
            "Epoch 0, Batch 297, Training Loss: 0.32748112082481384, Validation Loss: 0.4426, Accuracy: 0.7951\n",
            "Epoch 0, Batch 298, Training Loss: 0.3191578984260559, Validation Loss: 0.4423, Accuracy: 0.7953\n",
            "Epoch 0, Batch 299, Training Loss: 0.599361777305603, Validation Loss: 0.4428, Accuracy: 0.7955\n",
            "Epoch 0, Batch 300, Training Loss: 0.5568054914474487, Validation Loss: 0.4399, Accuracy: 0.7981\n",
            "Epoch 0, Batch 301, Training Loss: 0.30134719610214233, Validation Loss: 0.4381, Accuracy: 0.7996\n",
            "Epoch 0, Batch 302, Training Loss: 0.40368786454200745, Validation Loss: 0.4371, Accuracy: 0.7994\n",
            "Epoch 0, Batch 303, Training Loss: 0.47240617871284485, Validation Loss: 0.4396, Accuracy: 0.7972\n",
            "Epoch 0, Batch 304, Training Loss: 0.6808102130889893, Validation Loss: 0.4427, Accuracy: 0.7964\n",
            "Epoch 0, Batch 305, Training Loss: 0.41352057456970215, Validation Loss: 0.4448, Accuracy: 0.7947\n",
            "Epoch 0, Batch 306, Training Loss: 0.1822906881570816, Validation Loss: 0.4486, Accuracy: 0.7916\n",
            "Epoch 0, Batch 307, Training Loss: 0.5053141117095947, Validation Loss: 0.4491, Accuracy: 0.7909\n",
            "Epoch 0, Batch 308, Training Loss: 0.3367493152618408, Validation Loss: 0.4458, Accuracy: 0.7930\n",
            "Epoch 0, Batch 309, Training Loss: 0.4926590025424957, Validation Loss: 0.4414, Accuracy: 0.7954\n",
            "Epoch 0, Batch 310, Training Loss: 0.4652068614959717, Validation Loss: 0.4393, Accuracy: 0.7967\n",
            "Epoch 0, Batch 311, Training Loss: 0.5010004043579102, Validation Loss: 0.4379, Accuracy: 0.7987\n",
            "Epoch 0, Batch 312, Training Loss: 0.4754839539527893, Validation Loss: 0.4374, Accuracy: 0.7998\n",
            "Epoch 0, Batch 313, Training Loss: 0.3637387752532959, Validation Loss: 0.4397, Accuracy: 0.7977\n",
            "Epoch 0, Batch 314, Training Loss: 0.46590256690979004, Validation Loss: 0.4428, Accuracy: 0.7961\n",
            "Epoch 0, Batch 315, Training Loss: 0.547806441783905, Validation Loss: 0.4415, Accuracy: 0.7968\n",
            "Epoch 0, Batch 316, Training Loss: 0.3561132550239563, Validation Loss: 0.4409, Accuracy: 0.7979\n",
            "Epoch 0, Batch 317, Training Loss: 0.4896639585494995, Validation Loss: 0.4417, Accuracy: 0.7967\n",
            "Epoch 0, Batch 318, Training Loss: 0.42561984062194824, Validation Loss: 0.4413, Accuracy: 0.7982\n",
            "Epoch 0, Batch 319, Training Loss: 0.45754438638687134, Validation Loss: 0.4407, Accuracy: 0.7983\n",
            "Epoch 0, Batch 320, Training Loss: 0.32356032729148865, Validation Loss: 0.4400, Accuracy: 0.7983\n",
            "Epoch 0, Batch 321, Training Loss: 0.41405847668647766, Validation Loss: 0.4378, Accuracy: 0.7992\n",
            "Epoch 0, Batch 322, Training Loss: 0.47602203488349915, Validation Loss: 0.4365, Accuracy: 0.7995\n",
            "Epoch 0, Batch 323, Training Loss: 0.3105248212814331, Validation Loss: 0.4355, Accuracy: 0.7995\n",
            "Epoch 0, Batch 324, Training Loss: 0.4103989899158478, Validation Loss: 0.4352, Accuracy: 0.7994\n",
            "Epoch 0, Batch 325, Training Loss: 0.2838612496852875, Validation Loss: 0.4351, Accuracy: 0.7994\n",
            "Epoch 0, Batch 326, Training Loss: 0.4370342195034027, Validation Loss: 0.4353, Accuracy: 0.7990\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 1\n",
        "best_model_state = None\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model_state, val_loss = train(epoch, model, train_loader, test_loader, optimizer)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state = model_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9cd00d-8139-4d46-8a3f-ebe9335587b7",
      "metadata": {
        "id": "7f9cd00d-8139-4d46-8a3f-ebe9335587b7"
      },
      "outputs": [],
      "source": [
        "# Load the best model state\n",
        "if best_model_state:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "# Print the parameters of the model with the minimum validation loss\n",
        "print(\"Parameters of the model with the minimum validation loss:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.data}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Roberta"
      ],
      "metadata": {
        "id": "7QzpXURiyd-w"
      },
      "id": "7QzpXURiyd-w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf14e5f-2254-4d62-b319-449fbb24475c",
      "metadata": {
        "id": "fdf14e5f-2254-4d62-b319-449fbb24475c",
        "outputId": "68e591af-4c10-4aba-f4aa-25e919284c24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 0, Loss: 0.6756503582000732\n",
            "Epoch 0, Batch 100, Loss: 0.3952513635158539\n",
            "Epoch 0, Batch 200, Loss: 0.3906814455986023\n",
            "Epoch 0, Batch 300, Loss: 0.43320342898368835\n",
            "Epoch 0, Batch 400, Loss: 0.37270355224609375\n",
            "Epoch 0, Batch 500, Loss: 0.48311737179756165\n",
            "Epoch 0, Batch 600, Loss: 0.3824498951435089\n",
            "Epoch 0, Batch 700, Loss: 0.4639070928096771\n",
            "Epoch 0, Batch 800, Loss: 0.4812706708908081\n",
            "Epoch 0, Batch 900, Loss: 0.4120330810546875\n",
            "Epoch 0, Batch 1000, Loss: 0.5286306738853455\n",
            "Epoch 0, Batch 1100, Loss: 0.38292133808135986\n",
            "Epoch 0, Batch 1200, Loss: 0.5042079091072083\n",
            "Epoch 0, Batch 1300, Loss: 0.4365707337856293\n",
            "Epoch 0, Batch 1400, Loss: 0.43188339471817017\n",
            "Epoch 0, Batch 1500, Loss: 0.5428134202957153\n",
            "Epoch 0, Batch 1600, Loss: 0.4794994294643402\n",
            "Epoch 0, Batch 1700, Loss: 0.3960758149623871\n",
            "Epoch 0, Batch 1800, Loss: 0.3293410539627075\n",
            "Epoch 0, Batch 1900, Loss: 0.4995534420013428\n",
            "Epoch 0, Batch 2000, Loss: 0.30456405878067017\n",
            "Epoch 0, Batch 2100, Loss: 0.34314218163490295\n",
            "Epoch 0, Batch 2200, Loss: 0.47361263632774353\n",
            "Epoch 0, Batch 2300, Loss: 0.5113583207130432\n",
            "Epoch 0, Batch 2400, Loss: 0.36227959394454956\n",
            "Epoch 0, Batch 2500, Loss: 0.45579397678375244\n",
            "Epoch 0, Batch 2600, Loss: 0.38330182433128357\n",
            "Epoch 0, Batch 2700, Loss: 0.4288980960845947\n",
            "Epoch 1/1, Validation Loss: 0.4504, Accuracy: 0.7897\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data_sampled['text'].tolist(),\n",
        "    data_sampled['label'].tolist(),\n",
        "    test_size=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenize(train_texts)\n",
        "test_encodings = tokenize(test_texts)\n",
        "\n",
        "# Define a custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx]).clone().detach()\n",
        "        return item\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TextDataset(train_encodings, train_labels)\n",
        "test_dataset = TextDataset(test_encodings, test_labels)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load the model\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training function\n",
        "def train(epoch, model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            logits = outputs.logits\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            label_ids = labels.cpu().numpy()\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(label_ids)\n",
        "\n",
        "    average_loss = total_loss / len(test_loader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy, average_loss\n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch, model, train_loader, optimizer)\n",
        "    accuracy, val_loss = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT2"
      ],
      "metadata": {
        "id": "hfbKwyuDykTT"
      },
      "id": "hfbKwyuDykTT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a09b340-5f08-4f33-8198-d77ea7f8b267",
      "metadata": {
        "id": "4a09b340-5f08-4f33-8198-d77ea7f8b267",
        "outputId": "672eb7ac-6783-4201-a48f-b75d66316d80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "C:\\Users\\tatev\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:650: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 0, Loss: 0.982352077960968\n",
            "Epoch 0, Batch 100, Loss: 0.5775084495544434\n",
            "Epoch 0, Batch 200, Loss: 0.3348778188228607\n",
            "Epoch 0, Batch 300, Loss: 0.4932810068130493\n",
            "Epoch 0, Batch 400, Loss: 0.6009963750839233\n",
            "Epoch 0, Batch 500, Loss: 0.5519117116928101\n",
            "Epoch 0, Batch 600, Loss: 0.3971640467643738\n",
            "Epoch 0, Batch 700, Loss: 0.5178024172782898\n",
            "Epoch 0, Batch 800, Loss: 0.4836016893386841\n",
            "Epoch 0, Batch 900, Loss: 0.585265040397644\n",
            "Epoch 0, Batch 1000, Loss: 0.460182249546051\n",
            "Epoch 0, Batch 1100, Loss: 0.7870944142341614\n",
            "Epoch 0, Batch 1200, Loss: 0.6717519164085388\n",
            "Epoch 0, Batch 1300, Loss: 0.6331472396850586\n",
            "Epoch 0, Batch 1400, Loss: 0.49488747119903564\n",
            "Epoch 0, Batch 1500, Loss: 0.5924420952796936\n",
            "Epoch 0, Batch 1600, Loss: 0.3732205033302307\n",
            "Epoch 0, Batch 1700, Loss: 0.580284833908081\n",
            "Epoch 0, Batch 1800, Loss: 0.6608132123947144\n",
            "Epoch 0, Batch 1900, Loss: 0.8326858878135681\n",
            "Epoch 0, Batch 2000, Loss: 0.4280773103237152\n",
            "Epoch 0, Batch 2100, Loss: 0.6026865243911743\n",
            "Epoch 0, Batch 2200, Loss: 0.743364691734314\n",
            "Epoch 0, Batch 2300, Loss: 0.3342837393283844\n",
            "Epoch 0, Batch 2400, Loss: 0.5447114706039429\n",
            "Epoch 0, Batch 2500, Loss: 0.3481760621070862\n",
            "Epoch 0, Batch 2600, Loss: 0.15287663042545319\n",
            "Epoch 0, Batch 2700, Loss: 0.4977690279483795\n",
            "Epoch 0, Batch 2800, Loss: 0.23807230591773987\n",
            "Epoch 0, Batch 2900, Loss: 0.14460356533527374\n",
            "Epoch 0, Batch 3000, Loss: 0.6719057559967041\n",
            "Epoch 0, Batch 3100, Loss: 0.2938390374183655\n",
            "Epoch 0, Batch 3200, Loss: 0.2806963622570038\n",
            "Epoch 0, Batch 3300, Loss: 0.40514975786209106\n",
            "Epoch 0, Batch 3400, Loss: 0.5950378179550171\n",
            "Epoch 0, Batch 3500, Loss: 0.41618287563323975\n",
            "Epoch 0, Batch 3600, Loss: 0.5475724339485168\n",
            "Epoch 0, Batch 3700, Loss: 0.5962461233139038\n",
            "Epoch 0, Batch 3800, Loss: 0.4339989423751831\n",
            "Epoch 0, Batch 3900, Loss: 0.35483497381210327\n",
            "Epoch 0, Batch 4000, Loss: 0.3299378752708435\n",
            "Epoch 0, Batch 4100, Loss: 0.28110796213150024\n",
            "Epoch 0, Batch 4200, Loss: 0.21685051918029785\n",
            "Epoch 0, Batch 4300, Loss: 0.25169241428375244\n",
            "Epoch 0, Batch 4400, Loss: 0.6848815679550171\n",
            "Epoch 0, Batch 4500, Loss: 0.3215022087097168\n",
            "Epoch 0, Batch 4600, Loss: 0.6274864673614502\n",
            "Epoch 0, Batch 4700, Loss: 0.4504941701889038\n",
            "Epoch 0, Batch 4800, Loss: 0.4267723262310028\n",
            "Epoch 0, Batch 4900, Loss: 0.5100489854812622\n",
            "Epoch 0, Batch 5000, Loss: 0.3027360439300537\n",
            "Epoch 0, Batch 5100, Loss: 0.27442577481269836\n",
            "Epoch 0, Batch 5200, Loss: 0.5761757493019104\n",
            "Epoch 0, Batch 5300, Loss: 0.461064875125885\n",
            "Epoch 0, Batch 5400, Loss: 0.5763835310935974\n",
            "Epoch 0, Batch 5500, Loss: 0.24287396669387817\n",
            "Epoch 0, Batch 5600, Loss: 0.37903696298599243\n",
            "Epoch 0, Batch 5700, Loss: 0.5854408144950867\n",
            "Epoch 0, Batch 5800, Loss: 0.35805079340934753\n",
            "Epoch 0, Batch 5900, Loss: 0.3566526472568512\n",
            "Epoch 0, Batch 6000, Loss: 0.6033740639686584\n",
            "Epoch 0, Batch 6100, Loss: 0.5659238696098328\n",
            "Epoch 0, Batch 6200, Loss: 0.1740071326494217\n",
            "Epoch 0, Batch 6300, Loss: 0.7161898016929626\n",
            "Epoch 0, Batch 6400, Loss: 0.3604906499385834\n",
            "Epoch 0, Batch 6500, Loss: 0.3599347770214081\n",
            "Epoch 0, Batch 6600, Loss: 0.23166805505752563\n",
            "Epoch 0, Batch 6700, Loss: 0.35540899634361267\n",
            "Epoch 0, Batch 6800, Loss: 0.558289110660553\n",
            "Epoch 0, Batch 6900, Loss: 0.46859514713287354\n",
            "Epoch 0, Batch 7000, Loss: 0.6116130352020264\n",
            "Epoch 0, Batch 7100, Loss: 0.27502667903900146\n",
            "Epoch 0, Batch 7200, Loss: 0.7941469550132751\n",
            "Epoch 0, Batch 7300, Loss: 0.34350576996803284\n",
            "Epoch 0, Batch 7400, Loss: 0.4485729932785034\n",
            "Epoch 0, Batch 7500, Loss: 0.694451630115509\n",
            "Epoch 0, Batch 7600, Loss: 0.2449614405632019\n",
            "Epoch 0, Batch 7700, Loss: 0.518154501914978\n",
            "Epoch 0, Batch 7800, Loss: 0.8541887402534485\n",
            "Epoch 0, Batch 7900, Loss: 0.7847326993942261\n",
            "Epoch 0, Batch 8000, Loss: 0.1938651204109192\n",
            "Epoch 0, Batch 8100, Loss: 0.10911331325769424\n",
            "Epoch 0, Batch 8200, Loss: 0.27842622995376587\n",
            "Epoch 0, Batch 8300, Loss: 0.44683656096458435\n",
            "Epoch 0, Batch 8400, Loss: 0.3895629644393921\n",
            "Epoch 0, Batch 8500, Loss: 0.21209844946861267\n",
            "Epoch 0, Batch 8600, Loss: 0.2394983321428299\n",
            "Epoch 0, Batch 8700, Loss: 0.40234318375587463\n",
            "Epoch 0, Batch 8800, Loss: 0.46028047800064087\n",
            "Epoch 0, Batch 8900, Loss: 0.1680452823638916\n",
            "Epoch 0, Batch 9000, Loss: 0.5088574886322021\n",
            "Epoch 0, Batch 9100, Loss: 0.49804115295410156\n",
            "Epoch 0, Batch 9200, Loss: 0.44314756989479065\n",
            "Epoch 0, Batch 9300, Loss: 0.7318070530891418\n",
            "Epoch 0, Batch 9400, Loss: 0.4302412271499634\n",
            "Epoch 0, Batch 9500, Loss: 0.42038220167160034\n",
            "Epoch 0, Batch 9600, Loss: 0.44817912578582764\n",
            "Epoch 0, Batch 9700, Loss: 0.4545030891895294\n",
            "Epoch 0, Batch 9800, Loss: 0.07663971185684204\n",
            "Epoch 0, Batch 9900, Loss: 0.38005393743515015\n",
            "Epoch 0, Batch 10000, Loss: 0.6113444566726685\n",
            "Epoch 0, Batch 10100, Loss: 0.2952859103679657\n",
            "Epoch 0, Batch 10200, Loss: 0.35297125577926636\n",
            "Epoch 0, Batch 10300, Loss: 0.5074642300605774\n",
            "Epoch 0, Batch 10400, Loss: 0.5269805788993835\n",
            "Epoch 0, Batch 10500, Loss: 0.4590000510215759\n",
            "Epoch 0, Batch 10600, Loss: 0.3346591293811798\n",
            "Epoch 0, Batch 10700, Loss: 0.42881426215171814\n",
            "Epoch 0, Batch 10800, Loss: 0.3553483486175537\n",
            "Epoch 0, Batch 10900, Loss: 0.3737650215625763\n",
            "Epoch 0, Batch 11000, Loss: 0.29811719059944153\n",
            "Epoch 0, Batch 11100, Loss: 0.7201903462409973\n",
            "Epoch 0, Batch 11200, Loss: 0.5711596608161926\n",
            "Epoch 0, Batch 11300, Loss: 0.25133055448532104\n",
            "Epoch 0, Batch 11400, Loss: 0.37534961104393005\n",
            "Epoch 0, Batch 11500, Loss: 0.35090649127960205\n",
            "Epoch 0, Batch 11600, Loss: 0.49056488275527954\n",
            "Epoch 0, Batch 11700, Loss: 0.4715082347393036\n",
            "Epoch 0, Batch 11800, Loss: 0.7131327390670776\n",
            "Epoch 0, Batch 11900, Loss: 0.23439501225948334\n",
            "Epoch 0, Batch 12000, Loss: 0.7842357754707336\n",
            "Epoch 0, Batch 12100, Loss: 0.34166741371154785\n",
            "Epoch 0, Batch 12200, Loss: 0.45404326915740967\n",
            "Epoch 0, Batch 12300, Loss: 0.3182141184806824\n",
            "Epoch 0, Batch 12400, Loss: 0.6790580749511719\n",
            "Epoch 0, Batch 12500, Loss: 0.38295239210128784\n",
            "Epoch 0, Batch 12600, Loss: 0.2658498287200928\n",
            "Epoch 0, Batch 12700, Loss: 0.20381537079811096\n",
            "Epoch 0, Batch 12800, Loss: 0.35523682832717896\n",
            "Epoch 0, Batch 12900, Loss: 0.74398273229599\n",
            "Epoch 0, Batch 13000, Loss: 0.41282057762145996\n",
            "Epoch 0, Batch 13100, Loss: 0.5195580124855042\n",
            "Epoch 0, Batch 13200, Loss: 0.41940343379974365\n",
            "Epoch 0, Batch 13300, Loss: 0.6031771302223206\n",
            "Epoch 0, Batch 13400, Loss: 0.7097545266151428\n",
            "Epoch 0, Batch 13500, Loss: 0.3747085630893707\n",
            "Epoch 0, Batch 13600, Loss: 0.43998414278030396\n",
            "Epoch 0, Batch 13700, Loss: 0.42986443638801575\n",
            "Epoch 0, Batch 13800, Loss: 0.5502349734306335\n",
            "Epoch 0, Batch 13900, Loss: 0.3304617404937744\n",
            "Epoch 0, Batch 14000, Loss: 0.5859197378158569\n",
            "Epoch 0, Batch 14100, Loss: 0.6415817141532898\n",
            "Epoch 0, Batch 14200, Loss: 0.5087215900421143\n",
            "Epoch 0, Batch 14300, Loss: 0.26273322105407715\n",
            "Epoch 0, Batch 14400, Loss: 0.21111242473125458\n",
            "Epoch 0, Batch 14500, Loss: 0.17567351460456848\n",
            "Epoch 0, Batch 14600, Loss: 0.36315950751304626\n",
            "Epoch 0, Batch 14700, Loss: 0.8072512149810791\n",
            "Epoch 0, Batch 14800, Loss: 0.30854907631874084\n",
            "Epoch 0, Batch 14900, Loss: 0.20647655427455902\n",
            "Epoch 0, Batch 15000, Loss: 0.6703861951828003\n",
            "Epoch 0, Batch 15100, Loss: 0.26772141456604004\n",
            "Epoch 0, Batch 15200, Loss: 0.5432616472244263\n",
            "Epoch 0, Batch 15300, Loss: 0.8779114484786987\n",
            "Epoch 0, Batch 15400, Loss: 0.10781682282686234\n",
            "Epoch 0, Batch 15500, Loss: 0.7936629056930542\n",
            "Epoch 0, Batch 15600, Loss: 0.34744107723236084\n",
            "Epoch 0, Batch 15700, Loss: 0.13661424815654755\n",
            "Epoch 0, Batch 15800, Loss: 0.3012489974498749\n",
            "Epoch 0, Batch 15900, Loss: 0.2776249349117279\n",
            "Epoch 0, Batch 16000, Loss: 0.5941017866134644\n",
            "Epoch 0, Batch 16100, Loss: 0.21994854509830475\n",
            "Epoch 0, Batch 16200, Loss: 0.542540431022644\n",
            "Epoch 0, Batch 16300, Loss: 0.28553926944732666\n",
            "Epoch 0, Batch 16400, Loss: 0.6191322803497314\n",
            "Epoch 0, Batch 16500, Loss: 0.4851233959197998\n",
            "Epoch 0, Batch 16600, Loss: 0.28704261779785156\n",
            "Epoch 0, Batch 16700, Loss: 0.5498203039169312\n",
            "Epoch 0, Batch 16800, Loss: 0.45363977551460266\n",
            "Epoch 0, Batch 16900, Loss: 0.34831464290618896\n",
            "Epoch 0, Batch 17000, Loss: 0.15677142143249512\n",
            "Epoch 0, Batch 17100, Loss: 0.6132306456565857\n",
            "Epoch 0, Batch 17200, Loss: 0.5291150212287903\n",
            "Epoch 0, Batch 17300, Loss: 0.4332887828350067\n",
            "Epoch 0, Batch 17400, Loss: 0.3138696253299713\n",
            "Epoch 0, Batch 17500, Loss: 0.4427468776702881\n",
            "Epoch 0, Batch 17600, Loss: 0.60951828956604\n",
            "Epoch 0, Batch 17700, Loss: 0.7614219188690186\n",
            "Epoch 0, Batch 17800, Loss: 0.3412207365036011\n",
            "Epoch 0, Batch 17900, Loss: 0.4689582586288452\n",
            "Epoch 0, Batch 18000, Loss: 0.4203169047832489\n",
            "Epoch 0, Batch 18100, Loss: 0.4410618245601654\n",
            "Epoch 0, Batch 18200, Loss: 0.43756037950515747\n",
            "Epoch 0, Batch 18300, Loss: 0.6148132681846619\n",
            "Epoch 0, Batch 18400, Loss: 0.24178706109523773\n",
            "Epoch 0, Batch 18500, Loss: 0.6890113353729248\n",
            "Epoch 0, Batch 18600, Loss: 0.6314603090286255\n",
            "Epoch 0, Batch 18700, Loss: 0.39956533908843994\n",
            "Epoch 0, Batch 18800, Loss: 0.5486745238304138\n",
            "Epoch 0, Batch 18900, Loss: 0.30039435625076294\n",
            "Epoch 0, Batch 19000, Loss: 0.3919273018836975\n",
            "Epoch 0, Batch 19100, Loss: 0.6947906017303467\n",
            "Epoch 0, Batch 19200, Loss: 0.21977174282073975\n",
            "Epoch 0, Batch 19300, Loss: 0.8973896503448486\n",
            "Epoch 0, Batch 19400, Loss: 0.31981414556503296\n",
            "Epoch 0, Batch 19500, Loss: 0.6150702834129333\n",
            "Epoch 0, Batch 19600, Loss: 0.19082576036453247\n",
            "Epoch 0, Batch 19700, Loss: 0.37822604179382324\n",
            "Epoch 0, Batch 19800, Loss: 1.198364496231079\n",
            "Epoch 0, Batch 19900, Loss: 0.09605622291564941\n",
            "Epoch 0, Batch 20000, Loss: 0.3387463688850403\n",
            "Epoch 0, Batch 20100, Loss: 0.38481268286705017\n",
            "Epoch 0, Batch 20200, Loss: 0.764262855052948\n",
            "Epoch 0, Batch 20300, Loss: 0.34043073654174805\n",
            "Epoch 0, Batch 20400, Loss: 0.2670947313308716\n",
            "Epoch 0, Batch 20500, Loss: 0.2636308968067169\n",
            "Epoch 0, Batch 20600, Loss: 0.24150404334068298\n",
            "Epoch 0, Batch 20700, Loss: 0.2951754629611969\n",
            "Epoch 0, Batch 20800, Loss: 0.3955353796482086\n",
            "Epoch 0, Batch 20900, Loss: 0.5957849025726318\n",
            "Epoch 0, Batch 21000, Loss: 0.7608007192611694\n",
            "Epoch 0, Batch 21100, Loss: 0.40120983123779297\n",
            "Epoch 0, Batch 21200, Loss: 0.4351099133491516\n",
            "Epoch 0, Batch 21300, Loss: 0.4358093738555908\n",
            "Epoch 0, Batch 21400, Loss: 0.25048959255218506\n",
            "Epoch 0, Batch 21500, Loss: 0.2908343970775604\n",
            "Epoch 0, Batch 21600, Loss: 0.28128334879875183\n",
            "Epoch 0, Batch 21700, Loss: 0.3597422242164612\n",
            "Epoch 0, Batch 21800, Loss: 0.3047512173652649\n",
            "Epoch 0, Batch 21900, Loss: 0.5132302641868591\n",
            "Epoch 0, Batch 22000, Loss: 0.14237946271896362\n",
            "Epoch 0, Batch 22100, Loss: 0.48941099643707275\n",
            "Epoch 0, Batch 22200, Loss: 0.46794623136520386\n",
            "Epoch 0, Batch 22300, Loss: 0.4267579913139343\n",
            "Epoch 0, Batch 22400, Loss: 0.25382763147354126\n",
            "Epoch 0, Batch 22500, Loss: 0.49199140071868896\n",
            "Epoch 0, Batch 22600, Loss: 0.5267780423164368\n",
            "Epoch 0, Batch 22700, Loss: 0.5462071895599365\n",
            "Epoch 0, Batch 22800, Loss: 0.5256377458572388\n",
            "Epoch 0, Batch 22900, Loss: 0.473455011844635\n",
            "Epoch 0, Batch 23000, Loss: 0.48154133558273315\n",
            "Epoch 0, Batch 23100, Loss: 0.8964331150054932\n",
            "Epoch 0, Batch 23200, Loss: 0.3368661105632782\n",
            "Epoch 0, Batch 23300, Loss: 0.20168623328208923\n",
            "Epoch 0, Batch 23400, Loss: 0.3637402057647705\n",
            "Epoch 0, Batch 23500, Loss: 0.7111705541610718\n",
            "Epoch 0, Batch 23600, Loss: 0.14628788828849792\n",
            "Epoch 0, Batch 23700, Loss: 0.2552645206451416\n",
            "Epoch 0, Batch 23800, Loss: 0.4634087085723877\n",
            "Epoch 0, Batch 23900, Loss: 0.5256068110466003\n",
            "Epoch 0, Batch 24000, Loss: 0.47492074966430664\n",
            "Epoch 0, Batch 24100, Loss: 0.26213860511779785\n",
            "Epoch 0, Batch 24200, Loss: 0.15039993822574615\n",
            "Epoch 0, Batch 24300, Loss: 0.3071483373641968\n",
            "Epoch 0, Batch 24400, Loss: 0.12671519815921783\n",
            "Epoch 0, Batch 24500, Loss: 0.19787022471427917\n",
            "Epoch 0, Batch 24600, Loss: 0.24480678141117096\n",
            "Epoch 0, Batch 24700, Loss: 0.35101231932640076\n",
            "Epoch 0, Batch 24800, Loss: 0.2354099452495575\n",
            "Epoch 0, Batch 24900, Loss: 0.19382387399673462\n",
            "Epoch 0, Batch 25000, Loss: 0.4015882909297943\n",
            "Epoch 0, Batch 25100, Loss: 0.4607815146446228\n",
            "Epoch 0, Batch 25200, Loss: 0.17159345746040344\n",
            "Epoch 0, Batch 25300, Loss: 0.531589686870575\n",
            "Epoch 0, Batch 25400, Loss: 0.3644386827945709\n",
            "Epoch 0, Batch 25500, Loss: 0.25949928164482117\n",
            "Epoch 0, Batch 25600, Loss: 0.37819528579711914\n",
            "Epoch 0, Batch 25700, Loss: 0.5095247030258179\n",
            "Epoch 0, Batch 25800, Loss: 0.39246171712875366\n",
            "Epoch 0, Batch 25900, Loss: 0.49131107330322266\n",
            "Epoch 0, Batch 26000, Loss: 0.5803475975990295\n",
            "Epoch 0, Batch 26100, Loss: 0.4254126250743866\n",
            "Epoch 0, Batch 26200, Loss: 0.2968187928199768\n",
            "Epoch 0, Batch 26300, Loss: 0.3962220847606659\n",
            "Epoch 0, Batch 26400, Loss: 0.7740563750267029\n",
            "Epoch 0, Batch 26500, Loss: 0.42994940280914307\n",
            "Epoch 0, Batch 26600, Loss: 0.2378220409154892\n",
            "Epoch 0, Batch 26700, Loss: 0.37965863943099976\n",
            "Epoch 0, Batch 26800, Loss: 0.2550447881221771\n",
            "Epoch 0, Batch 26900, Loss: 0.4237825870513916\n",
            "Epoch 0, Batch 27000, Loss: 0.24442829191684723\n",
            "Epoch 0, Batch 27100, Loss: 0.39253488183021545\n",
            "Epoch 0, Batch 27200, Loss: 0.28010478615760803\n",
            "Epoch 0, Batch 27300, Loss: 0.21468228101730347\n",
            "Epoch 0, Batch 27400, Loss: 0.24306650459766388\n",
            "Epoch 0, Batch 27500, Loss: 0.1725846379995346\n",
            "Epoch 0, Batch 27600, Loss: 0.5010965466499329\n",
            "Epoch 0, Batch 27700, Loss: 0.44159090518951416\n",
            "Epoch 0, Batch 27800, Loss: 0.4420352578163147\n",
            "Epoch 0, Batch 27900, Loss: 0.6021536588668823\n",
            "Epoch 0, Batch 28000, Loss: 0.30746757984161377\n",
            "Epoch 0, Batch 28100, Loss: 0.44451767206192017\n",
            "Epoch 0, Batch 28200, Loss: 0.38088393211364746\n",
            "Epoch 0, Batch 28300, Loss: 0.44193029403686523\n",
            "Epoch 0, Batch 28400, Loss: 0.5438643097877502\n",
            "Epoch 0, Batch 28500, Loss: 0.45704296231269836\n",
            "Epoch 0, Batch 28600, Loss: 0.23263812065124512\n",
            "Epoch 0, Batch 28700, Loss: 0.4123268723487854\n",
            "Epoch 0, Batch 28800, Loss: 0.6464516520500183\n",
            "Epoch 0, Batch 28900, Loss: 0.47807055711746216\n",
            "Epoch 0, Batch 29000, Loss: 0.6424471735954285\n",
            "Epoch 0, Batch 29100, Loss: 0.36621418595314026\n",
            "Epoch 0, Batch 29200, Loss: 0.5393359661102295\n",
            "Epoch 0, Batch 29300, Loss: 0.4437098801136017\n",
            "Epoch 0, Batch 29400, Loss: 0.08237729221582413\n",
            "Epoch 0, Batch 29500, Loss: 0.3625069856643677\n",
            "Epoch 0, Batch 29600, Loss: 0.36513879895210266\n",
            "Epoch 0, Batch 29700, Loss: 0.4791855812072754\n",
            "Epoch 0, Batch 29800, Loss: 0.3117028772830963\n",
            "Epoch 0, Batch 29900, Loss: 0.36585119366645813\n",
            "Epoch 0, Batch 30000, Loss: 0.6639823913574219\n",
            "Epoch 0, Batch 30100, Loss: 0.567910373210907\n",
            "Epoch 0, Batch 30200, Loss: 0.8098675012588501\n",
            "Epoch 0, Batch 30300, Loss: 0.566007673740387\n",
            "Epoch 0, Batch 30400, Loss: 0.7938050031661987\n",
            "Epoch 0, Batch 30500, Loss: 0.1709076464176178\n",
            "Epoch 0, Batch 30600, Loss: 0.130157470703125\n",
            "Epoch 0, Batch 30700, Loss: 0.38012561202049255\n",
            "Epoch 0, Batch 30800, Loss: 0.6756296753883362\n",
            "Epoch 0, Batch 30900, Loss: 0.3680697977542877\n",
            "Epoch 0, Batch 31000, Loss: 0.39244359731674194\n",
            "Epoch 0, Batch 31100, Loss: 0.20514492690563202\n",
            "Epoch 0, Batch 31200, Loss: 0.9786585569381714\n",
            "Epoch 0, Batch 31300, Loss: 0.4187588095664978\n",
            "Epoch 0, Batch 31400, Loss: 0.7481866478919983\n",
            "Epoch 0, Batch 31500, Loss: 0.5851717591285706\n",
            "Epoch 0, Batch 31600, Loss: 0.5239545702934265\n",
            "Epoch 0, Batch 31700, Loss: 0.19309890270233154\n",
            "Epoch 0, Batch 31800, Loss: 0.1946636289358139\n",
            "Epoch 0, Batch 31900, Loss: 0.28873932361602783\n",
            "Epoch 0, Batch 32000, Loss: 0.30000412464141846\n",
            "Epoch 0, Batch 32100, Loss: 0.7514111399650574\n",
            "Epoch 0, Batch 32200, Loss: 0.7656949162483215\n",
            "Epoch 0, Batch 32300, Loss: 0.1956280767917633\n",
            "Epoch 0, Batch 32400, Loss: 0.48405492305755615\n",
            "Epoch 0, Batch 32500, Loss: 0.4992859661579132\n",
            "Epoch 0, Batch 32600, Loss: 0.4868277311325073\n",
            "Epoch 0, Batch 32700, Loss: 0.20331095159053802\n",
            "Epoch 0, Batch 32800, Loss: 0.9681825637817383\n",
            "Epoch 0, Batch 32900, Loss: 0.41742196679115295\n",
            "Epoch 0, Batch 33000, Loss: 0.5916827321052551\n",
            "Epoch 0, Batch 33100, Loss: 0.267357736825943\n",
            "Epoch 0, Batch 33200, Loss: 0.40883851051330566\n",
            "Epoch 0, Batch 33300, Loss: 0.37804076075553894\n",
            "Epoch 0, Batch 33400, Loss: 0.49154946208000183\n",
            "Epoch 0, Batch 33500, Loss: 0.32539454102516174\n",
            "Epoch 0, Batch 33600, Loss: 0.21356606483459473\n",
            "Epoch 0, Batch 33700, Loss: 0.4891112446784973\n",
            "Epoch 0, Batch 33800, Loss: 0.20861554145812988\n",
            "Epoch 0, Batch 33900, Loss: 0.37778955698013306\n",
            "Epoch 0, Batch 34000, Loss: 0.7299565076828003\n",
            "Epoch 0, Batch 34100, Loss: 0.3207465410232544\n",
            "Epoch 0, Batch 34200, Loss: 0.25983375310897827\n",
            "Epoch 0, Batch 34300, Loss: 0.5101392269134521\n",
            "Epoch 0, Batch 34400, Loss: 0.5119513273239136\n",
            "Epoch 0, Batch 34500, Loss: 0.09050589799880981\n",
            "Epoch 0, Batch 34600, Loss: 0.6496770977973938\n",
            "Epoch 0, Batch 34700, Loss: 0.5389094352722168\n",
            "Epoch 0, Batch 34800, Loss: 0.31535056233406067\n",
            "Epoch 0, Batch 34900, Loss: 0.5881909132003784\n",
            "Epoch 0, Batch 35000, Loss: 0.25272735953330994\n",
            "Epoch 0, Batch 35100, Loss: 0.6254903674125671\n",
            "Epoch 0, Batch 35200, Loss: 0.2955550253391266\n",
            "Epoch 0, Batch 35300, Loss: 0.5092544555664062\n",
            "Epoch 0, Batch 35400, Loss: 0.2114284634590149\n",
            "Epoch 0, Batch 35500, Loss: 0.06953337788581848\n",
            "Epoch 0, Batch 35600, Loss: 0.7854685187339783\n",
            "Epoch 0, Batch 35700, Loss: 0.18944163620471954\n",
            "Epoch 0, Batch 35800, Loss: 0.5795872211456299\n",
            "Epoch 0, Batch 35900, Loss: 0.4442903697490692\n",
            "Epoch 0, Batch 36000, Loss: 0.6511930823326111\n",
            "Epoch 0, Batch 36100, Loss: 0.5327186584472656\n",
            "Epoch 0, Batch 36200, Loss: 0.5295155048370361\n",
            "Epoch 0, Batch 36300, Loss: 0.6222615242004395\n",
            "Epoch 0, Batch 36400, Loss: 0.32162734866142273\n",
            "Epoch 0, Batch 36500, Loss: 0.17343616485595703\n",
            "Epoch 0, Batch 36600, Loss: 0.4209873080253601\n",
            "Epoch 0, Batch 36700, Loss: 0.5445961356163025\n",
            "Epoch 0, Batch 36800, Loss: 0.17371076345443726\n",
            "Epoch 0, Batch 36900, Loss: 0.7808829545974731\n",
            "Epoch 0, Batch 37000, Loss: 0.5923460125923157\n",
            "Epoch 0, Batch 37100, Loss: 0.5133324861526489\n",
            "Epoch 0, Batch 37200, Loss: 0.4069173336029053\n",
            "Epoch 0, Batch 37300, Loss: 0.41135168075561523\n",
            "Epoch 0, Batch 37400, Loss: 0.18831458687782288\n",
            "Epoch 0, Batch 37500, Loss: 0.18322241306304932\n",
            "Epoch 0, Batch 37600, Loss: 0.4428504705429077\n",
            "Epoch 0, Batch 37700, Loss: 0.31968456506729126\n",
            "Epoch 0, Batch 37800, Loss: 0.37575381994247437\n",
            "Epoch 0, Batch 37900, Loss: 0.15240779519081116\n",
            "Epoch 0, Batch 38000, Loss: 0.16634295880794525\n",
            "Epoch 0, Batch 38100, Loss: 1.0099564790725708\n",
            "Epoch 0, Batch 38200, Loss: 0.6116915941238403\n",
            "Epoch 0, Batch 38300, Loss: 0.36785826086997986\n",
            "Epoch 0, Batch 38400, Loss: 0.28424525260925293\n",
            "Epoch 0, Batch 38500, Loss: 0.32052943110466003\n",
            "Epoch 0, Batch 38600, Loss: 0.42100250720977783\n",
            "Epoch 0, Batch 38700, Loss: 0.391347736120224\n",
            "Epoch 0, Batch 38800, Loss: 0.1548091620206833\n",
            "Epoch 0, Batch 38900, Loss: 0.31432104110717773\n",
            "Epoch 0, Batch 39000, Loss: 0.5472199320793152\n",
            "Epoch 0, Batch 39100, Loss: 0.3020092248916626\n",
            "Epoch 0, Batch 39200, Loss: 0.8355682492256165\n",
            "Epoch 0, Batch 39300, Loss: 0.5890420079231262\n",
            "Epoch 0, Batch 39400, Loss: 0.48464468121528625\n",
            "Epoch 0, Batch 39500, Loss: 0.20347940921783447\n",
            "Epoch 0, Batch 39600, Loss: 0.08856680989265442\n",
            "Epoch 0, Batch 39700, Loss: 0.35461342334747314\n",
            "Epoch 0, Batch 39800, Loss: 0.23636925220489502\n",
            "Epoch 0, Batch 39900, Loss: 0.5920342803001404\n",
            "Epoch 0, Batch 40000, Loss: 0.5289979577064514\n",
            "Epoch 0, Batch 40100, Loss: 0.44623783230781555\n",
            "Epoch 0, Batch 40200, Loss: 0.49813130497932434\n",
            "Epoch 0, Batch 40300, Loss: 0.3027840852737427\n",
            "Epoch 0, Batch 40400, Loss: 0.23903892934322357\n",
            "Epoch 0, Batch 40500, Loss: 0.9763159155845642\n",
            "Epoch 0, Batch 40600, Loss: 0.5224701166152954\n",
            "Epoch 0, Batch 40700, Loss: 0.27979251742362976\n",
            "Epoch 0, Batch 40800, Loss: 0.40359562635421753\n",
            "Epoch 0, Batch 40900, Loss: 0.5049536824226379\n",
            "Epoch 0, Batch 41000, Loss: 0.278019517660141\n",
            "Epoch 0, Batch 41100, Loss: 0.20971429347991943\n",
            "Epoch 0, Batch 41200, Loss: 0.5525817275047302\n",
            "Epoch 0, Batch 41300, Loss: 0.4471514821052551\n",
            "Epoch 0, Batch 41400, Loss: 0.6778606176376343\n",
            "Epoch 0, Batch 41500, Loss: 0.7917671203613281\n",
            "Epoch 0, Batch 41600, Loss: 0.557729959487915\n",
            "Epoch 0, Batch 41700, Loss: 0.20794212818145752\n",
            "Epoch 0, Batch 41800, Loss: 0.31442776322364807\n",
            "Epoch 0, Batch 41900, Loss: 0.6296241879463196\n",
            "Epoch 0, Batch 42000, Loss: 0.5214659571647644\n",
            "Epoch 0, Batch 42100, Loss: 0.28699249029159546\n",
            "Epoch 0, Batch 42200, Loss: 0.7541983723640442\n",
            "Epoch 0, Batch 42300, Loss: 0.2753216326236725\n",
            "Epoch 0, Batch 42400, Loss: 0.27522245049476624\n",
            "Epoch 0, Batch 42500, Loss: 0.2206176072359085\n",
            "Epoch 0, Batch 42600, Loss: 0.3845747411251068\n",
            "Epoch 0, Batch 42700, Loss: 0.33353662490844727\n",
            "Epoch 0, Batch 42800, Loss: 0.19602328538894653\n",
            "Epoch 0, Batch 42900, Loss: 0.8376618027687073\n",
            "Epoch 0, Batch 43000, Loss: 0.20994932949543\n",
            "Epoch 0, Batch 43100, Loss: 0.2456817626953125\n",
            "Epoch 0, Batch 43200, Loss: 0.22748605906963348\n",
            "Epoch 0, Batch 43300, Loss: 0.16854554414749146\n",
            "Epoch 0, Batch 43400, Loss: 0.3614192306995392\n",
            "Epoch 0, Batch 43500, Loss: 0.44992807507514954\n",
            "Epoch 0, Batch 43600, Loss: 0.4180578887462616\n",
            "Epoch 0, Batch 43700, Loss: 0.3777521252632141\n",
            "Epoch 0, Batch 43800, Loss: 0.5631285905838013\n",
            "Epoch 0, Batch 43900, Loss: 0.8558139204978943\n",
            "Epoch 0, Batch 44000, Loss: 0.25464263558387756\n",
            "Epoch 0, Batch 44100, Loss: 0.2332782745361328\n",
            "Epoch 0, Batch 44200, Loss: 0.24847570061683655\n",
            "Epoch 0, Batch 44300, Loss: 0.3080591559410095\n",
            "Epoch 0, Batch 44400, Loss: 0.29205283522605896\n",
            "Epoch 0, Batch 44500, Loss: 0.401849627494812\n",
            "Epoch 0, Batch 44600, Loss: 0.6306754350662231\n",
            "Epoch 0, Batch 44700, Loss: 0.28664401173591614\n",
            "Epoch 0, Batch 44800, Loss: 0.13064756989479065\n",
            "Epoch 0, Batch 44900, Loss: 0.3843342661857605\n",
            "Epoch 0, Batch 45000, Loss: 0.21013140678405762\n",
            "Epoch 0, Batch 45100, Loss: 0.6085602641105652\n",
            "Epoch 0, Batch 45200, Loss: 0.26061517000198364\n",
            "Epoch 0, Batch 45300, Loss: 0.3258225917816162\n",
            "Epoch 0, Batch 45400, Loss: 0.5595659613609314\n",
            "Epoch 0, Batch 45500, Loss: 0.44746413826942444\n",
            "Epoch 0, Batch 45600, Loss: 0.08539198338985443\n",
            "Epoch 0, Batch 45700, Loss: 0.3738688826560974\n",
            "Epoch 0, Batch 45800, Loss: 0.31424978375434875\n",
            "Epoch 0, Batch 45900, Loss: 0.3537878394126892\n",
            "Epoch 0, Batch 46000, Loss: 0.5821879506111145\n",
            "Epoch 0, Batch 46100, Loss: 0.6468509435653687\n",
            "Epoch 0, Batch 46200, Loss: 0.11250334978103638\n",
            "Epoch 0, Batch 46300, Loss: 0.37216395139694214\n",
            "Epoch 0, Batch 46400, Loss: 0.4760722815990448\n",
            "Epoch 0, Batch 46500, Loss: 0.22475165128707886\n",
            "Epoch 0, Batch 46600, Loss: 0.711121678352356\n",
            "Epoch 0, Batch 46700, Loss: 0.2555086016654968\n",
            "Epoch 0, Batch 46800, Loss: 0.4848278760910034\n",
            "Epoch 0, Batch 46900, Loss: 0.5022530555725098\n",
            "Epoch 0, Batch 47000, Loss: 0.19442686438560486\n",
            "Epoch 0, Batch 47100, Loss: 0.305454820394516\n",
            "Epoch 0, Batch 47200, Loss: 0.6788617372512817\n",
            "Epoch 0, Batch 47300, Loss: 0.44119688868522644\n",
            "Epoch 0, Batch 47400, Loss: 0.9213008880615234\n",
            "Epoch 0, Batch 47500, Loss: 0.3015763461589813\n",
            "Epoch 0, Batch 47600, Loss: 0.8982707262039185\n",
            "Epoch 0, Batch 47700, Loss: 0.26424476504325867\n",
            "Epoch 0, Batch 47800, Loss: 0.22536656260490417\n",
            "Epoch 0, Batch 47900, Loss: 0.22414745390415192\n",
            "Epoch 0, Batch 48000, Loss: 0.3749406933784485\n",
            "Epoch 0, Batch 48100, Loss: 0.46585631370544434\n",
            "Epoch 0, Batch 48200, Loss: 0.5185725688934326\n",
            "Epoch 0, Batch 48300, Loss: 0.3770531415939331\n",
            "Epoch 0, Batch 48400, Loss: 0.4470932185649872\n",
            "Epoch 0, Batch 48500, Loss: 0.32447952032089233\n",
            "Epoch 0, Batch 48600, Loss: 0.37201303243637085\n",
            "Epoch 0, Batch 48700, Loss: 0.3010738492012024\n",
            "Epoch 0, Batch 48800, Loss: 0.5530794262886047\n",
            "Epoch 0, Batch 48900, Loss: 0.1775018870830536\n",
            "Epoch 0, Batch 49000, Loss: 0.2270616739988327\n",
            "Epoch 0, Batch 49100, Loss: 0.5004085302352905\n",
            "Epoch 0, Batch 49200, Loss: 0.4142167568206787\n",
            "Epoch 0, Batch 49300, Loss: 0.4624786674976349\n",
            "Epoch 0, Batch 49400, Loss: 0.3034473657608032\n",
            "Epoch 0, Batch 49500, Loss: 0.31838172674179077\n",
            "Epoch 0, Batch 49600, Loss: 0.3965272307395935\n",
            "Epoch 0, Batch 49700, Loss: 0.5623233914375305\n",
            "Epoch 0, Batch 49800, Loss: 0.303309828042984\n",
            "Epoch 0, Batch 49900, Loss: 0.14424662292003632\n",
            "Epoch 0, Batch 50000, Loss: 0.7850089073181152\n",
            "Epoch 0, Batch 50100, Loss: 0.5755254626274109\n",
            "Epoch 0, Batch 50200, Loss: 0.40499427914619446\n",
            "Epoch 0, Batch 50300, Loss: 0.1766005903482437\n",
            "Epoch 0, Batch 50400, Loss: 0.207675039768219\n",
            "Epoch 0, Batch 50500, Loss: 0.4707117974758148\n",
            "Epoch 0, Batch 50600, Loss: 0.8014330863952637\n",
            "Epoch 0, Batch 50700, Loss: 0.6742182374000549\n",
            "Epoch 0, Batch 50800, Loss: 0.3006373345851898\n",
            "Epoch 0, Batch 50900, Loss: 0.2950116991996765\n",
            "Epoch 0, Batch 51000, Loss: 0.44404250383377075\n",
            "Epoch 0, Batch 51100, Loss: 0.614639937877655\n",
            "Epoch 0, Batch 51200, Loss: 0.8766627311706543\n",
            "Epoch 0, Batch 51300, Loss: 0.38332056999206543\n",
            "Epoch 0, Batch 51400, Loss: 0.5335885286331177\n",
            "Epoch 0, Batch 51500, Loss: 0.7056808471679688\n",
            "Epoch 0, Batch 51600, Loss: 0.09977681934833527\n",
            "Epoch 0, Batch 51700, Loss: 0.24256175756454468\n",
            "Epoch 0, Batch 51800, Loss: 0.28245243430137634\n",
            "Epoch 0, Batch 51900, Loss: 0.2616479992866516\n",
            "Epoch 0, Batch 52000, Loss: 1.244871973991394\n",
            "Epoch 0, Batch 52100, Loss: 0.6061089038848877\n",
            "Epoch 0, Batch 52200, Loss: 0.12117254734039307\n",
            "Epoch 0, Batch 52300, Loss: 0.3721919059753418\n",
            "Epoch 0, Batch 52400, Loss: 0.6162303686141968\n",
            "Epoch 0, Batch 52500, Loss: 0.7249006628990173\n",
            "Epoch 0, Batch 52600, Loss: 0.9146209359169006\n",
            "Epoch 0, Batch 52700, Loss: 0.2736802399158478\n",
            "Epoch 0, Batch 52800, Loss: 0.17056109011173248\n",
            "Epoch 0, Batch 52900, Loss: 0.2135430872440338\n",
            "Epoch 0, Batch 53000, Loss: 0.13144546747207642\n",
            "Epoch 0, Batch 53100, Loss: 0.40963247418403625\n",
            "Epoch 0, Batch 53200, Loss: 0.806094765663147\n",
            "Epoch 0, Batch 53300, Loss: 0.6600470542907715\n",
            "Epoch 0, Batch 53400, Loss: 0.2534426152706146\n",
            "Epoch 0, Batch 53500, Loss: 0.16887249052524567\n",
            "Epoch 0, Batch 53600, Loss: 0.16961392760276794\n",
            "Epoch 0, Batch 53700, Loss: 0.1476346254348755\n",
            "Epoch 0, Batch 53800, Loss: 0.7371202707290649\n",
            "Epoch 0, Batch 53900, Loss: 0.4242222309112549\n",
            "Epoch 0, Batch 54000, Loss: 0.44910669326782227\n",
            "Epoch 0, Batch 54100, Loss: 0.46604838967323303\n",
            "Epoch 0, Batch 54200, Loss: 0.5472907423973083\n",
            "Epoch 0, Batch 54300, Loss: 0.20393896102905273\n",
            "Epoch 0, Batch 54400, Loss: 0.08405982702970505\n",
            "Epoch 0, Batch 54500, Loss: 0.6456772089004517\n",
            "Epoch 0, Batch 54600, Loss: 0.19383582472801208\n",
            "Epoch 0, Batch 54700, Loss: 1.1157488822937012\n",
            "Epoch 0, Batch 54800, Loss: 0.19013655185699463\n",
            "Epoch 0, Batch 54900, Loss: 0.2948460876941681\n",
            "Epoch 0, Batch 55000, Loss: 0.6060642600059509\n",
            "Epoch 0, Batch 55100, Loss: 0.6760706305503845\n",
            "Epoch 0, Batch 55200, Loss: 0.592269241809845\n",
            "Epoch 0, Batch 55300, Loss: 0.052037470042705536\n",
            "Epoch 0, Batch 55400, Loss: 0.4489375948905945\n",
            "Epoch 0, Batch 55500, Loss: 0.15485341846942902\n",
            "Epoch 0, Batch 55600, Loss: 0.44970595836639404\n",
            "Epoch 0, Batch 55700, Loss: 0.2701069414615631\n",
            "Epoch 0, Batch 55800, Loss: 0.3594699501991272\n",
            "Epoch 0, Batch 55900, Loss: 0.4099586308002472\n",
            "Epoch 0, Batch 56000, Loss: 0.3382652699947357\n",
            "Epoch 0, Batch 56100, Loss: 0.7276781797409058\n",
            "Epoch 0, Batch 56200, Loss: 0.6114172339439392\n",
            "Epoch 0, Batch 56300, Loss: 0.09532825648784637\n",
            "Epoch 0, Batch 56400, Loss: 0.4710429012775421\n",
            "Epoch 0, Batch 56500, Loss: 0.31432047486305237\n",
            "Epoch 0, Batch 56600, Loss: 0.2729329764842987\n",
            "Epoch 0, Batch 56700, Loss: 0.13247159123420715\n",
            "Epoch 0, Batch 56800, Loss: 0.5894477367401123\n",
            "Epoch 0, Batch 56900, Loss: 0.24535737931728363\n",
            "Epoch 0, Batch 57000, Loss: 0.33548474311828613\n",
            "Epoch 0, Batch 57100, Loss: 0.40088969469070435\n",
            "Epoch 0, Batch 57200, Loss: 0.49802789092063904\n",
            "Epoch 0, Batch 57300, Loss: 0.7746015191078186\n",
            "Epoch 0, Batch 57400, Loss: 0.2499997913837433\n",
            "Epoch 0, Batch 57500, Loss: 0.7629755139350891\n",
            "Epoch 0, Batch 57600, Loss: 0.3259996771812439\n",
            "Epoch 0, Batch 57700, Loss: 0.5736820697784424\n",
            "Epoch 0, Batch 57800, Loss: 0.18547363579273224\n",
            "Epoch 0, Batch 57900, Loss: 0.27165114879608154\n",
            "Epoch 0, Batch 58000, Loss: 0.10179733484983444\n",
            "Epoch 0, Batch 58100, Loss: 0.34133124351501465\n",
            "Epoch 0, Batch 58200, Loss: 0.4783279299736023\n",
            "Epoch 0, Batch 58300, Loss: 0.3313610553741455\n",
            "Epoch 0, Batch 58400, Loss: 0.36433103680610657\n",
            "Epoch 0, Batch 58500, Loss: 0.2182331383228302\n",
            "Epoch 0, Batch 58600, Loss: 0.30185526609420776\n",
            "Epoch 0, Batch 58700, Loss: 0.3934473693370819\n",
            "Epoch 0, Batch 58800, Loss: 0.20807942748069763\n",
            "Epoch 0, Batch 58900, Loss: 0.36107274889945984\n",
            "Epoch 0, Batch 59000, Loss: 0.19349364936351776\n",
            "Epoch 0, Batch 59100, Loss: 0.2685483694076538\n",
            "Epoch 0, Batch 59200, Loss: 0.41612759232521057\n",
            "Epoch 0, Batch 59300, Loss: 0.34531840682029724\n",
            "Epoch 0, Batch 59400, Loss: 0.5901243090629578\n",
            "Epoch 0, Batch 59500, Loss: 0.22289906442165375\n",
            "Epoch 0, Batch 59600, Loss: 0.36734238266944885\n",
            "Epoch 0, Batch 59700, Loss: 0.35774222016334534\n",
            "Epoch 0, Batch 59800, Loss: 0.4961930215358734\n",
            "Epoch 0, Batch 59900, Loss: 0.13126473128795624\n",
            "Epoch 0, Batch 60000, Loss: 0.47617289423942566\n",
            "Epoch 0, Batch 60100, Loss: 0.24634139239788055\n",
            "Epoch 0, Batch 60200, Loss: 0.13783681392669678\n",
            "Epoch 0, Batch 60300, Loss: 0.22411613166332245\n",
            "Epoch 0, Batch 60400, Loss: 0.07519348710775375\n",
            "Epoch 0, Batch 60500, Loss: 0.5266997814178467\n",
            "Epoch 0, Batch 60600, Loss: 0.59517902135849\n",
            "Epoch 0, Batch 60700, Loss: 0.5028842091560364\n",
            "Epoch 0, Batch 60800, Loss: 0.3567820191383362\n",
            "Epoch 0, Batch 60900, Loss: 0.27140408754348755\n",
            "Epoch 0, Batch 61000, Loss: 0.23841306567192078\n",
            "Epoch 0, Batch 61100, Loss: 0.6143764853477478\n",
            "Epoch 0, Batch 61200, Loss: 1.0246994495391846\n",
            "Epoch 0, Batch 61300, Loss: 0.28614166378974915\n",
            "Epoch 0, Batch 61400, Loss: 0.39792415499687195\n",
            "Epoch 0, Batch 61500, Loss: 0.22910185158252716\n",
            "Epoch 0, Batch 61600, Loss: 0.2419862598180771\n",
            "Epoch 0, Batch 61700, Loss: 0.4701005518436432\n",
            "Epoch 0, Batch 61800, Loss: 0.26242461800575256\n",
            "Epoch 0, Batch 61900, Loss: 0.6186572313308716\n",
            "Epoch 0, Batch 62000, Loss: 0.8818303346633911\n",
            "Epoch 0, Batch 62100, Loss: 0.10545533895492554\n",
            "Epoch 0, Batch 62200, Loss: 0.5072236657142639\n",
            "Epoch 0, Batch 62300, Loss: 0.3529670238494873\n",
            "Epoch 0, Batch 62400, Loss: 0.5094282627105713\n",
            "Epoch 0, Batch 62500, Loss: 0.9435769319534302\n",
            "Epoch 0, Batch 62600, Loss: 0.3543086647987366\n",
            "Epoch 0, Batch 62700, Loss: 0.20180071890354156\n",
            "Epoch 0, Batch 62800, Loss: 0.17023038864135742\n",
            "Epoch 0, Batch 62900, Loss: 0.3797850012779236\n",
            "Epoch 0, Batch 63000, Loss: 0.27052071690559387\n",
            "Epoch 0, Batch 63100, Loss: 0.7410743832588196\n",
            "Epoch 0, Batch 63200, Loss: 0.16375720500946045\n",
            "Epoch 0, Batch 63300, Loss: 0.6232023239135742\n",
            "Epoch 0, Batch 63400, Loss: 0.3639533519744873\n",
            "Epoch 0, Batch 63500, Loss: 0.6401545405387878\n",
            "Epoch 0, Batch 63600, Loss: 0.5250791907310486\n",
            "Epoch 0, Batch 63700, Loss: 0.48443177342414856\n",
            "Epoch 0, Batch 63800, Loss: 0.483924925327301\n",
            "Epoch 0, Batch 63900, Loss: 0.36534738540649414\n",
            "Epoch 0, Batch 64000, Loss: 0.4304535686969757\n",
            "Epoch 0, Batch 64100, Loss: 0.21047738194465637\n",
            "Epoch 0, Batch 64200, Loss: 0.31150907278060913\n",
            "Epoch 0, Batch 64300, Loss: 0.7332385182380676\n",
            "Epoch 0, Batch 64400, Loss: 0.3741966485977173\n",
            "Epoch 0, Batch 64500, Loss: 0.30645161867141724\n",
            "Epoch 0, Batch 64600, Loss: 0.5259197354316711\n",
            "Epoch 0, Batch 64700, Loss: 0.27766120433807373\n",
            "Epoch 0, Batch 64800, Loss: 0.5510960221290588\n",
            "Epoch 0, Batch 64900, Loss: 0.12104944884777069\n",
            "Epoch 0, Batch 65000, Loss: 0.14526909589767456\n",
            "Epoch 0, Batch 65100, Loss: 0.4113011360168457\n",
            "Epoch 0, Batch 65200, Loss: 0.19380763173103333\n",
            "Epoch 0, Batch 65300, Loss: 0.4186769425868988\n",
            "Epoch 0, Batch 65400, Loss: 0.4125485420227051\n",
            "Epoch 0, Batch 65500, Loss: 0.6336361169815063\n",
            "Epoch 0, Batch 65600, Loss: 0.6459901332855225\n",
            "Epoch 0, Batch 65700, Loss: 0.7540031671524048\n",
            "Epoch 0, Batch 65800, Loss: 0.5661620497703552\n",
            "Epoch 0, Batch 65900, Loss: 0.1860223114490509\n",
            "Epoch 0, Batch 66000, Loss: 0.3942296504974365\n",
            "Epoch 0, Batch 66100, Loss: 0.1350826472043991\n",
            "Epoch 0, Batch 66200, Loss: 0.21727928519248962\n",
            "Epoch 0, Batch 66300, Loss: 0.24569331109523773\n",
            "Epoch 0, Batch 66400, Loss: 0.3303728401660919\n",
            "Epoch 0, Batch 66500, Loss: 0.37053439021110535\n",
            "Epoch 0, Batch 66600, Loss: 0.19126485288143158\n",
            "Epoch 0, Batch 66700, Loss: 0.7671956419944763\n",
            "Epoch 0, Batch 66800, Loss: 0.13716894388198853\n",
            "Epoch 0, Batch 66900, Loss: 0.4848977029323578\n",
            "Epoch 0, Batch 67000, Loss: 0.39231282472610474\n",
            "Epoch 0, Batch 67100, Loss: 0.40321552753448486\n",
            "Epoch 0, Batch 67200, Loss: 0.2110927850008011\n",
            "Epoch 0, Batch 67300, Loss: 0.6282047033309937\n",
            "Epoch 0, Batch 67400, Loss: 0.9057818651199341\n",
            "Epoch 0, Batch 67500, Loss: 0.6752608418464661\n",
            "Epoch 0, Batch 67600, Loss: 0.06653791666030884\n",
            "Epoch 0, Batch 67700, Loss: 0.2897721529006958\n",
            "Epoch 0, Batch 67800, Loss: 0.5595616698265076\n",
            "Epoch 0, Batch 67900, Loss: 0.3202805519104004\n",
            "Epoch 0, Batch 68000, Loss: 0.6187520027160645\n",
            "Epoch 0, Batch 68100, Loss: 0.6586753129959106\n",
            "Epoch 0, Batch 68200, Loss: 0.1510188728570938\n",
            "Epoch 0, Batch 68300, Loss: 0.24098587036132812\n",
            "Epoch 0, Batch 68400, Loss: 0.3508410155773163\n",
            "Epoch 0, Batch 68500, Loss: 0.21008647978305817\n",
            "Epoch 0, Batch 68600, Loss: 0.7009053230285645\n",
            "Epoch 0, Batch 68700, Loss: 0.4752926528453827\n",
            "Epoch 0, Batch 68800, Loss: 0.6842659711837769\n",
            "Epoch 0, Batch 68900, Loss: 0.4898436963558197\n",
            "Epoch 0, Batch 69000, Loss: 0.3546052873134613\n",
            "Epoch 0, Batch 69100, Loss: 0.7298575639724731\n",
            "Epoch 0, Batch 69200, Loss: 0.17879600822925568\n",
            "Epoch 0, Batch 69300, Loss: 0.39710336923599243\n",
            "Epoch 0, Batch 69400, Loss: 0.4718826413154602\n",
            "Epoch 0, Batch 69500, Loss: 0.48515036702156067\n",
            "Epoch 0, Batch 69600, Loss: 0.6799538731575012\n",
            "Epoch 0, Batch 69700, Loss: 0.3429664373397827\n",
            "Epoch 0, Batch 69800, Loss: 0.4323982000350952\n",
            "Epoch 0, Batch 69900, Loss: 0.11518640071153641\n",
            "Epoch 0, Batch 70000, Loss: 0.4958856999874115\n",
            "Epoch 0, Batch 70100, Loss: 0.12921541929244995\n",
            "Epoch 0, Batch 70200, Loss: 0.28111979365348816\n",
            "Epoch 0, Batch 70300, Loss: 0.49220165610313416\n",
            "Epoch 0, Batch 70400, Loss: 0.21306146681308746\n",
            "Epoch 0, Batch 70500, Loss: 0.16202408075332642\n",
            "Epoch 0, Batch 70600, Loss: 0.14857442677021027\n",
            "Epoch 0, Batch 70700, Loss: 0.18556711077690125\n",
            "Epoch 0, Batch 70800, Loss: 0.5782999396324158\n",
            "Epoch 0, Batch 70900, Loss: 0.2130070924758911\n",
            "Epoch 0, Batch 71000, Loss: 0.3816164433956146\n",
            "Epoch 0, Batch 71100, Loss: 0.42129284143447876\n",
            "Epoch 0, Batch 71200, Loss: 0.2880670428276062\n",
            "Epoch 0, Batch 71300, Loss: 0.17740076780319214\n",
            "Epoch 0, Batch 71400, Loss: 0.6721770763397217\n",
            "Epoch 0, Batch 71500, Loss: 0.5080891847610474\n",
            "Epoch 0, Batch 71600, Loss: 0.36371493339538574\n",
            "Epoch 0, Batch 71700, Loss: 0.3556478023529053\n",
            "Epoch 0, Batch 71800, Loss: 0.3191598057746887\n",
            "Epoch 0, Batch 71900, Loss: 0.519496738910675\n",
            "Epoch 0, Batch 72000, Loss: 0.3788398802280426\n",
            "Epoch 0, Batch 72100, Loss: 0.3302079439163208\n",
            "Epoch 0, Batch 72200, Loss: 0.32127803564071655\n",
            "Epoch 0, Batch 72300, Loss: 0.08518294990062714\n",
            "Epoch 0, Batch 72400, Loss: 0.13193276524543762\n",
            "Epoch 0, Batch 72500, Loss: 0.6604694724082947\n",
            "Epoch 0, Batch 72600, Loss: 0.2884768843650818\n",
            "Epoch 0, Batch 72700, Loss: 0.8695890307426453\n",
            "Epoch 0, Batch 72800, Loss: 0.42795586585998535\n",
            "Epoch 0, Batch 72900, Loss: 0.6053741574287415\n",
            "Epoch 0, Batch 73000, Loss: 0.4527518153190613\n",
            "Epoch 0, Batch 73100, Loss: 0.18872034549713135\n",
            "Epoch 0, Batch 73200, Loss: 0.7547963857650757\n",
            "Epoch 0, Batch 73300, Loss: 0.6741762757301331\n",
            "Epoch 0, Batch 73400, Loss: 0.5402096509933472\n",
            "Epoch 0, Batch 73500, Loss: 0.4799540936946869\n",
            "Epoch 0, Batch 73600, Loss: 0.3714028596878052\n",
            "Epoch 0, Batch 73700, Loss: 0.5465620160102844\n",
            "Epoch 0, Batch 73800, Loss: 0.2252679467201233\n",
            "Epoch 0, Batch 73900, Loss: 0.38222068548202515\n",
            "Epoch 0, Batch 74000, Loss: 0.2774816155433655\n",
            "Epoch 0, Batch 74100, Loss: 0.21456660330295563\n",
            "Epoch 0, Batch 74200, Loss: 0.29592347145080566\n",
            "Epoch 0, Batch 74300, Loss: 0.6064546704292297\n",
            "Epoch 0, Batch 74400, Loss: 0.48789092898368835\n",
            "Epoch 0, Batch 74500, Loss: 0.26198530197143555\n",
            "Epoch 0, Batch 74600, Loss: 0.3471578061580658\n",
            "Epoch 0, Batch 74700, Loss: 0.21408912539482117\n",
            "Epoch 0, Batch 74800, Loss: 0.3121286630630493\n",
            "Epoch 0, Batch 74900, Loss: 0.6087695360183716\n",
            "Epoch 0, Batch 75000, Loss: 0.2702524662017822\n",
            "Epoch 0, Batch 75100, Loss: 0.2780957818031311\n",
            "Epoch 0, Batch 75200, Loss: 0.34139180183410645\n",
            "Epoch 0, Batch 75300, Loss: 0.22760027647018433\n",
            "Epoch 0, Batch 75400, Loss: 0.7976511120796204\n",
            "Epoch 0, Batch 75500, Loss: 0.4610329568386078\n",
            "Epoch 0, Batch 75600, Loss: 0.3650238513946533\n",
            "Epoch 0, Batch 75700, Loss: 0.4292932450771332\n",
            "Epoch 0, Batch 75800, Loss: 0.19133034348487854\n",
            "Epoch 0, Batch 75900, Loss: 0.7565190196037292\n",
            "Epoch 0, Batch 76000, Loss: 0.41245055198669434\n",
            "Epoch 0, Batch 76100, Loss: 0.6556727290153503\n",
            "Epoch 0, Batch 76200, Loss: 0.5291751623153687\n",
            "Epoch 0, Batch 76300, Loss: 0.5473752617835999\n",
            "Epoch 0, Batch 76400, Loss: 0.32850921154022217\n",
            "Epoch 0, Batch 76500, Loss: 0.3262663781642914\n",
            "Epoch 0, Batch 76600, Loss: 0.22675465047359467\n",
            "Epoch 0, Batch 76700, Loss: 0.22348088026046753\n",
            "Epoch 0, Batch 76800, Loss: 0.47186851501464844\n",
            "Epoch 0, Batch 76900, Loss: 0.4154488742351532\n",
            "Epoch 0, Batch 77000, Loss: 0.5192939043045044\n",
            "Epoch 0, Batch 77100, Loss: 0.5557488799095154\n",
            "Epoch 0, Batch 77200, Loss: 0.27354347705841064\n",
            "Epoch 0, Batch 77300, Loss: 0.287973552942276\n",
            "Epoch 0, Batch 77400, Loss: 0.19387491047382355\n",
            "Epoch 0, Batch 77500, Loss: 0.34491175413131714\n",
            "Epoch 0, Batch 77600, Loss: 0.318625807762146\n",
            "Epoch 0, Batch 77700, Loss: 0.37522050738334656\n",
            "Epoch 0, Batch 77800, Loss: 0.48110848665237427\n",
            "Epoch 0, Batch 77900, Loss: 0.14653271436691284\n",
            "Epoch 0, Batch 78000, Loss: 0.7562544941902161\n",
            "Epoch 0, Batch 78100, Loss: 0.776862621307373\n",
            "Epoch 0, Batch 78200, Loss: 0.3287862241268158\n",
            "Epoch 0, Batch 78300, Loss: 0.5104901790618896\n",
            "Epoch 0, Batch 78400, Loss: 0.22306063771247864\n",
            "Epoch 0, Batch 78500, Loss: 0.33589279651641846\n",
            "Epoch 0, Batch 78600, Loss: 0.7466979622840881\n",
            "Epoch 0, Batch 78700, Loss: 0.339318186044693\n",
            "Epoch 0, Batch 78800, Loss: 0.05588390678167343\n",
            "Epoch 0, Batch 78900, Loss: 0.5921154022216797\n",
            "Epoch 0, Batch 79000, Loss: 0.6553028225898743\n",
            "Epoch 0, Batch 79100, Loss: 0.5641776919364929\n",
            "Epoch 0, Batch 79200, Loss: 0.32591456174850464\n",
            "Epoch 0, Batch 79300, Loss: 0.2110903412103653\n",
            "Epoch 0, Batch 79400, Loss: 0.23813709616661072\n",
            "Epoch 0, Batch 79500, Loss: 0.5894346833229065\n",
            "Epoch 0, Batch 79600, Loss: 0.9543205499649048\n",
            "Epoch 0, Batch 79700, Loss: 0.29896610975265503\n",
            "Epoch 0, Batch 79800, Loss: 0.40695661306381226\n",
            "Epoch 0, Batch 79900, Loss: 0.8704741597175598\n",
            "Epoch 0, Batch 80000, Loss: 0.3662472069263458\n",
            "Epoch 0, Batch 80100, Loss: 0.5487584471702576\n",
            "Epoch 0, Batch 80200, Loss: 0.2735309898853302\n",
            "Epoch 0, Batch 80300, Loss: 0.11694979667663574\n",
            "Epoch 0, Batch 80400, Loss: 0.6629288196563721\n",
            "Epoch 0, Batch 80500, Loss: 0.2681637704372406\n",
            "Epoch 0, Batch 80600, Loss: 0.5796233415603638\n",
            "Epoch 0, Batch 80700, Loss: 0.5427681803703308\n",
            "Epoch 0, Batch 80800, Loss: 0.1489698737859726\n",
            "Epoch 0, Batch 80900, Loss: 0.33349186182022095\n",
            "Epoch 0, Batch 81000, Loss: 0.22973915934562683\n",
            "Epoch 0, Batch 81100, Loss: 0.33493557572364807\n",
            "Epoch 0, Batch 81200, Loss: 0.5034493803977966\n",
            "Epoch 0, Batch 81300, Loss: 0.07354804873466492\n",
            "Epoch 0, Batch 81400, Loss: 0.3105272054672241\n",
            "Epoch 0, Batch 81500, Loss: 0.16055992245674133\n",
            "Epoch 0, Batch 81600, Loss: 0.2624683976173401\n",
            "Epoch 0, Batch 81700, Loss: 0.3263694941997528\n",
            "Epoch 0, Batch 81800, Loss: 0.12428977340459824\n",
            "Epoch 0, Batch 81900, Loss: 0.3482876121997833\n",
            "Epoch 0, Batch 82000, Loss: 0.3274346888065338\n",
            "Epoch 0, Batch 82100, Loss: 0.2581891715526581\n",
            "Epoch 0, Batch 82200, Loss: 0.33982715010643005\n",
            "Epoch 0, Batch 82300, Loss: 0.19908970594406128\n",
            "Epoch 0, Batch 82400, Loss: 0.3214159309864044\n",
            "Epoch 0, Batch 82500, Loss: 0.1885080188512802\n",
            "Epoch 0, Batch 82600, Loss: 0.32348838448524475\n",
            "Epoch 0, Batch 82700, Loss: 0.35251584649086\n",
            "Epoch 0, Batch 82800, Loss: 0.44631731510162354\n",
            "Epoch 0, Batch 82900, Loss: 0.43500906229019165\n",
            "Epoch 0, Batch 83000, Loss: 0.17579834163188934\n",
            "Epoch 0, Batch 83100, Loss: 0.3281978964805603\n",
            "Epoch 0, Batch 83200, Loss: 0.3393278121948242\n",
            "Epoch 0, Batch 83300, Loss: 0.16425126791000366\n",
            "Epoch 0, Batch 83400, Loss: 0.4210876226425171\n",
            "Epoch 0, Batch 83500, Loss: 0.5495765805244446\n",
            "Epoch 0, Batch 83600, Loss: 0.42566245794296265\n",
            "Epoch 0, Batch 83700, Loss: 0.1401970535516739\n",
            "Epoch 0, Batch 83800, Loss: 0.29875755310058594\n",
            "Epoch 0, Batch 83900, Loss: 0.6383563280105591\n",
            "Epoch 0, Batch 84000, Loss: 0.17077171802520752\n",
            "Epoch 0, Batch 84100, Loss: 0.41804125905036926\n",
            "Epoch 0, Batch 84200, Loss: 0.3338788151741028\n",
            "Epoch 0, Batch 84300, Loss: 0.2011236846446991\n",
            "Epoch 0, Batch 84400, Loss: 0.5370217561721802\n",
            "Epoch 0, Batch 84500, Loss: 0.41091808676719666\n",
            "Epoch 0, Batch 84600, Loss: 0.2767224907875061\n",
            "Epoch 0, Batch 84700, Loss: 0.28072068095207214\n",
            "Epoch 0, Batch 84800, Loss: 0.6532596349716187\n",
            "Epoch 0, Batch 84900, Loss: 0.32033294439315796\n",
            "Epoch 0, Batch 85000, Loss: 0.14600160717964172\n",
            "Epoch 0, Batch 85100, Loss: 0.17895130813121796\n",
            "Epoch 0, Batch 85200, Loss: 0.610558271408081\n",
            "Epoch 0, Batch 85300, Loss: 0.4964507222175598\n",
            "Epoch 0, Batch 85400, Loss: 0.16905614733695984\n",
            "Epoch 0, Batch 85500, Loss: 0.2700728178024292\n",
            "Epoch 0, Batch 85600, Loss: 0.2682453989982605\n",
            "Epoch 0, Batch 85700, Loss: 0.5864537358283997\n",
            "Epoch 0, Batch 85800, Loss: 0.29856112599372864\n",
            "Epoch 0, Batch 85900, Loss: 0.3041493892669678\n",
            "Epoch 0, Batch 86000, Loss: 0.07063063979148865\n",
            "Epoch 0, Batch 86100, Loss: 0.28002846240997314\n",
            "Epoch 0, Batch 86200, Loss: 0.2393103539943695\n",
            "Epoch 0, Batch 86300, Loss: 0.7522416114807129\n",
            "Epoch 0, Batch 86400, Loss: 0.6590844988822937\n",
            "Epoch 0, Batch 86500, Loss: 0.4201817512512207\n",
            "Epoch 0, Batch 86600, Loss: 0.31336966156959534\n",
            "Epoch 0, Batch 86700, Loss: 0.20283357799053192\n",
            "Epoch 0, Batch 86800, Loss: 0.45237812399864197\n",
            "Epoch 0, Batch 86900, Loss: 0.08937294781208038\n",
            "Epoch 0, Batch 87000, Loss: 0.22716976702213287\n",
            "Epoch 0, Batch 87100, Loss: 0.39639490842819214\n",
            "Epoch 0, Batch 87200, Loss: 0.36510562896728516\n",
            "Epoch 0, Batch 87300, Loss: 0.9172150492668152\n",
            "Epoch 0, Batch 87400, Loss: 0.18058034777641296\n",
            "Epoch 0, Batch 87500, Loss: 0.21371996402740479\n",
            "Epoch 0, Batch 87600, Loss: 0.11226116865873337\n",
            "Epoch 0, Batch 87700, Loss: 0.6191921234130859\n",
            "Epoch 0, Batch 87800, Loss: 0.4058324992656708\n",
            "Epoch 0, Batch 87900, Loss: 0.4533657431602478\n",
            "Epoch 0, Batch 88000, Loss: 0.398098886013031\n",
            "Epoch 0, Batch 88100, Loss: 0.6347376704216003\n",
            "Epoch 0, Batch 88200, Loss: 0.9722570180892944\n",
            "Epoch 0, Batch 88300, Loss: 0.742186427116394\n",
            "Epoch 0, Batch 88400, Loss: 0.13261693716049194\n",
            "Epoch 0, Batch 88500, Loss: 0.4542725384235382\n",
            "Epoch 0, Batch 88600, Loss: 0.34662899374961853\n",
            "Epoch 0, Batch 88700, Loss: 0.5020108819007874\n",
            "Epoch 0, Batch 88800, Loss: 0.39674851298332214\n",
            "Epoch 0, Batch 88900, Loss: 0.4875798523426056\n",
            "Epoch 0, Batch 89000, Loss: 0.3287317156791687\n",
            "Epoch 0, Batch 89100, Loss: 0.2312721461057663\n",
            "Epoch 0, Batch 89200, Loss: 0.2642647624015808\n",
            "Epoch 0, Batch 89300, Loss: 0.6761070489883423\n",
            "Epoch 0, Batch 89400, Loss: 0.3337225317955017\n",
            "Epoch 0, Batch 89500, Loss: 0.20960988104343414\n",
            "Epoch 0, Batch 89600, Loss: 0.5101496577262878\n",
            "Epoch 0, Batch 89700, Loss: 0.7874757051467896\n",
            "Epoch 0, Batch 89800, Loss: 0.9284813404083252\n",
            "Epoch 0, Batch 89900, Loss: 0.3113642930984497\n",
            "Epoch 0, Batch 90000, Loss: 0.6067524552345276\n",
            "Epoch 0, Batch 90100, Loss: 0.406943678855896\n",
            "Epoch 0, Batch 90200, Loss: 0.5700985193252563\n",
            "Epoch 0, Batch 90300, Loss: 0.2209581583738327\n",
            "Epoch 0, Batch 90400, Loss: 0.6184219717979431\n",
            "Epoch 0, Batch 90500, Loss: 0.6264038681983948\n",
            "Epoch 0, Batch 90600, Loss: 0.4276547431945801\n",
            "Epoch 0, Batch 90700, Loss: 0.5276442766189575\n",
            "Epoch 0, Batch 90800, Loss: 0.42872047424316406\n",
            "Epoch 0, Batch 90900, Loss: 0.32412055134773254\n",
            "Epoch 0, Batch 91000, Loss: 0.6330544352531433\n",
            "Epoch 0, Batch 91100, Loss: 0.6416411995887756\n",
            "Epoch 0, Batch 91200, Loss: 0.10821986943483353\n",
            "Epoch 0, Batch 91300, Loss: 0.6467889547348022\n",
            "Epoch 0, Batch 91400, Loss: 0.2910337448120117\n",
            "Epoch 0, Batch 91500, Loss: 0.25203898549079895\n",
            "Epoch 0, Batch 91600, Loss: 0.3796451985836029\n",
            "Epoch 0, Batch 91700, Loss: 0.26715677976608276\n",
            "Epoch 0, Batch 91800, Loss: 0.6577958464622498\n",
            "Epoch 0, Batch 91900, Loss: 0.2904174327850342\n",
            "Epoch 0, Batch 92000, Loss: 0.34109142422676086\n",
            "Epoch 0, Batch 92100, Loss: 0.2652572989463806\n",
            "Epoch 0, Batch 92200, Loss: 0.4422299563884735\n",
            "Epoch 0, Batch 92300, Loss: 0.07943698763847351\n",
            "Epoch 0, Batch 92400, Loss: 0.890735387802124\n",
            "Epoch 0, Batch 92500, Loss: 0.13142699003219604\n",
            "Epoch 0, Batch 92600, Loss: 0.31294846534729004\n",
            "Epoch 0, Batch 92700, Loss: 0.3840692639350891\n",
            "Epoch 0, Batch 92800, Loss: 0.5233816504478455\n",
            "Epoch 0, Batch 92900, Loss: 0.21846431493759155\n",
            "Epoch 0, Batch 93000, Loss: 0.14371229708194733\n",
            "Epoch 0, Batch 93100, Loss: 0.22864627838134766\n",
            "Epoch 0, Batch 93200, Loss: 0.28281551599502563\n",
            "Epoch 0, Batch 93300, Loss: 0.21897439658641815\n",
            "Epoch 0, Batch 93400, Loss: 0.27546706795692444\n",
            "Epoch 0, Batch 93500, Loss: 0.2522706389427185\n",
            "Epoch 0, Batch 93600, Loss: 0.9546014666557312\n",
            "Epoch 0, Batch 93700, Loss: 0.2654813826084137\n",
            "Epoch 0, Batch 93800, Loss: 0.1383550465106964\n",
            "Epoch 0, Batch 93900, Loss: 0.24963334202766418\n",
            "Epoch 0, Batch 94000, Loss: 0.48244714736938477\n",
            "Epoch 0, Batch 94100, Loss: 0.8173766732215881\n",
            "Epoch 0, Batch 94200, Loss: 0.22712087631225586\n",
            "Epoch 0, Batch 94300, Loss: 0.1474265456199646\n",
            "Epoch 0, Batch 94400, Loss: 0.7356786727905273\n",
            "Epoch 0, Batch 94500, Loss: 1.1423202753067017\n",
            "Epoch 0, Batch 94600, Loss: 0.6509301662445068\n",
            "Epoch 0, Batch 94700, Loss: 0.3024764060974121\n",
            "Epoch 0, Batch 94800, Loss: 0.25223174691200256\n",
            "Epoch 0, Batch 94900, Loss: 0.47071710228919983\n",
            "Epoch 0, Batch 95000, Loss: 0.4221040606498718\n",
            "Epoch 0, Batch 95100, Loss: 0.6846045851707458\n",
            "Epoch 0, Batch 95200, Loss: 0.590371310710907\n",
            "Epoch 0, Batch 95300, Loss: 0.6556896567344666\n",
            "Epoch 0, Batch 95400, Loss: 0.2527500092983246\n",
            "Epoch 0, Batch 95500, Loss: 0.5591744184494019\n",
            "Epoch 0, Batch 95600, Loss: 0.43449145555496216\n",
            "Epoch 0, Batch 95700, Loss: 0.5365074872970581\n",
            "Epoch 0, Batch 95800, Loss: 0.2538111209869385\n",
            "Epoch 0, Batch 95900, Loss: 0.2502817213535309\n",
            "Epoch 0, Batch 96000, Loss: 0.32588738203048706\n",
            "Epoch 0, Batch 96100, Loss: 0.44702214002609253\n",
            "Epoch 0, Batch 96200, Loss: 0.11873267590999603\n",
            "Epoch 0, Batch 96300, Loss: 0.716596245765686\n",
            "Epoch 0, Batch 96400, Loss: 0.6336238384246826\n",
            "Epoch 0, Batch 96500, Loss: 0.2314726561307907\n",
            "Epoch 0, Batch 96600, Loss: 0.06468138098716736\n",
            "Epoch 0, Batch 96700, Loss: 0.7026842832565308\n",
            "Epoch 0, Batch 96800, Loss: 0.5020001530647278\n",
            "Epoch 0, Batch 96900, Loss: 0.1673458218574524\n",
            "Epoch 0, Batch 97000, Loss: 0.6729728579521179\n",
            "Epoch 0, Batch 97100, Loss: 0.8070459961891174\n",
            "Epoch 0, Batch 97200, Loss: 0.8298481702804565\n",
            "Epoch 0, Batch 97300, Loss: 0.15677335858345032\n",
            "Epoch 0, Batch 97400, Loss: 0.5661696791648865\n",
            "Epoch 0, Batch 97500, Loss: 0.3695751428604126\n",
            "Epoch 0, Batch 97600, Loss: 0.4410807192325592\n",
            "Epoch 0, Batch 97700, Loss: 0.2488170713186264\n",
            "Epoch 0, Batch 97800, Loss: 0.3129279613494873\n",
            "Epoch 0, Batch 97900, Loss: 0.5210191011428833\n",
            "Epoch 0, Batch 98000, Loss: 0.2920404076576233\n",
            "Epoch 0, Batch 98100, Loss: 0.4876723885536194\n",
            "Epoch 0, Batch 98200, Loss: 0.3317197263240814\n",
            "Epoch 0, Batch 98300, Loss: 0.5642470121383667\n",
            "Epoch 0, Batch 98400, Loss: 0.33910971879959106\n",
            "Epoch 0, Batch 98500, Loss: 0.35299888253211975\n",
            "Epoch 0, Batch 98600, Loss: 0.4319092035293579\n",
            "Epoch 0, Batch 98700, Loss: 0.474746435880661\n",
            "Epoch 1/1, Validation Loss: 0.3786, Accuracy: 0.8303\n",
            "Model saved!\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 does not have a padding token, so we use eos_token\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenize(train_texts)\n",
        "test_encodings = tokenize(test_texts)\n",
        "\n",
        "# Define a custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx]).clone().detach()\n",
        "        return item\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TextDataset(train_encodings, train_labels)\n",
        "test_dataset = TextDataset(test_encodings, test_labels)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Use a smaller batch size for GPT-2\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Load the model\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id  # Explicitly set the padding token id\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training function\n",
        "def train(epoch, model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            logits = outputs.logits\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            label_ids = labels.cpu().numpy()\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(label_ids)\n",
        "\n",
        "    average_loss = total_loss / len(test_loader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy, average_loss\n",
        "\n",
        "# Training loop with checkpointing\n",
        "EPOCHS = 1\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch, model, train_loader, optimizer)\n",
        "    accuracy, val_loss = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"Model saved!\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab203209-4f5f-4ef5-b337-7bc4b7a5f26d",
      "metadata": {
        "id": "ab203209-4f5f-4ef5-b337-7bc4b7a5f26d",
        "outputId": "5488fbc9-1cf9-4bca-d8c1-de90a6c61866"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2ForSequenceClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assuming you have completed training and saved the best model\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id  # Set padding token id\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.load_state_dict(torch.load('best_model.pt', map_location=device))\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b6cb74-743a-4df9-87b3-4a225cd33dc4",
      "metadata": {
        "id": "62b6cb74-743a-4df9-87b3-4a225cd33dc4",
        "outputId": "091e5873-c180-4bb8-ed12-c89073e70f4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c625c1e-bf00-42ef-b1d7-12b98b401c71",
      "metadata": {
        "id": "5c625c1e-bf00-42ef-b1d7-12b98b401c71",
        "outputId": "b2a6ffb5-2ed1-4916-e6f9-70ef2dc13dd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Assuming you have the data and model loaded as mentioned earlier\n",
        "# Tokenize the data\n",
        "predict_encodings = tokenize(data_sampled['text'].tolist())\n",
        "\n",
        "# Create a dataset class for prediction\n",
        "class PredictDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "# Create the prediction dataset and dataloader\n",
        "predict_dataset = PredictDataset(predict_encodings)\n",
        "predict_loader = DataLoader(predict_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Load the trained model\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.load_state_dict(torch.load('best_model.pt', map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Predict function\n",
        "def predict(model, predict_loader):\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in predict_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            predictions.extend(preds)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Perform predictions\n",
        "predicted_labels = predict(model, predict_loader)\n",
        "\n",
        "# Add the predictions to the dataframe\n",
        "data_sampled['predicted_label'] = predicted_labels\n",
        "\n",
        "# Print the first few rows to check the predictions\n",
        "print(data_sampled.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: Best model based on my experiments GPT2"
      ],
      "metadata": {
        "id": "I7us3klOyniW"
      },
      "id": "I7us3klOyniW"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "toc-autonumbering": false,
    "toc-showcode": false
  },
  "nbformat": 4,
  "nbformat_minor": 5
}